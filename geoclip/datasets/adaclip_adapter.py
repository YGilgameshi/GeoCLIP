"""
GeoCLIP - AdaCLIPæ•°æ®é›†é€‚é…å™¨
å°†åŸæœ‰çš„2Dæ•°æ®é›†é€‚é…ä¸º3Dæ•°æ®é›†ï¼Œé€šè¿‡æ·±åº¦ä¼°è®¡ç”Ÿæˆä¼ª3Dæ•°æ®
"""

import torch
import torch.nn as nn
import numpy as np
import random
import cv2
import os
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
import json
from tqdm import tqdm
from PIL import Image
import hashlib
import torch.utils.data as data

# # å¯¼å…¥AdaCLIPåŸæœ‰çš„æ•°æ®é›†åŸºç±»
# try:
#     from dataset.base_dataset import BaseDataset
# except ImportError:
#     print("è­¦å‘Š: æ— æ³•å¯¼å…¥BaseDatasetï¼Œè¯·ç¡®ä¿åœ¨AdaCLIPé¡¹ç›®æ ¹ç›®å½•ä¸‹è¿è¡Œ")

from geoclip.models.depth_estimator import DepthEstimator
from geoclip.utils.transforms import GeoCLIPTransforms


class DataSolver:
    def __init__(self, root, clsnames):
        self.root = root
        self.clsnames = clsnames
        self.path = os.path.join(root, 'meta.json')

    def run(self):
        with open(self.path, 'r') as f:
            info = json.load(f)

        info_required = dict(train={}, test={})
        for cls in self.clsnames:
            for k in info.keys():
                info_required[k][cls] = info[k][cls]

        return info_required


class BaseDataset(data.Dataset):
    def __init__(self, clsnames, transform, target_transform, root, aug_rate=0., training=True):
        self.root = root
        self.transform = transform
        self.target_transform = target_transform
        self.aug_rate = aug_rate
        self.training = training
        self.data_all = []
        self.cls_names = clsnames

        solver = DataSolver(root, clsnames)
        meta_info = solver.run()

        self.meta_info = meta_info['test']  # Only utilize the test dataset for both training and testing
        for cls_name in self.cls_names:
            self.data_all.extend(self.meta_info[cls_name])

        self.length = len(self.data_all)

    def __len__(self):
        return self.length

    def combine_img(self, cls_name):
        """
        From April-GAN: https://github.com/ByChelsea/VAND-APRIL-GAN
        Here we combine four images into a single image for data augmentation.
        """
        img_info = random.sample(self.meta_info[cls_name], 4)

        img_ls = []
        mask_ls = []

        for data in img_info:
            img_path = os.path.join(self.root, data['img_path'])
            mask_path = os.path.join(self.root, data['mask_path'])

            img = Image.open(img_path).convert('RGB')
            img_ls.append(img)

            if not data['anomaly']:
                img_mask = Image.fromarray(np.zeros((img.size[0], img.size[1])), mode='L')
            else:
                img_mask = np.array(Image.open(mask_path).convert('L')) > 0
                img_mask = Image.fromarray(img_mask.astype(np.uint8) * 255, mode='L')

            mask_ls.append(img_mask)

        # Image
        image_width, image_height = img_ls[0].size
        result_image = Image.new("RGB", (2 * image_width, 2 * image_height))
        for i, img in enumerate(img_ls):
            row = i // 2
            col = i % 2
            x = col * image_width
            y = row * image_height
            result_image.paste(img, (x, y))

        # Mask
        result_mask = Image.new("L", (2 * image_width, 2 * image_height))
        for i, img in enumerate(mask_ls):
            row = i // 2
            col = i % 2
            x = col * image_width
            y = row * image_height
            result_mask.paste(img, (x, y))

        return result_image, result_mask

    def __getitem__(self, index):
        data = self.data_all[index]
        img_path = os.path.join(self.root, data['img_path'])
        mask_path = os.path.join(self.root, data['mask_path'])
        cls_name = data['cls_name']
        anomaly = data['anomaly']
        random_number = random.random()

        if self.training and random_number < self.aug_rate:
            img, img_mask = self.combine_img(cls_name)
        else:
            if img_path.endswith('.tif'):
                img = cv2.imread(img_path)
                img = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
            else:
                img = Image.open(img_path).convert('RGB')
            if anomaly == 0:
                img_mask = Image.fromarray(np.zeros((img.size[0], img.size[1])), mode='L')
            else:
                if data['mask_path']:
                    img_mask = np.array(Image.open(mask_path).convert('L')) > 0
                    img_mask = Image.fromarray(img_mask.astype(np.uint8) * 255, mode='L')
                else:
                    img_mask = Image.fromarray(np.zeros((img.size[0], img.size[1])), mode='L')
        # Transforms
        if self.transform is not None:
            img = self.transform(img)
        if self.target_transform is not None and img_mask is not None:
            img_mask = self.target_transform(img_mask)
        if img_mask is None:
            img_mask = []

        return {
            'img': img,
            'img_mask': img_mask,
            'cls_name': cls_name,
            'anomaly': anomaly,
            'img_path': img_path
        }




class AdaCLIPToGeoCLIPAdapter:
    """
    AdaCLIPæ•°æ®é›†åˆ°GeoCLIPçš„é€‚é…å™¨
    ä¸ºåŸæœ‰2Dæ•°æ®é›†ç”Ÿæˆæ·±åº¦ä¿¡æ¯
    """

    def __init__(self,
                 depth_estimator: Optional[DepthEstimator] = None,
                 cache_depth: bool = True,
                 depth_cache_dir: str = "./depth_cache",
                 device: str = None):

        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")

        # åˆå§‹åŒ–æ·±åº¦ä¼°è®¡å™¨
        if depth_estimator is None:
            print("åˆå§‹åŒ–æ·±åº¦ä¼°è®¡å™¨...")
            self.depth_estimator = DepthEstimator(
                model_type="DPT_Large",  # ä½¿ç”¨ä¿®å¤åçš„æ¨¡å‹åç§°
                device=self.device
            )
        else:
            self.depth_estimator = depth_estimator

        self.cache_depth = cache_depth
        self.depth_cache_dir = Path(depth_cache_dir)

        if self.cache_depth:
            self.depth_cache_dir.mkdir(exist_ok=True, parents=True)
            print(f"æ·±åº¦ç¼“å­˜ç›®å½•: {self.depth_cache_dir}")

        self.transforms = GeoCLIPTransforms()

        print(f"AdaCLIPæ•°æ®é›†é€‚é…å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"æ·±åº¦ç¼“å­˜: {'å¯ç”¨' if cache_depth else 'ç¦ç”¨'}")
        print(f"è¿è¡Œè®¾å¤‡: {self.device}")

    def get_depth_cache_path(self, image_path: str) -> str:
        """è·å–æ·±åº¦ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        # ä½¿ç”¨MD5 hashç”Ÿæˆç¨³å®šçš„ç¼“å­˜æ–‡ä»¶å
        path_str = str(image_path)
        path_hash = hashlib.md5(path_str.encode()).hexdigest()[:16]
        cache_filename = f"depth_{path_hash}.npy"
        return str(self.depth_cache_dir / cache_filename)

    # def estimate_or_load_depth(self, image_path: str, image: np.ndarray = None) -> np.ndarray:
    #     """ä¼°è®¡æˆ–åŠ è½½ç¼“å­˜çš„æ·±åº¦å›¾"""
    #     cache_path = self.get_depth_cache_path(image_path)
    #
    #     # å°è¯•ä»ç¼“å­˜åŠ è½½
    #     if self.cache_depth and os.path.exists(cache_path):
    #         try:
    #             depth = np.load(cache_path)
    #             return depth.astype(np.float32)
    #         except Exception as e:
    #             print(f"åŠ è½½ç¼“å­˜å¤±è´¥ {cache_path}: {e}")
    #
    #     # å¦‚æœæ²¡æœ‰æä¾›å›¾åƒï¼Œä»è·¯å¾„åŠ è½½
    #     if image is None:
    #         if not os.path.exists(image_path):
    #             raise FileNotFoundError(f"å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {image_path}")
    #         image = cv2.imread(image_path)
    #         if image is None:
    #             raise ValueError(f"æ— æ³•è¯»å–å›¾åƒ: {image_path}")
    #         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    #
    #     # ä¼°è®¡æ·±åº¦
    #     depth = self.estimate_depth(image)
    #
    #     # ä¿å­˜åˆ°ç¼“å­˜
    #     if self.cache_depth:
    #         try:
    #             np.save(cache_path, depth)
    #         except Exception as e:
    #             print(f"ä¿å­˜ç¼“å­˜å¤±è´¥ {cache_path}: {e}")
    #
    #     return depth

    def estimate_or_load_depth(self, image_path: str, image: np.ndarray = None) -> np.ndarray:
        """ä¼°è®¡æˆ–åŠ è½½ç¼“å­˜çš„æ·±åº¦å›¾"""
        cache_path = self.get_depth_cache_path(image_path)

        # å°è¯•ä»ç¼“å­˜åŠ è½½
        if self.cache_depth and os.path.exists(cache_path):
            try:
                depth = np.load(cache_path)
                return depth.astype(np.float32)
            except Exception as e:
                print(f"åŠ è½½ç¼“å­˜å¤±è´¥ {cache_path}: {e}")

        # å¦‚æœæ²¡æœ‰æä¾›å›¾åƒï¼Œä»è·¯å¾„åŠ è½½
        if image is None:
            if not os.path.exists(image_path):
                raise FileNotFoundError(f"å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {image_path}")

            # ä½¿ç”¨PILåŠ è½½å›¾åƒï¼Œæ›´ç¨³å®š
            try:
                from PIL import Image
                pil_image = Image.open(image_path).convert('RGB')
                image = np.array(pil_image)
            except Exception:
                # å›é€€åˆ°OpenCV
                image = cv2.imread(image_path)
                if image is None:
                    raise ValueError(f"æ— æ³•è¯»å–å›¾åƒ: {image_path}")
                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        # ä¼°è®¡æ·±åº¦
        depth = self.estimate_depth(image)

        # ä¿å­˜åˆ°ç¼“å­˜
        if self.cache_depth:
            try:
                # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
                os.makedirs(os.path.dirname(cache_path), exist_ok=True)
                np.save(cache_path, depth)
            except Exception as e:
                print(f"ä¿å­˜ç¼“å­˜å¤±è´¥ {cache_path}: {e}")

        return depth

    # def estimate_depth(self, image) -> np.ndarray:
    #     """ä½¿ç”¨æ·±åº¦ä¼°è®¡å™¨ä¼°è®¡æ·±åº¦"""
    #     try:
    #         # ç¬¬ä¸€æ­¥ï¼šç¡®ä¿è½¬æ¢ä¸ºnumpyæ•°ç»„
    #         if hasattr(image, 'convert'):  # PIL Image
    #             image = np.array(image.convert('RGB'))
    #         elif isinstance(image, str):  # æ–‡ä»¶è·¯å¾„
    #             import cv2
    #             image = cv2.imread(image)
    #             if image is None:
    #                 raise ValueError(f"æ— æ³•è¯»å–å›¾åƒ: {image}")
    #             image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    #
    #         # ç¡®ä¿æ­¤æ—¶imageç¡®å®æ˜¯numpyæ•°ç»„
    #         if not isinstance(image, np.ndarray):
    #             raise TypeError(f"å›¾åƒè½¬æ¢å¤±è´¥ï¼Œå½“å‰ç±»å‹: {type(image)}")
    #
    #         # ç¬¬äºŒæ­¥ï¼šæ ‡å‡†åŒ–å›¾åƒæ ¼å¼
    #         if len(image.shape) == 2:  # ç°åº¦å›¾
    #             image = np.stack([image, image, image], axis=2)  # è½¬ä¸ºRGB
    #         elif len(image.shape) == 3 and image.shape[2] == 1:  # å•é€šé“
    #             image = np.repeat(image, 3, axis=2)
    #         elif len(image.shape) == 3 and image.shape[2] == 4:  # RGBA
    #             image = image[:, :, :3]  # å»æ‰alphaé€šé“
    #
    #         # ç¡®ä¿æ˜¯3é€šé“RGB
    #         if len(image.shape) != 3 or image.shape[2] != 3:
    #             raise ValueError(f"å›¾åƒæ ¼å¼é”™è¯¯: {image.shape}")
    #
    #         # ç¬¬ä¸‰æ­¥ï¼šè½¬æ¢ä¸ºtorch tensor
    #         if image.dtype == np.uint8:
    #             # å…ˆè½¬æ¢ä¸ºfloatï¼Œå†é™¤ä»¥255
    #             image_float = image.astype(np.float32) / 255.0
    #             image_tensor = torch.from_numpy(image_float).permute(2, 0, 1)
    #         else:
    #             # å·²ç»æ˜¯floatç±»å‹
    #             image_tensor = torch.from_numpy(image.astype(np.float32)).permute(2, 0, 1)
    #
    #         # ç¬¬å››æ­¥ï¼šæ·»åŠ batchç»´åº¦å¹¶ç§»åˆ°è®¾å¤‡
    #         image_tensor = image_tensor.unsqueeze(0).to(self.device)
    #
    #         # ç¬¬äº”æ­¥ï¼šæ·±åº¦ä¼°è®¡
    #         with torch.no_grad():
    #             depth_tensor = self.depth_estimator(image_tensor)
    #
    #         # ç¬¬å…­æ­¥ï¼šè½¬æ¢å›numpy
    #         if depth_tensor.dim() == 4:
    #             depth = depth_tensor[0, 0].cpu().numpy()
    #         elif depth_tensor.dim() == 3:
    #             depth = depth_tensor[0].cpu().numpy()
    #         else:
    #             depth = depth_tensor.cpu().numpy()
    #
    #         return depth.astype(np.float32)
    #
    #     except Exception as e:
    #         print(f"æ·±åº¦ä¼°è®¡å¤±è´¥: {e}")
    #         import traceback
    #         traceback.print_exc()  # æ‰“å°å®Œæ•´é”™è¯¯ä¿¡æ¯ç”¨äºè°ƒè¯•
    #
    #         # è¿”å›é»˜è®¤æ·±åº¦å›¾
    #         try:
    #             if isinstance(image, np.ndarray) and len(image.shape) >= 2:
    #                 h, w = image.shape[:2]
    #             else:
    #                 h, w = 256, 256
    #             return np.ones((h, w), dtype=np.float32)
    #         except:
    #             return np.ones((256, 256), dtype=np.float32)

    def estimate_depth(self, image) -> np.ndarray:
        """ä½¿ç”¨æ·±åº¦ä¼°è®¡å™¨ä¼°è®¡æ·±åº¦"""
        try:
            # ç»Ÿä¸€è½¬æ¢ä¸ºnumpyæ•°ç»„
            if hasattr(image, 'convert'):  # PIL Image
                image = np.array(image.convert('RGB'))
            elif isinstance(image, str):  # æ–‡ä»¶è·¯å¾„
                import cv2
                image = cv2.imread(image)
                if image is None:
                    raise ValueError(f"æ— æ³•è¯»å–å›¾åƒ: {image}")
                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

            if not isinstance(image, np.ndarray):
                raise TypeError(f"ä¸æ”¯æŒçš„å›¾åƒç±»å‹: {type(image)}")

            # ç¡®ä¿å›¾åƒæ ¼å¼æ­£ç¡®
            if len(image.shape) == 2:
                image = np.stack([image, image, image], axis=2)
            elif len(image.shape) == 3 and image.shape[2] != 3:
                if image.shape[2] == 1:
                    image = np.repeat(image, 3, axis=2)
                elif image.shape[2] == 4:
                    image = image[:, :, :3]

            # ç›´æ¥ä½¿ç”¨numpyæ•°ç»„ï¼Œè½¬æ¢ä¸ºtensor
            if image.dtype == np.uint8:
                image_float = image.astype(np.float32) / 255.0
            else:
                image_float = image.astype(np.float32)

            # è½¬æ¢ä¸ºtensor
            image_tensor = torch.from_numpy(image_float).permute(2, 0, 1).unsqueeze(0)
            image_tensor = image_tensor.to(self.device)

            # è°ƒç”¨æ·±åº¦ä¼°è®¡å™¨
            with torch.no_grad():
                depth_tensor = self.depth_estimator(image_tensor)

            # ç¡®ä¿æ˜¯tensoråå†è°ƒç”¨dim()
            if not isinstance(depth_tensor, torch.Tensor):
                # å¦‚æœä¸æ˜¯tensorï¼Œå°è¯•è½¬æ¢
                depth_tensor = torch.tensor(depth_tensor)

            # è½¬æ¢å›numpy
            if depth_tensor.dim() == 4:
                depth = depth_tensor[0, 0].cpu().numpy()
            elif depth_tensor.dim() == 3:
                depth = depth_tensor[0].cpu().numpy()
            else:
                depth = depth_tensor.cpu().numpy()

            return depth.astype(np.float32)

        except Exception as e:
            print(f"æ·±åº¦ä¼°è®¡å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()

            h, w = (image.shape[:2] if isinstance(image, np.ndarray) and len(image.shape) >= 2
                    else (256, 256))
            return np.ones((h, w), dtype=np.float32)

    def preprocess_dataset(self, dataset_class, dataset_config: Dict[str, Any]) -> Dict[str, int]:
        """
        é¢„å¤„ç†æ•´ä¸ªæ•°æ®é›†ï¼Œç”Ÿæˆæ‰€æœ‰æ·±åº¦å›¾

        Args:
            dataset_class: æ•°æ®é›†ç±»
            dataset_config: æ•°æ®é›†é…ç½®å‚æ•°

        Returns:
            å¤„ç†ç»Ÿè®¡ä¿¡æ¯
        """

        print(f"å¼€å§‹é¢„å¤„ç†æ•°æ®é›†: {dataset_config.get('name', 'Unknown')}")

        stats = {
            'total_processed': 0,
            'cache_hits': 0,
            'new_estimates': 0,
            'errors': 0
        }

        # å¤„ç†è®­ç»ƒå’Œæµ‹è¯•æ•°æ®
        for training in [True, False]:
            phase = 'train' if training else 'test'
            print(f"å¤„ç† {phase} æ•°æ®...")

            try:
                # åˆ›å»ºæ•°æ®é›†å®ä¾‹ï¼ˆä½¿ç”¨æ’ç­‰å˜æ¢é¿å…å¤æ‚å¤„ç†ï¼‰
                config = dataset_config.copy()
                config['training'] = training
                config['transform'] = lambda x: x
                config['target_transform'] = lambda x: x

                dataset = dataset_class(**config)

                # éå†æ•°æ®é›†
                for idx in tqdm(range(len(dataset)), desc=f"Processing {phase}"):
                    try:
                        # è·å–æ ·æœ¬ä¿¡æ¯ï¼ˆä½†ä¸å®é™…åŠ è½½æ•°æ®ï¼‰
                        data_info = dataset.data_all[idx]
                        img_path = data_info['img_path']
                        full_path = os.path.join(dataset.root, img_path)

                        # æ£€æŸ¥ç¼“å­˜
                        cache_path = self.get_depth_cache_path(full_path)
                        if os.path.exists(cache_path):
                            stats['cache_hits'] += 1
                        else:
                            # ä¼°è®¡æ·±åº¦ï¼ˆä¼šè‡ªåŠ¨ç¼“å­˜ï¼‰
                            _ = self.estimate_or_load_depth(full_path)
                            stats['new_estimates'] += 1

                        stats['total_processed'] += 1

                    except Exception as e:
                        print(f"å¤„ç†æ ·æœ¬ {idx} æ—¶å‡ºé”™: {e}")
                        stats['errors'] += 1

            except Exception as e:
                print(f"åˆ›å»º {phase} æ•°æ®é›†æ—¶å‡ºé”™: {e}")

        print(f"æ•°æ®é›†é¢„å¤„ç†å®Œæˆ:")
        print(f"  æ€»å¤„ç†: {stats['total_processed']}")
        print(f"  ç¼“å­˜å‘½ä¸­: {stats['cache_hits']}")
        print(f"  æ–°ä¼°è®¡: {stats['new_estimates']}")
        print(f"  é”™è¯¯: {stats['errors']}")

        return stats


class GeoCLIP_AdaCLIPDataset(BaseDataset):
    """
    GeoCLIPç‰ˆæœ¬çš„AdaCLIPæ•°æ®é›†
    åœ¨åŸæœ‰æ•°æ®é›†åŸºç¡€ä¸Šæ·»åŠ æ·±åº¦ä¿¡æ¯
    """

    def __init__(self,
                 dataset_name: str,
                 clsnames: List[str],
                 transform,
                 target_transform,
                 root: str,
                 aug_rate: float = 0.0,
                 training: bool = True,
                 depth_adapter: Optional[AdaCLIPToGeoCLIPAdapter] = None,
                 depth_transform=None):

        self.dataset_name = dataset_name
        self.depth_adapter = depth_adapter or AdaCLIPToGeoCLIPAdapter()
        self.depth_transform = depth_transform

        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
        super().__init__(
            clsnames=clsnames,
            transform=transform,
            target_transform=target_transform,
            root=root,
            aug_rate=aug_rate,
            training=training
        )

        print(f"GeoCLIP-{dataset_name} æ•°æ®é›†åˆå§‹åŒ–å®Œæˆ")
        print(f"  æ ·æœ¬æ•°é‡: {len(self.data_all)}")
        print(f"  ç±»åˆ«æ•°é‡: {len(self.cls_names)}")

    def __getitem__(self, index):
        """é‡å†™getitemæ–¹æ³•ï¼Œæ·»åŠ æ·±åº¦ä¿¡æ¯"""
        # è·å–åŸæœ‰æ•°æ®
        original_sample = super().__getitem__(index)

        # æ·»åŠ æ·±åº¦ä¿¡æ¯
        img_path = self.data_all[index]['img_path']
        full_img_path = os.path.join(self.root, img_path)

        try:
            # ä¼°è®¡æˆ–åŠ è½½æ·±åº¦
            depth = self.depth_adapter.estimate_or_load_depth(full_img_path)

            # åº”ç”¨æ·±åº¦å˜æ¢
            if self.depth_transform is not None:
                # å½’ä¸€åŒ–æ·±åº¦å›¾åˆ°[0, 1]èŒƒå›´
                depth_min, depth_max = depth.min(), depth.max()
                if depth_max > depth_min:
                    depth_normalized = (depth - depth_min) / (depth_max - depth_min)
                else:
                    depth_normalized = np.zeros_like(depth)

                # è½¬æ¢ä¸ºPIL Image
                depth_pil = Image.fromarray((depth_normalized * 255).astype(np.uint8), mode='L')
                depth_transformed = self.depth_transform(depth_pil)

                # å¦‚æœæ˜¯tensorï¼Œè½¬æ¢å›åŸå§‹æ·±åº¦å€¼èŒƒå›´
                if isinstance(depth_transformed, torch.Tensor):
                    if depth_max > depth_min:
                        depth_final = depth_transformed.float() * (depth_max - depth_min) + depth_min
                    else:
                        depth_final = depth_transformed.float()
                else:
                    depth_final = np.array(depth_transformed, dtype=np.float32)
                    if depth_max > depth_min:
                        depth_final = depth_final * (depth_max - depth_min) + depth_min

            else:
                # ç›´æ¥è½¬æ¢ä¸ºtensor
                depth_final = torch.from_numpy(depth).float()
                if depth_final.dim() == 2:
                    depth_final = depth_final.unsqueeze(0)  # æ·»åŠ é€šé“ç»´åº¦

            # æ·»åŠ åˆ°æ ·æœ¬ä¸­
            original_sample['depth'] = depth_final
            original_sample['has_depth'] = True
            original_sample['depth_range'] = (depth.min().item(), depth.max().item())

        except Exception as e:
            print(f"ä¸ºæ ·æœ¬ {index} ç”Ÿæˆæ·±åº¦å›¾æ—¶å‡ºé”™: {e}")
            # åˆ›å»ºé›¶æ·±åº¦å›¾ä½œä¸ºfallback
            if hasattr(original_sample['img'], 'shape'):
                h, w = original_sample['img'].shape[-2:]
                original_sample['depth'] = torch.zeros(1, h, w, dtype=torch.float32)
            else:
                original_sample['depth'] = torch.zeros(1, 256, 256, dtype=torch.float32)
            original_sample['has_depth'] = False
            original_sample['depth_range'] = (0.0, 0.0)

        return original_sample


# ä¾¿åˆ©çš„å·¥å‚å‡½æ•°
def create_geoclip_dataset(dataset_name: str,
                           clsnames: List[str] = None,
                           transform=None,
                           target_transform=None,
                           depth_transform=None,
                           root: str = None,
                           training: bool = True,
                           preprocess_all: bool = False,
                           cache_depth: bool = True,
                           depth_cache_dir: str = None,
                           **kwargs) -> GeoCLIP_AdaCLIPDataset:
    """
    åˆ›å»ºGeoCLIPç‰ˆæœ¬çš„AdaCLIPæ•°æ®é›†

    Args:
        dataset_name: æ•°æ®é›†åç§° ('mvtec', 'visa', 'colondb', ç­‰)
        clsnames: ç±»åˆ«åç§°åˆ—è¡¨
        transform: å›¾åƒå˜æ¢
        target_transform: ç›®æ ‡å˜æ¢
        depth_transform: æ·±åº¦å›¾å˜æ¢
        root: æ•°æ®é›†æ ¹ç›®å½•
        training: æ˜¯å¦ä¸ºè®­ç»ƒæ¨¡å¼
        preprocess_all: æ˜¯å¦é¢„å¤„ç†æ•´ä¸ªæ•°æ®é›†
        cache_depth: æ˜¯å¦ç¼“å­˜æ·±åº¦å›¾
        depth_cache_dir: æ·±åº¦ç¼“å­˜ç›®å½•
        **kwargs: å…¶ä»–å‚æ•°

    Returns:
        GeoCLIPæ•°æ®é›†å®ä¾‹
    """

    # å¯¼å…¥å¯¹åº”çš„æ•°æ®é›†ä¿¡æ¯
    dataset_class = None
    default_clsnames = None
    default_root = None

    if dataset_name == 'mvtec':
        try:
            from dataset.mvtec import MVTecDataset, MVTEC_CLS_NAMES, MVTEC_ROOT
            dataset_class = MVTecDataset
            default_clsnames = MVTEC_CLS_NAMES
            default_root = MVTEC_ROOT
        except ImportError:
            raise ImportError(f"æ— æ³•å¯¼å…¥{dataset_name}æ•°æ®é›†ï¼Œè¯·æ£€æŸ¥å¯¼å…¥è·¯å¾„")

    elif dataset_name == 'visa':
        try:
            from dataset.visa import VisaDataset, VISA_CLS_NAMES, VISA_ROOT
            dataset_class = VisaDataset
            default_clsnames = VISA_CLS_NAMES
            default_root = VISA_ROOT
        except ImportError:
            raise ImportError(f"æ— æ³•å¯¼å…¥{dataset_name}æ•°æ®é›†ï¼Œè¯·æ£€æŸ¥å¯¼å…¥è·¯å¾„")

    elif dataset_name == 'colondb':
        try:
            from dataset.colondb import ColonDBDataset, ColonDB_CLS_NAMES, ColonDB_ROOT
            dataset_class = ColonDBDataset
            default_clsnames = ColonDB_CLS_NAMES
            default_root = ColonDB_ROOT
        except ImportError:
            raise ImportError(f"æ— æ³•å¯¼å…¥{dataset_name}æ•°æ®é›†ï¼Œè¯·æ£€æŸ¥å¯¼å…¥è·¯å¾„")

    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®é›†: {dataset_name}")

    # ä½¿ç”¨é»˜è®¤å€¼
    if clsnames is None:
        clsnames = default_clsnames
    if root is None:
        root = default_root
    if depth_cache_dir is None:
        depth_cache_dir = f"./depth_cache/{dataset_name}"

    # åˆ›å»ºé€‚é…å™¨
    adapter = AdaCLIPToGeoCLIPAdapter(
        cache_depth=cache_depth,
        depth_cache_dir=depth_cache_dir
    )

    # å¦‚æœéœ€è¦é¢„å¤„ç†æ•´ä¸ªæ•°æ®é›†
    if preprocess_all:
        print("å¼€å§‹é¢„å¤„ç†æ•°æ®é›†...")
        dataset_config = {
            'name': dataset_name,
            'clsnames': clsnames,
            'root': root,
            **kwargs
        }
        adapter.preprocess_dataset(dataset_class, dataset_config)

    # åˆ›å»ºæ•°æ®é›†
    dataset = GeoCLIP_AdaCLIPDataset(
        dataset_name=dataset_name,
        clsnames=clsnames,
        transform=transform,
        target_transform=target_transform,
        depth_transform=depth_transform,
        root=root,
        training=training,
        depth_adapter=adapter,
        **{k: v for k, v in kwargs.items()
           if k not in ['cache_depth', 'depth_cache_dir','depth_adapter']}
    )

    return dataset


# æµ‹è¯•å’Œä½¿ç”¨ç¤ºä¾‹
def test_adapter():
    """æµ‹è¯•é€‚é…å™¨åŠŸèƒ½"""
    print("=== æµ‹è¯•AdaCLIPæ•°æ®é›†é€‚é…å™¨ ===")

    try:
        # 1. æµ‹è¯•æ·±åº¦ä¼°è®¡é€‚é…å™¨
        print("\n1. æµ‹è¯•æ·±åº¦ä¼°è®¡é€‚é…å™¨")
        adapter = AdaCLIPToGeoCLIPAdapter(
            cache_depth=True,
            depth_cache_dir="./test_depth_cache"
        )

        # åˆ›å»ºæµ‹è¯•å›¾åƒ
        test_image = np.random.randint(0, 255, (256, 256, 3), dtype=np.uint8)
        test_path = "test_image.jpg"

        # æµ‹è¯•æ·±åº¦ä¼°è®¡
        depth = adapter.estimate_or_load_depth(test_path, test_image)
        print(f"âœ… æ·±åº¦ä¼°è®¡æˆåŠŸ:")
        print(f"   æ·±åº¦å½¢çŠ¶: {depth.shape}")
        print(f"   æ·±åº¦èŒƒå›´: {depth.min():.3f} - {depth.max():.3f}")

        # æµ‹è¯•ç¼“å­˜åŠŸèƒ½
        depth2 = adapter.estimate_or_load_depth(test_path, test_image)
        cache_works = np.allclose(depth, depth2)
        print(f"âœ… ç¼“å­˜æµ‹è¯•: {'æˆåŠŸ' if cache_works else 'å¤±è´¥'}")

        # 2. æµ‹è¯•æ•°æ®é›†åˆ›å»ºï¼ˆå¯èƒ½éœ€è¦å®é™…æ•°æ®é›†ï¼‰
        print("\n2. æµ‹è¯•æ•°æ®é›†åˆ›å»º")
        try:
            import torchvision.transforms as transforms

            # ç®€å•çš„transform
            simple_transform = transforms.Compose([
                transforms.Resize((224, 224)),
                transforms.ToTensor()
            ])

            depth_transform = transforms.Compose([
                transforms.Resize((224, 224)),
                transforms.ToTensor()
            ])

            dataset = create_geoclip_dataset(
                dataset_name='mvtec',
                clsnames=['bottle'],  # åªæµ‹è¯•ä¸€ä¸ªç±»åˆ«
                transform=simple_transform,
                target_transform=simple_transform,
                depth_transform=depth_transform,
                training=True,
                preprocess_all=False,
                cache_depth=True
            )

            print(f"âœ… GeoCLIPæ•°æ®é›†åˆ›å»ºæˆåŠŸ: {len(dataset)} ä¸ªæ ·æœ¬")

            # æµ‹è¯•è·å–æ ·æœ¬
            if len(dataset) > 0:
                sample = dataset[0]
                print(f"âœ… æ ·æœ¬é”®: {list(sample.keys())}")
                if 'depth' in sample:
                    print(f"   æ·±åº¦å½¢çŠ¶: {sample['depth'].shape}")
                    print(f"   æ˜¯å¦æœ‰æ·±åº¦: {sample['has_depth']}")
                    print(f"   æ·±åº¦èŒƒå›´: {sample.get('depth_range', 'N/A')}")

        except Exception as e:
            print(f"âš ï¸ æ•°æ®é›†æµ‹è¯•è·³è¿‡ (å¯èƒ½ç¼ºå°‘æ•°æ®): {e}")

        print("\nğŸ‰ é€‚é…å™¨æµ‹è¯•å®Œæˆ!")

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    test_adapter()