"""
GeoCLIP - ä¸»æ¨¡å‹
æ•´åˆ2D CLIPç‰¹å¾ã€3Då‡ ä½•ç‰¹å¾å’Œæ·±åº¦ä¿¡æ¯è¿›è¡Œå¼‚å¸¸æ£€æµ‹
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Optional, Tuple, Union, Any
import numpy as np
import open_clip

# å¯¼å…¥GeoCLIPç»„ä»¶
from geoclip.models.depth_estimator import DepthEstimator
from geoclip.models.geometry_encoder import create_geometry_encoder, VoxelEncoder
from geoclip.utils.voxel_utils import DepthToVoxelConverter


class FeatureFusionModule(nn.Module):
    """
    2D CLIPç‰¹å¾ä¸3Då‡ ä½•ç‰¹å¾èåˆæ¨¡å—
    """

    def __init__(self,
                 clip_dim: int = 512,
                 geometry_dim: int = 512,
                 fusion_dim: int = 1024,
                 output_dim: int = 512,
                 fusion_type: str = "cross_attention"):
        super(FeatureFusionModule, self).__init__()

        self.fusion_type = fusion_type
        self.clip_dim = clip_dim
        self.geometry_dim = geometry_dim
        self.fusion_dim = fusion_dim
        self.output_dim = output_dim

        if fusion_type == "cross_attention":
            self._build_cross_attention()
        elif fusion_type == "adaptive_fusion":
            self._build_adaptive_fusion()
        elif fusion_type == "simple_concat":
            self._build_simple_concat()
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„èåˆç±»å‹: {fusion_type}")

    def _build_cross_attention(self):
        """æ„å»ºäº¤å‰æ³¨æ„åŠ›èåˆ"""
        # æŠ•å½±åˆ°ç›¸åŒç»´åº¦
        self.clip_proj = nn.Linear(self.clip_dim, self.fusion_dim)
        self.geometry_proj = nn.Linear(self.geometry_dim, self.fusion_dim)

        # äº¤å‰æ³¨æ„åŠ›
        self.cross_attention = nn.MultiheadAttention(
            embed_dim=self.fusion_dim,
            num_heads=8,
            dropout=0.1,
            batch_first=True
        )

        # è¾“å‡ºæŠ•å½±
        self.output_proj = nn.Sequential(
            nn.Linear(self.fusion_dim * 2, self.output_dim),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1),
            nn.Linear(self.output_dim, self.output_dim)
        )

    def _build_adaptive_fusion(self):
        """æ„å»ºè‡ªé€‚åº”èåˆ"""
        # ç‰¹å¾æŠ•å½±
        self.clip_proj = nn.Linear(self.clip_dim, self.fusion_dim)
        self.geometry_proj = nn.Linear(self.geometry_dim, self.fusion_dim)

        # è‡ªé€‚åº”æƒé‡ç½‘ç»œ
        self.weight_net = nn.Sequential(
            nn.Linear(self.fusion_dim * 2, self.fusion_dim),
            nn.ReLU(inplace=True),
            nn.Linear(self.fusion_dim, 2),
            nn.Softmax(dim=-1)
        )

        # è¾“å‡ºæŠ•å½±
        self.output_proj = nn.Linear(self.fusion_dim, self.output_dim)

    def _build_simple_concat(self):
        """æ„å»ºç®€å•è¿æ¥èåˆ"""
        self.fusion_proj = nn.Sequential(
            nn.Linear(self.clip_dim + self.geometry_dim, self.fusion_dim),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1),
            nn.Linear(self.fusion_dim, self.output_dim)
        )

    def forward(self, clip_features: torch.Tensor,
                geometry_features: torch.Tensor) -> torch.Tensor:
        """
        èåˆ2Då’Œ3Dç‰¹å¾

        Args:
            clip_features: CLIP 2Dç‰¹å¾ [B, clip_dim]
            geometry_features: å‡ ä½•3Dç‰¹å¾ [B, geometry_dim]

        Returns:
            fused_features: èåˆç‰¹å¾ [B, output_dim]
        """
        if self.fusion_type == "cross_attention":
            return self._cross_attention_forward(clip_features, geometry_features)
        elif self.fusion_type == "adaptive_fusion":
            return self._adaptive_fusion_forward(clip_features, geometry_features)
        elif self.fusion_type == "simple_concat":
            return self._simple_concat_forward(clip_features, geometry_features)

    # æŠ•å½±
    def _cross_attention_forward(self, clip_feat, geometry_feat):
        clip_proj = self.clip_proj(clip_feat).unsqueeze(1)  # [B, 1, fusion_dim]
        geometry_proj = self.geometry_proj(geometry_feat).unsqueeze(1)  # [B, 1, fusion_dim]

        # äº¤å‰æ³¨æ„åŠ›ï¼šCLIPæŸ¥è¯¢å‡ ä½•ç‰¹å¾
        clip_enhanced, _ = self.cross_attention(clip_proj, geometry_proj, geometry_proj)

        # äº¤å‰æ³¨æ„åŠ›ï¼šå‡ ä½•æŸ¥è¯¢CLIPç‰¹å¾
        geometry_enhanced, _ = self.cross_attention(geometry_proj, clip_proj, clip_proj)

        # è¿æ¥å’ŒæŠ•å½±
        fused = torch.cat([clip_enhanced.squeeze(1), geometry_enhanced.squeeze(1)], dim=1)
        return self.output_proj(fused)

    def _adaptive_fusion_forward(self, clip_feat, geometry_feat):
        # æŠ•å½±
        clip_proj = self.clip_proj(clip_feat)
        geometry_proj = self.geometry_proj(geometry_feat)

        # è®¡ç®—è‡ªé€‚åº”æƒé‡
        combined = torch.cat([clip_proj, geometry_proj], dim=1)
        weights = self.weight_net(combined)  # [B, 2]

        # åŠ æƒèåˆ
        w1, w2 = weights[:, 0:1], weights[:, 1:2]
        fused = w1 * clip_proj + w2 * geometry_proj

        return self.output_proj(fused)

    def _simple_concat_forward(self, clip_feat, geometry_feat):
        # ç®€å•è¿æ¥
        concatenated = torch.cat([clip_feat, geometry_feat], dim=1)
        return self.fusion_proj(concatenated)


class AnomalyDetectionHead(nn.Module):
    """
    å¼‚å¸¸æ£€æµ‹å¤´
    åŸºäºèåˆç‰¹å¾è¿›è¡Œå¼‚å¸¸æ£€æµ‹
    """

    def __init__(self,
                 input_dim: int = 512,
                 hidden_dim: int = 256,
                 num_classes: int = 2,  # æ­£å¸¸/å¼‚å¸¸
                 detection_type: str = "classification"):
        super(AnomalyDetectionHead, self).__init__()

        self.detection_type = detection_type
        self.input_dim = input_dim

        if detection_type == "classification":
            self.head = nn.Sequential(
                nn.Linear(input_dim, hidden_dim),
                nn.ReLU(inplace=True),
                nn.Dropout(0.1),
                nn.Linear(hidden_dim, hidden_dim),
                nn.ReLU(inplace=True),
                nn.Dropout(0.1),
                nn.Linear(hidden_dim, num_classes)
            )
        elif detection_type == "regression":
            self.head = nn.Sequential(
                nn.Linear(input_dim, hidden_dim),
                nn.ReLU(inplace=True),
                nn.Dropout(0.1),
                nn.Linear(hidden_dim, hidden_dim),
                nn.ReLU(inplace=True),
                nn.Dropout(0.1),
                nn.Linear(hidden_dim, 1),
                nn.Sigmoid()  # å¼‚å¸¸åˆ†æ•° [0, 1]
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ£€æµ‹ç±»å‹: {detection_type}")

    def forward(self, features: torch.Tensor) -> torch.Tensor:
        """
        å¼‚å¸¸æ£€æµ‹å‰å‘ä¼ æ’­

        Args:
            features: èåˆç‰¹å¾ [B, input_dim]

        Returns:
            predictions: é¢„æµ‹ç»“æœ
                - classification: [B, num_classes]
                - regression: [B, 1]
        """
        return self.head(features)


class GeoCLIP(nn.Module):
    """
    GeoCLIPä¸»æ¨¡å‹
    æ•´åˆ2D CLIPã€3Då‡ ä½•ã€æ·±åº¦ä¿¡æ¯è¿›è¡Œå¼‚å¸¸æ£€æµ‹
    """

    def __init__(self,
                 # 2Dæ¨¡å‹é…ç½®
                 clip_model_name: str = "ViT-B/16",
                 clip_pretrained: str = "openai",

                 # 3Dæ¨¡å‹é…ç½®
                 depth_estimator_type: str = "DPT_Large",
                 geometry_encoder_config: Dict = None,

                 # èåˆé…ç½®
                 fusion_type: str = "cross_attention",
                 fusion_dim: int = 1024,
                 output_dim: int = 512,

                 # å¼‚å¸¸æ£€æµ‹é…ç½®
                 detection_type: str = "regression",
                 num_classes: int = 2,

                 # å…¶ä»–é…ç½®
                 device: str = "cuda",
                 freeze_clip: bool = False):

        super(GeoCLIP, self).__init__()

        self.device = device
        self.freeze_clip = freeze_clip

        # 1. 2D CLIPæ¨¡å‹
        self.clip_model = self._load_clip_model(clip_model_name, clip_pretrained)
        if freeze_clip:
            for param in self.clip_model.parameters():
                param.requires_grad = False

        # 2. æ·±åº¦ä¼°è®¡å™¨
        self.depth_estimator = DepthEstimator(
            model_type=depth_estimator_type,
            device=device
        )

        # 3. æ·±åº¦åˆ°ä½“ç´ è½¬æ¢å™¨
        self.voxel_converter = DepthToVoxelConverter(
            voxel_size=64,
            depth_range=(0.1, 10.0)
        )

        # 4. 3Då‡ ä½•ç¼–ç å™¨
        if geometry_encoder_config is None:
            geometry_encoder_config = {
                'type': 'voxel',
                'in_channels': 4,  # RGB + Depth
                'output_channels': 512
            }

        self.geometry_encoder = create_geometry_encoder(geometry_encoder_config)

        # 5. ç‰¹å¾èåˆæ¨¡å—
        # å®‰å…¨åœ°è·å–CLIPç»´åº¦
        def get_clip_dim(model):
            """å®‰å…¨åœ°è·å–CLIPæ¨¡å‹çš„ç‰¹å¾ç»´åº¦"""
            # æ–¹æ³•1: æ£€æŸ¥transformer.width
            if hasattr(model, 'transformer') and hasattr(model.transformer, 'width'):
                return model.transformer.width

            # æ–¹æ³•2: æ£€æŸ¥widthå±æ€§
            if hasattr(model, 'width'):
                return model.width

            # æ–¹æ³•3: æ£€æŸ¥visual.width
            if hasattr(model, 'visual') and hasattr(model.visual, 'width'):
                return model.visual.width

            # æ–¹æ³•4: æ ¹æ®æ¨¡å‹åç§°æ¨æ–­
            model_name_lower = clip_model_name.lower()
            if 'vit-b' in model_name_lower or 'vitb' in model_name_lower:
                return 768
            elif 'vit-l' in model_name_lower or 'vitl' in model_name_lower:
                return 1024
            elif 'vit-h' in model_name_lower or 'vith' in model_name_lower:
                return 1280
            elif 'rn50' in model_name_lower or 'resnet50' in model_name_lower:
                return 1024
            elif 'rn101' in model_name_lower or 'resnet101' in model_name_lower:
                return 512

            # é»˜è®¤å€¼
            print(f"âš  æ— æ³•ç¡®å®šCLIPç»´åº¦ï¼Œä½¿ç”¨é»˜è®¤å€¼512")
            return 512

        # clip_dim = get_clip_dim(self.clip_model)
        clip_dim = 768
        geometry_dim = geometry_encoder_config['output_channels']

        print(f"  ç‰¹å¾ç»´åº¦ - CLIP: {clip_dim}, å‡ ä½•: {geometry_dim}")

        self.fusion_module = FeatureFusionModule(
            clip_dim=clip_dim,
            geometry_dim=geometry_dim,
            fusion_dim=fusion_dim,
            output_dim=output_dim,
            fusion_type=fusion_type
        )
        clip_dim = self.clip_model.transformer.width if hasattr(self.clip_model, 'transformer') else 512
        geometry_dim = geometry_encoder_config['output_channels']

        self.fusion_module = FeatureFusionModule(
            clip_dim=clip_dim,
            geometry_dim=geometry_dim,
            fusion_dim=fusion_dim,
            output_dim=output_dim,
            fusion_type=fusion_type
        )

        # 6. å¼‚å¸¸æ£€æµ‹å¤´
        self.anomaly_head = AnomalyDetectionHead(
            input_dim=output_dim,
            detection_type=detection_type,
            num_classes=num_classes
        )

        print(f"GeoCLIPæ¨¡å‹åˆå§‹åŒ–å®Œæˆ:")
        print(f"  CLIPæ¨¡å‹: {clip_model_name}")
        print(f"  æ·±åº¦ä¼°è®¡: {depth_estimator_type}")
        print(f"  å‡ ä½•ç¼–ç : {geometry_encoder_config['type']}")
        print(f"  èåˆæ–¹å¼: {fusion_type}")
        print(f"  æ£€æµ‹ç±»å‹: {detection_type}")

    # def _load_clip_model(self, model_name: str, pretrained: str):
    #     """åŠ è½½CLIPæ¨¡å‹ - ä»æœ¬åœ°ç¼“å­˜åŠ è½½"""
    #
    #     import os
    #     from pathlib import Path
    #
    #     model_name_converted = model_name.replace('/', '-')
    #
    #     # æŒ‡å®šç¼“å­˜ç›®å½•
    #     cache_dir = Path.home() / '.cache' / 'open_clip'
    #     cache_dir.mkdir(parents=True, exist_ok=True)
    #
    #     print(f"æ­£åœ¨ä»ç¼“å­˜åŠ è½½CLIPæ¨¡å‹: {model_name_converted}")
    #     print(f"ç¼“å­˜ç›®å½•: {cache_dir}")
    #
    #     try:
    #         # å°è¯•ä»æœ¬åœ°ç¼“å­˜åŠ è½½
    #         model, _, preprocess = open_clip.create_model_and_transforms(
    #             model_name_converted,
    #             pretrained=pretrained if pretrained != 'openai' else 'openai',
    #             cache_dir=str(cache_dir)
    #         )
    #
    #         model = model.to(self.device)
    #         self.preprocess = preprocess
    #         print(f"æˆåŠŸåŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {model_name_converted}")
    #         return model
    #
    #     except Exception as e:
    #         print(f"ä»ç¼“å­˜åŠ è½½å¤±è´¥: {e}")
    #         # æ£€æŸ¥ç¼“å­˜æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    #         cache_files = list(cache_dir.glob("*.pt"))
    #         print(f"ç¼“å­˜ç›®å½•ä¸­çš„æ–‡ä»¶: {cache_files}")
    #
    #         # å¦‚æœæœ‰.ptæ–‡ä»¶ï¼Œå°è¯•ç›´æ¥åŠ è½½
    #         if cache_files:
    #             print("å°è¯•ç›´æ¥åŠ è½½æœ¬åœ°æƒé‡æ–‡ä»¶...")
    #             model, _, preprocess = open_clip.create_model_and_transforms(
    #                 model_name_converted,
    #                 pretrained=None
    #             )
    #             # æ‰‹åŠ¨åŠ è½½æƒé‡
    #             state_dict = torch.load(cache_files[0], map_location=self.device)
    #             model.load_state_dict(state_dict, strict=False)
    #             model = model.to(self.device)
    #             self.preprocess = preprocess
    #             print("æˆåŠŸä»æœ¬åœ°æ–‡ä»¶åŠ è½½æƒé‡")
    #             return model
    #         else:
    #             raise RuntimeError(f"ç¼“å­˜ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°æ¨¡å‹æ–‡ä»¶: {cache_dir}")

    def _load_clip_model(self, model_name: str, pretrained: str):
        """åŠ è½½CLIPæ¨¡å‹ - ä¼˜å…ˆä»æœ¬åœ°ç¼“å­˜åŠ è½½TorchScriptæ¨¡å‹"""
        import open_clip
        import torch
        from pathlib import Path
        from torchvision import transforms

        model_name_converted = model_name.replace('/', '-')
        cache_dir = Path.home() / '.cache' / 'open_clip'
        cache_dir.mkdir(parents=True, exist_ok=True)

        print(f"æ­£åœ¨ä»æœ¬åœ°åŠ è½½CLIPæ¨¡å‹: {model_name_converted}")

        # å®šä¹‰æ ‡å‡†çš„CLIPé¢„å¤„ç†pipeline
        standard_preprocess = transforms.Compose([
            transforms.Resize(224, interpolation=transforms.InterpolationMode.BICUBIC),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(
                mean=[0.48145466, 0.4578275, 0.40821073],
                std=[0.26862954, 0.26130258, 0.27577711]
            )
        ])

        # 1. ä¼˜å…ˆå°è¯•åŠ è½½æœ¬åœ°çš„TorchScriptæ¨¡å‹(.ptæ–‡ä»¶)
        pt_files = list(cache_dir.glob(f"*{model_name_converted}*.pt"))
        if not pt_files:
            pt_files = list(cache_dir.glob("*.pt"))

        if pt_files:
            print(f"å‘ç°æœ¬åœ°TorchScriptæ¨¡å‹: {pt_files[0]}")
            try:
                jit_model = torch.jit.load(str(pt_files[0]), map_location=self.device)
                jit_model.eval()

                # ä¸ºTorchScriptæ¨¡å‹åˆ›å»ºåŒ…è£…å™¨,æ·»åŠ ç¼ºå¤±çš„å±æ€§
                class TorchScriptWrapper:
                    def __init__(self, jit_model, device):
                        self.model = jit_model
                        self.device = device
                        # å°è¯•æ¨æ–­æˆ–è®¾ç½®é»˜è®¤å±æ€§
                        self.width = 768  # ViT-Bé»˜è®¤å®½åº¦,å¯æ ¹æ®å®é™…æ¨¡å‹è°ƒæ•´
                        self.visual = self  # æŸäº›ä»£ç å¯èƒ½è®¿é—®model.visual

                        # æ·»åŠ transformerå±æ€§æ¨¡æ‹Ÿ
                        class TransformerProxy:
                            def __init__(self, width):
                                self.width = width

                        self.transformer = TransformerProxy(self.width)

                    def encode_image(self, image):
                        """å›¾åƒç¼–ç æ–¹æ³•"""
                        return self.model.encode_image(image)

                    def encode_text(self, text):
                        """æ–‡æœ¬ç¼–ç æ–¹æ³•"""
                        return self.model.encode_text(text)

                    def __call__(self, *args, **kwargs):
                        return self.model(*args, **kwargs)

                    def to(self, device):
                        self.model.to(device)
                        return self

                    def eval(self):
                        self.model.eval()
                        return self

                    def __getattr__(self, name):
                        # è½¬å‘å…¶ä»–å±æ€§è®¿é—®åˆ°åŸå§‹æ¨¡å‹
                        try:
                            return getattr(self.model, name)
                        except AttributeError:
                            raise AttributeError(f"TorchScriptæ¨¡å‹æ²¡æœ‰å±æ€§: {name}")

                model = TorchScriptWrapper(jit_model, self.device)
                self.preprocess = standard_preprocess
                print("âœ“ æˆåŠŸåŠ è½½æœ¬åœ°TorchScriptæ¨¡å‹(å·²æ·»åŠ å…¼å®¹å±‚)")
                return model
            except Exception as e:
                print(f"âœ— åŠ è½½TorchScriptæ¨¡å‹å¤±è´¥: {e}")

        # 2. å°è¯•ä½¿ç”¨open_clipä»æœ¬åœ°ç¼“å­˜åŠ è½½é¢„è®­ç»ƒæƒé‡
        try:
            print("å°è¯•ä½¿ç”¨open_clipåŠ è½½é¢„è®­ç»ƒæ¨¡å‹...")
            model, _, preprocess = open_clip.create_model_and_transforms(
                model_name_converted,
                pretrained=pretrained if pretrained != 'openai' else 'openai',
                cache_dir=str(cache_dir)
            )
            model = model.to(self.device)
            model.eval()
            self.preprocess = preprocess
            print("âœ“ æˆåŠŸåŠ è½½open_clipé¢„è®­ç»ƒæ¨¡å‹")
            return model
        except Exception as e:
            print(f"âœ— open_clipåŠ è½½å¤±è´¥: {e}")

        # 3. æœ€åå›é€€: åˆ›å»ºæ— é¢„è®­ç»ƒæƒé‡çš„æ¨¡å‹
        print("âš  å›é€€åˆ°æ— é¢„è®­ç»ƒæƒé‡æ¨¡å‹")
        try:
            model, _, preprocess = open_clip.create_model_and_transforms(
                model_name_converted,
                pretrained=None
            )
            model = model.to(self.device)
            model.eval()

            # ä½¿ç”¨æ ‡å‡†é¢„å¤„ç†æˆ–open_clipæä¾›çš„é¢„å¤„ç†
            self.preprocess = preprocess if preprocess is not None else standard_preprocess
            print("âœ“ æˆåŠŸåˆ›å»ºæ— é¢„è®­ç»ƒæƒé‡æ¨¡å‹(éœ€è¦é‡æ–°è®­ç»ƒ)")
            return model
        except Exception as e:
            print(f"âœ— åˆ›å»ºæ¨¡å‹å¤±è´¥: {e}")
            raise RuntimeError(f"æ— æ³•åŠ è½½æˆ–åˆ›å»ºCLIPæ¨¡å‹: {e}")

    def forward(self, batch: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """
        GeoCLIPå‰å‘ä¼ æ’­

        Args:
            batch: æ‰¹æ¬¡æ•°æ®ï¼ŒåŒ…å«ï¼š
                - 'image': RGBå›¾åƒ [B, 3, H, W]
                - 'depth': æ·±åº¦å›¾ [B, 1, H, W] (å¯é€‰ï¼Œå¦‚æœæ²¡æœ‰ä¼šè‡ªåŠ¨ä¼°è®¡)
                - 'text': æ–‡æœ¬token (å¯é€‰)

        Returns:
            è¾“å‡ºå­—å…¸ï¼ŒåŒ…å«ï¼š
                - 'anomaly_score': å¼‚å¸¸åˆ†æ•°
                - 'clip_features': CLIPç‰¹å¾
                - 'geometry_features': å‡ ä½•ç‰¹å¾
                - 'fused_features': èåˆç‰¹å¾
        """
        images = batch['image']
        batch_size = images.size(0)

        # 1. è·å–æ·±åº¦å›¾
        if 'depth' in batch and batch['depth'] is not None:
            depth_maps = batch['depth']
        else:
            # ä½¿ç”¨æ·±åº¦ä¼°è®¡å™¨ä¼°è®¡æ·±åº¦
            with torch.no_grad():
                depth_maps = self.depth_estimator(images)

        # 2. CLIPç‰¹å¾æå–
        if 'text' in batch:
            # å¦‚æœæœ‰æ–‡æœ¬ï¼Œä½¿ç”¨å›¾åƒ-æ–‡æœ¬å¯¹æ¯”
            clip_features = self.clip_model.encode_image(images)
        else:
            # ä»…ä½¿ç”¨å›¾åƒç¼–ç 
            clip_features = self.clip_model.encode_image(images)

        # 3. è½¬æ¢ä¸ºä½“ç´ è¡¨ç¤º
        # åˆå¹¶RGBå’Œæ·±åº¦ä¿¡æ¯
        rgbd_images = torch.cat([images, depth_maps], dim=1)  # [B, 4, H, W]

        # è½¬æ¢ä¸º3Dä½“ç´ 
        voxels = self.voxel_converter.images_to_voxels(rgbd_images)  # [B, 4, D, H, W]

        # 4. 3Då‡ ä½•ç‰¹å¾æå–
        geometry_features = self.geometry_encoder(voxels)

        # 5. ç‰¹å¾èåˆ
        fused_features = self.fusion_module(clip_features, geometry_features)

        # 6. å¼‚å¸¸æ£€æµ‹
        anomaly_predictions = self.anomaly_head(fused_features)

        # è¿”å›ç»“æœ
        results = {
            'anomaly_predictions': anomaly_predictions,
            'clip_features': clip_features,
            'geometry_features': geometry_features,
            'fused_features': fused_features,
            'depth_maps': depth_maps
        }

        return results

    def predict_anomaly(self, batch: Dict[str, torch.Tensor]) -> torch.Tensor:
        """
        é¢„æµ‹å¼‚å¸¸åˆ†æ•°

        Args:
            batch: è¾“å…¥æ‰¹æ¬¡

        Returns:
            anomaly_scores: å¼‚å¸¸åˆ†æ•° [B] æˆ– [B, num_classes]
        """
        with torch.no_grad():
            results = self.forward(batch)
            predictions = results['anomaly_predictions']

            if self.anomaly_head.detection_type == "regression":
                return predictions.squeeze(-1)  # [B]
            else:
                # åˆ†ç±»æƒ…å†µï¼Œè¿”å›å¼‚å¸¸ç±»åˆ«çš„æ¦‚ç‡
                return F.softmax(predictions, dim=-1)[:, 1]  # [B]

    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)

        return {
            'total_parameters': total_params,
            'trainable_parameters': trainable_params,
            'clip_frozen': self.freeze_clip,
            'fusion_type': self.fusion_module.fusion_type,
            'detection_type': self.anomaly_head.detection_type,
            'device': self.device
        }


def create_geoclip_model(config: Dict[str, Any]) -> GeoCLIP:
    """
    å·¥å‚å‡½æ•°ï¼šæ ¹æ®é…ç½®åˆ›å»ºGeoCLIPæ¨¡å‹

    Args:
        config: æ¨¡å‹é…ç½®å­—å…¸

    Returns:
        GeoCLIPæ¨¡å‹å®ä¾‹
    """
    return GeoCLIP(
        clip_model_name=config.get('clip_model', 'ViT-B/16'),
        clip_pretrained=config.get('clip_pretrained', 'openai'),
        depth_estimator_type=config.get('depth_estimator', 'DPT_Large'),
        geometry_encoder_config=config.get('geometry_encoder', None),
        fusion_type=config.get('fusion_type', 'cross_attention'),
        fusion_dim=config.get('fusion_dim', 1024),
        output_dim=config.get('output_dim', 512),
        detection_type=config.get('detection_type', 'regression'),
        num_classes=config.get('num_classes', 2),
        device=config.get('device', 'cuda'),
        freeze_clip=config.get('freeze_clip', False)
    )


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    print("=== æµ‹è¯•GeoCLIPæ¨¡å‹ ===")

    try:
        # åˆ›å»ºæ¨¡å‹é…ç½®
        config = {
            'clip_model': 'ViT-B/16',
            'depth_estimator': 'DPT_Large',
            'fusion_type': 'cross_attention',
            'detection_type': 'regression',
            'device': 'cuda' if torch.cuda.is_available() else 'cpu'
        }

        # åˆ›å»ºæ¨¡å‹
        model = create_geoclip_model(config)
        print(f"âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ")
        print(f"æ¨¡å‹ä¿¡æ¯: {model.get_model_info()}")

        # åˆ›å»ºæµ‹è¯•æ•°æ®
        batch_size = 2
        test_batch = {
            'image': torch.randn(batch_size, 3, 224, 224),
            # 'depth': torch.randn(batch_size, 1, 224, 224),  # å¯é€‰
        }

        # ç§»åŠ¨åˆ°è®¾å¤‡
        device = config['device']
        model = model.to(device)
        for key in test_batch:
            test_batch[key] = test_batch[key].to(device)

        # å‰å‘ä¼ æ’­æµ‹è¯•
        print("\næµ‹è¯•å‰å‘ä¼ æ’­...")
        results = model(test_batch)

        print(f"âœ… å‰å‘ä¼ æ’­æˆåŠŸ:")
        for key, value in results.items():
            if isinstance(value, torch.Tensor):
                print(f"  {key}: {value.shape}")

        # å¼‚å¸¸é¢„æµ‹æµ‹è¯•
        anomaly_scores = model.predict_anomaly(test_batch)
        print(f"âœ… å¼‚å¸¸é¢„æµ‹: {anomaly_scores.shape}")
        print(f"å¼‚å¸¸åˆ†æ•°èŒƒå›´: {anomaly_scores.min():.3f} - {anomaly_scores.max():.3f}")

        print("\nğŸ‰ GeoCLIPæ¨¡å‹æµ‹è¯•å®Œæˆ!")

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()