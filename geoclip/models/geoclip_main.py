"""
GeoCLIP - ä¸»æ¨¡å‹
æ•´åˆ2D CLIPç‰¹å¾ã€3Då‡ ä½•ç‰¹å¾å’Œæ·±åº¦ä¿¡æ¯è¿›è¡Œå¼‚å¸¸æ£€æµ‹
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Optional, Tuple, Union, Any
import numpy as np

# å¯¼å…¥GeoCLIPç»„ä»¶
from geoclip.models.depth_estimator import DepthEstimator
from geoclip.models.geometry_encoder import create_geometry_encoder, VoxelEncoder
from geoclip.utils.voxel_utils import DepthToVoxelConverter

# å¯¼å…¥AdaCLIPçš„CLIPæ¨¡å‹
try:
    from method.clip_model import load_clip_model
    from method.ada_clip import AdaCLIP
except ImportError:
    print("è­¦å‘Š: æ— æ³•å¯¼å…¥AdaCLIPæ¨¡å‹ï¼Œè¯·ç¡®ä¿åœ¨æ­£ç¡®çš„ç¯å¢ƒä¸­è¿è¡Œ")


class FeatureFusionModule(nn.Module):
    """
    2D CLIPç‰¹å¾ä¸3Då‡ ä½•ç‰¹å¾èåˆæ¨¡å—
    """

    def __init__(self,
                 clip_dim: int = 512,
                 geometry_dim: int = 512,
                 fusion_dim: int = 1024,
                 output_dim: int = 512,
                 fusion_type: str = "cross_attention"):
        super(FeatureFusionModule, self).__init__()

        self.fusion_type = fusion_type
        self.clip_dim = clip_dim
        self.geometry_dim = geometry_dim
        self.fusion_dim = fusion_dim
        self.output_dim = output_dim

        if fusion_type == "cross_attention":
            self._build_cross_attention()
        elif fusion_type == "adaptive_fusion":
            self._build_adaptive_fusion()
        elif fusion_type == "simple_concat":
            self._build_simple_concat()
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„èåˆç±»å‹: {fusion_type}")

    def _build_cross_attention(self):
        """æ„å»ºäº¤å‰æ³¨æ„åŠ›èåˆ"""
        # æŠ•å½±åˆ°ç›¸åŒç»´åº¦
        self.clip_proj = nn.Linear(self.clip_dim, self.fusion_dim)
        self.geometry_proj = nn.Linear(self.geometry_dim, self.fusion_dim)

        # äº¤å‰æ³¨æ„åŠ›
        self.cross_attention = nn.MultiheadAttention(
            embed_dim=self.fusion_dim,
            num_heads=8,
            dropout=0.1,
            batch_first=True
        )

        # è¾“å‡ºæŠ•å½±
        self.output_proj = nn.Sequential(
            nn.Linear(self.fusion_dim * 2, self.output_dim),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1),
            nn.Linear(self.output_dim, self.output_dim)
        )

    def _build_adaptive_fusion(self):
        """æ„å»ºè‡ªé€‚åº”èåˆ"""
        # ç‰¹å¾æŠ•å½±
        self.clip_proj = nn.Linear(self.clip_dim, self.fusion_dim)
        self.geometry_proj = nn.Linear(self.geometry_dim, self.fusion_dim)

        # è‡ªé€‚åº”æƒé‡ç½‘ç»œ
        self.weight_net = nn.Sequential(
            nn.Linear(self.fusion_dim * 2, self.fusion_dim),
            nn.ReLU(inplace=True),
            nn.Linear(self.fusion_dim, 2),
            nn.Softmax(dim=-1)
        )

        # è¾“å‡ºæŠ•å½±
        self.output_proj = nn.Linear(self.fusion_dim, self.output_dim)

    def _build_simple_concat(self):
        """æ„å»ºç®€å•è¿æ¥èåˆ"""
        self.fusion_proj = nn.Sequential(
            nn.Linear(self.clip_dim + self.geometry_dim, self.fusion_dim),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1),
            nn.Linear(self.fusion_dim, self.output_dim)
        )

    def forward(self, clip_features: torch.Tensor,
                geometry_features: torch.Tensor) -> torch.Tensor:
        """
        èåˆ2Då’Œ3Dç‰¹å¾

        Args:
            clip_features: CLIP 2Dç‰¹å¾ [B, clip_dim]
            geometry_features: å‡ ä½•3Dç‰¹å¾ [B, geometry_dim]

        Returns:
            fused_features: èåˆç‰¹å¾ [B, output_dim]
        """
        if self.fusion_type == "cross_attention":
            return self._cross_attention_forward(clip_features, geometry_features)
        elif self.fusion_type == "adaptive_fusion":
            return self._adaptive_fusion_forward(clip_features, geometry_features)
        elif self.fusion_type == "simple_concat":
            return self._simple_concat_forward(clip_features, geometry_features)

    def _cross_attention_forward(self, clip_feat, geometry_feat):
        # æŠ•å½±
        clip_proj = self.clip_proj(clip_feat).unsqueeze(1)  # [B, 1, fusion_dim]
        geometry_proj = self.geometry_proj(geometry_feat).unsqueeze(1)  # [B, 1, fusion_dim]

        # äº¤å‰æ³¨æ„åŠ›ï¼šCLIPæŸ¥è¯¢å‡ ä½•ç‰¹å¾
        clip_enhanced, _ = self.cross_attention(clip_proj, geometry_proj, geometry_proj)

        # äº¤å‰æ³¨æ„åŠ›ï¼šå‡ ä½•æŸ¥è¯¢CLIPç‰¹å¾
        geometry_enhanced, _ = self.cross_attention(geometry_proj, clip_proj, clip_proj)

        # è¿æ¥å’ŒæŠ•å½±
        fused = torch.cat([clip_enhanced.squeeze(1), geometry_enhanced.squeeze(1)], dim=1)
        return self.output_proj(fused)

    def _adaptive_fusion_forward(self, clip_feat, geometry_feat):
        # æŠ•å½±
        clip_proj = self.clip_proj(clip_feat)
        geometry_proj = self.geometry_proj(geometry_feat)

        # è®¡ç®—è‡ªé€‚åº”æƒé‡
        combined = torch.cat([clip_proj, geometry_proj], dim=1)
        weights = self.weight_net(combined)  # [B, 2]

        # åŠ æƒèåˆ
        w1, w2 = weights[:, 0:1], weights[:, 1:2]
        fused = w1 * clip_proj + w2 * geometry_proj

        return self.output_proj(fused)

    def _simple_concat_forward(self, clip_feat, geometry_feat):
        # ç®€å•è¿æ¥
        concatenated = torch.cat([clip_feat, geometry_feat], dim=1)
        return self.fusion_proj(concatenated)


class AnomalyDetectionHead(nn.Module):
    """
    å¼‚å¸¸æ£€æµ‹å¤´
    åŸºäºèåˆç‰¹å¾è¿›è¡Œå¼‚å¸¸æ£€æµ‹
    """

    def __init__(self,
                 input_dim: int = 512,
                 hidden_dim: int = 256,
                 num_classes: int = 2,  # æ­£å¸¸/å¼‚å¸¸
                 detection_type: str = "classification"):
        super(AnomalyDetectionHead, self).__init__()

        self.detection_type = detection_type
        self.input_dim = input_dim

        if detection_type == "classification":
            self.head = nn.Sequential(
                nn.Linear(input_dim, hidden_dim),
                nn.ReLU(inplace=True),
                nn.Dropout(0.1),
                nn.Linear(hidden_dim, hidden_dim),
                nn.ReLU(inplace=True),
                nn.Dropout(0.1),
                nn.Linear(hidden_dim, num_classes)
            )
        elif detection_type == "regression":
            self.head = nn.Sequential(
                nn.Linear(input_dim, hidden_dim),
                nn.ReLU(inplace=True),
                nn.Dropout(0.1),
                nn.Linear(hidden_dim, hidden_dim),
                nn.ReLU(inplace=True),
                nn.Dropout(0.1),
                nn.Linear(hidden_dim, 1),
                nn.Sigmoid()  # å¼‚å¸¸åˆ†æ•° [0, 1]
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ£€æµ‹ç±»å‹: {detection_type}")

    def forward(self, features: torch.Tensor) -> torch.Tensor:
        """
        å¼‚å¸¸æ£€æµ‹å‰å‘ä¼ æ’­

        Args:
            features: èåˆç‰¹å¾ [B, input_dim]

        Returns:
            predictions: é¢„æµ‹ç»“æœ
                - classification: [B, num_classes]
                - regression: [B, 1]
        """
        return self.head(features)


class GeoCLIP(nn.Module):
    """
    GeoCLIPä¸»æ¨¡å‹
    æ•´åˆ2D CLIPã€3Då‡ ä½•ã€æ·±åº¦ä¿¡æ¯è¿›è¡Œå¼‚å¸¸æ£€æµ‹
    """

    def __init__(self,
                 # 2Dæ¨¡å‹é…ç½®
                 clip_model_name: str = "ViT-B/16",
                 clip_pretrained: str = "openai",

                 # 3Dæ¨¡å‹é…ç½®
                 depth_estimator_type: str = "DPT_Large",
                 geometry_encoder_config: Dict = None,

                 # èåˆé…ç½®
                 fusion_type: str = "cross_attention",
                 fusion_dim: int = 1024,
                 output_dim: int = 512,

                 # å¼‚å¸¸æ£€æµ‹é…ç½®
                 detection_type: str = "regression",
                 num_classes: int = 2,

                 # å…¶ä»–é…ç½®
                 device: str = "cuda",
                 freeze_clip: bool = False):

        super(GeoCLIP, self).__init__()

        self.device = device
        self.freeze_clip = freeze_clip

        # 1. 2D CLIPæ¨¡å‹
        self.clip_model = self._load_clip_model(clip_model_name, clip_pretrained)
        if freeze_clip:
            for param in self.clip_model.parameters():
                param.requires_grad = False

        # 2. æ·±åº¦ä¼°è®¡å™¨
        self.depth_estimator = DepthEstimator(
            model_type=depth_estimator_type,
            device=device
        )

        # 3. æ·±åº¦åˆ°ä½“ç´ è½¬æ¢å™¨
        self.voxel_converter = DepthToVoxelConverter(
            voxel_size=64,
            depth_range=(0.1, 10.0)
        )

        # 4. 3Då‡ ä½•ç¼–ç å™¨
        if geometry_encoder_config is None:
            geometry_encoder_config = {
                'type': 'voxel',
                'in_channels': 4,  # RGB + Depth
                'output_channels': 512
            }

        self.geometry_encoder = create_geometry_encoder(geometry_encoder_config)

        # 5. ç‰¹å¾èåˆæ¨¡å—
        clip_dim = self.clip_model.transformer.width if hasattr(self.clip_model, 'transformer') else 512
        geometry_dim = geometry_encoder_config['output_channels']

        self.fusion_module = FeatureFusionModule(
            clip_dim=clip_dim,
            geometry_dim=geometry_dim,
            fusion_dim=fusion_dim,
            output_dim=output_dim,
            fusion_type=fusion_type
        )

        # 6. å¼‚å¸¸æ£€æµ‹å¤´
        self.anomaly_head = AnomalyDetectionHead(
            input_dim=output_dim,
            detection_type=detection_type,
            num_classes=num_classes
        )

        print(f"GeoCLIPæ¨¡å‹åˆå§‹åŒ–å®Œæˆ:")
        print(f"  CLIPæ¨¡å‹: {clip_model_name}")
        print(f"  æ·±åº¦ä¼°è®¡: {depth_estimator_type}")
        print(f"  å‡ ä½•ç¼–ç : {geometry_encoder_config['type']}")
        print(f"  èåˆæ–¹å¼: {fusion_type}")
        print(f"  æ£€æµ‹ç±»å‹: {detection_type}")

    def _load_clip_model(self, model_name: str, pretrained: str):
        """åŠ è½½CLIPæ¨¡å‹"""
        try:
            # å°è¯•ä½¿ç”¨AdaCLIPçš„åŠ è½½æ–¹å¼
            clip_model = load_clip_model(model_name, pretrained)
            return clip_model
        except:
            # å›é€€åˆ°æ ‡å‡†CLIP
            try:
                import clip
                model, preprocess = clip.load(model_name, device=self.device)
                return model
            except:
                raise RuntimeError("æ— æ³•åŠ è½½CLIPæ¨¡å‹ï¼Œè¯·æ£€æŸ¥å®‰è£…")

    def forward(self, batch: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """
        GeoCLIPå‰å‘ä¼ æ’­

        Args:
            batch: æ‰¹æ¬¡æ•°æ®ï¼ŒåŒ…å«ï¼š
                - 'image': RGBå›¾åƒ [B, 3, H, W]
                - 'depth': æ·±åº¦å›¾ [B, 1, H, W] (å¯é€‰ï¼Œå¦‚æœæ²¡æœ‰ä¼šè‡ªåŠ¨ä¼°è®¡)
                - 'text': æ–‡æœ¬token (å¯é€‰)

        Returns:
            è¾“å‡ºå­—å…¸ï¼ŒåŒ…å«ï¼š
                - 'anomaly_score': å¼‚å¸¸åˆ†æ•°
                - 'clip_features': CLIPç‰¹å¾
                - 'geometry_features': å‡ ä½•ç‰¹å¾
                - 'fused_features': èåˆç‰¹å¾
        """
        images = batch['image']
        batch_size = images.size(0)

        # 1. è·å–æ·±åº¦å›¾
        if 'depth' in batch and batch['depth'] is not None:
            depth_maps = batch['depth']
        else:
            # ä½¿ç”¨æ·±åº¦ä¼°è®¡å™¨ä¼°è®¡æ·±åº¦
            with torch.no_grad():
                depth_maps = self.depth_estimator(images)

        # 2. CLIPç‰¹å¾æå–
        if 'text' in batch:
            # å¦‚æœæœ‰æ–‡æœ¬ï¼Œä½¿ç”¨å›¾åƒ-æ–‡æœ¬å¯¹æ¯”
            clip_features = self.clip_model.encode_image(images)
        else:
            # ä»…ä½¿ç”¨å›¾åƒç¼–ç 
            clip_features = self.clip_model.encode_image(images)

        # 3. è½¬æ¢ä¸ºä½“ç´ è¡¨ç¤º
        # åˆå¹¶RGBå’Œæ·±åº¦ä¿¡æ¯
        rgbd_images = torch.cat([images, depth_maps], dim=1)  # [B, 4, H, W]

        # è½¬æ¢ä¸º3Dä½“ç´ 
        voxels = self.voxel_converter.images_to_voxels(rgbd_images)  # [B, 4, D, H, W]

        # 4. 3Då‡ ä½•ç‰¹å¾æå–
        geometry_features = self.geometry_encoder(voxels)

        # 5. ç‰¹å¾èåˆ
        fused_features = self.fusion_module(clip_features, geometry_features)

        # 6. å¼‚å¸¸æ£€æµ‹
        anomaly_predictions = self.anomaly_head(fused_features)

        # è¿”å›ç»“æœ
        results = {
            'anomaly_predictions': anomaly_predictions,
            'clip_features': clip_features,
            'geometry_features': geometry_features,
            'fused_features': fused_features,
            'depth_maps': depth_maps
        }

        return results

    def predict_anomaly(self, batch: Dict[str, torch.Tensor]) -> torch.Tensor:
        """
        é¢„æµ‹å¼‚å¸¸åˆ†æ•°

        Args:
            batch: è¾“å…¥æ‰¹æ¬¡

        Returns:
            anomaly_scores: å¼‚å¸¸åˆ†æ•° [B] æˆ– [B, num_classes]
        """
        with torch.no_grad():
            results = self.forward(batch)
            predictions = results['anomaly_predictions']

            if self.anomaly_head.detection_type == "regression":
                return predictions.squeeze(-1)  # [B]
            else:
                # åˆ†ç±»æƒ…å†µï¼Œè¿”å›å¼‚å¸¸ç±»åˆ«çš„æ¦‚ç‡
                return F.softmax(predictions, dim=-1)[:, 1]  # [B]

    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)

        return {
            'total_parameters': total_params,
            'trainable_parameters': trainable_params,
            'clip_frozen': self.freeze_clip,
            'fusion_type': self.fusion_module.fusion_type,
            'detection_type': self.anomaly_head.detection_type,
            'device': self.device
        }


def create_geoclip_model(config: Dict[str, Any]) -> GeoCLIP:
    """
    å·¥å‚å‡½æ•°ï¼šæ ¹æ®é…ç½®åˆ›å»ºGeoCLIPæ¨¡å‹

    Args:
        config: æ¨¡å‹é…ç½®å­—å…¸

    Returns:
        GeoCLIPæ¨¡å‹å®ä¾‹
    """
    return GeoCLIP(
        clip_model_name=config.get('clip_model', 'ViT-B/16'),
        clip_pretrained=config.get('clip_pretrained', 'openai'),
        depth_estimator_type=config.get('depth_estimator', 'DPT_Large'),
        geometry_encoder_config=config.get('geometry_encoder', None),
        fusion_type=config.get('fusion_type', 'cross_attention'),
        fusion_dim=config.get('fusion_dim', 1024),
        output_dim=config.get('output_dim', 512),
        detection_type=config.get('detection_type', 'regression'),
        num_classes=config.get('num_classes', 2),
        device=config.get('device', 'cuda'),
        freeze_clip=config.get('freeze_clip', False)
    )


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    print("=== æµ‹è¯•GeoCLIPæ¨¡å‹ ===")

    try:
        # åˆ›å»ºæ¨¡å‹é…ç½®
        config = {
            'clip_model': 'ViT-B/16',
            'depth_estimator': 'DPT_Large',
            'fusion_type': 'cross_attention',
            'detection_type': 'regression',
            'device': 'cuda' if torch.cuda.is_available() else 'cpu'
        }

        # åˆ›å»ºæ¨¡å‹
        model = create_geoclip_model(config)
        print(f"âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ")
        print(f"æ¨¡å‹ä¿¡æ¯: {model.get_model_info()}")

        # åˆ›å»ºæµ‹è¯•æ•°æ®
        batch_size = 2
        test_batch = {
            'image': torch.randn(batch_size, 3, 224, 224),
            # 'depth': torch.randn(batch_size, 1, 224, 224),  # å¯é€‰
        }

        # ç§»åŠ¨åˆ°è®¾å¤‡
        device = config['device']
        model = model.to(device)
        for key in test_batch:
            test_batch[key] = test_batch[key].to(device)

        # å‰å‘ä¼ æ’­æµ‹è¯•
        print("\næµ‹è¯•å‰å‘ä¼ æ’­...")
        results = model(test_batch)

        print(f"âœ… å‰å‘ä¼ æ’­æˆåŠŸ:")
        for key, value in results.items():
            if isinstance(value, torch.Tensor):
                print(f"  {key}: {value.shape}")

        # å¼‚å¸¸é¢„æµ‹æµ‹è¯•
        anomaly_scores = model.predict_anomaly(test_batch)
        print(f"âœ… å¼‚å¸¸é¢„æµ‹: {anomaly_scores.shape}")
        print(f"å¼‚å¸¸åˆ†æ•°èŒƒå›´: {anomaly_scores.min():.3f} - {anomaly_scores.max():.3f}")

        print("\nğŸ‰ GeoCLIPæ¨¡å‹æµ‹è¯•å®Œæˆ!")

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()