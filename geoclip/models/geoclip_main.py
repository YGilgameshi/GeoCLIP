"""
GeoCLIP - ä¸»æ¨¡å‹
æ•´åˆ2D CLIPç‰¹å¾ã€3Då‡ ä½•ç‰¹å¾å’Œæ·±åº¦ä¿¡æ¯è¿›è¡Œå¼‚å¸¸æ£€æµ‹
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Optional, Tuple, Union, Any
import numpy as np
import open_clip

# å¯¼å…¥GeoCLIPç»„ä»¶
from geoclip.models.depth_estimator import DepthEstimator
from geoclip.models.geometry_encoder import create_geometry_encoder, VoxelEncoder
from geoclip.utils.voxel_utils import DepthToVoxelConverter


class FeatureFusionModule(nn.Module):
    """
    2D CLIPç‰¹å¾ä¸3Då‡ ä½•ç‰¹å¾èåˆæ¨¡å—
    """

    def __init__(self,
                 clip_dim: int = 512,
                 geometry_dim: int = 512,
                 fusion_dim: int = 1024,
                 output_dim: int = 512,
                 fusion_type: str = "cross_attention"):
        super(FeatureFusionModule, self).__init__()

        self.fusion_type = fusion_type
        self.clip_dim = clip_dim
        self.geometry_dim = geometry_dim
        self.fusion_dim = fusion_dim
        self.output_dim = output_dim

        if fusion_type == "cross_attention":
            self._build_cross_attention()
        elif fusion_type == "adaptive_fusion":
            self._build_adaptive_fusion()
        elif fusion_type == "simple_concat":
            self._build_simple_concat()
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„èåˆç±»å‹: {fusion_type}")

    def _build_cross_attention(self):
        """æ„å»ºäº¤å‰æ³¨æ„åŠ›èåˆ"""
        # æŠ•å½±åˆ°ç›¸åŒç»´åº¦
        self.clip_proj = nn.Linear(self.clip_dim, self.fusion_dim)
        self.geometry_proj = nn.Linear(self.geometry_dim, self.fusion_dim)

        # äº¤å‰æ³¨æ„åŠ›
        self.cross_attention = nn.MultiheadAttention(
            embed_dim=self.fusion_dim,
            num_heads=8,
            dropout=0.1,
            batch_first=True
        )

        # è¾“å‡ºæŠ•å½±
        self.output_proj = nn.Sequential(
            nn.Linear(self.fusion_dim * 2, self.output_dim),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1),
            nn.Linear(self.output_dim, self.output_dim)
        )

    def _build_adaptive_fusion(self):
        """æ„å»ºè‡ªé€‚åº”èåˆ"""
        # ç‰¹å¾æŠ•å½±
        self.clip_proj = nn.Linear(self.clip_dim, self.fusion_dim)
        self.geometry_proj = nn.Linear(self.geometry_dim, self.fusion_dim)

        # è‡ªé€‚åº”æƒé‡ç½‘ç»œ
        self.weight_net = nn.Sequential(
            nn.Linear(self.fusion_dim * 2, self.fusion_dim),
            nn.ReLU(inplace=True),
            nn.Linear(self.fusion_dim, 2),
            nn.Softmax(dim=-1)
        )

        # è¾“å‡ºæŠ•å½±
        self.output_proj = nn.Linear(self.fusion_dim, self.output_dim)

    def _build_simple_concat(self):
        """æ„å»ºç®€å•è¿æ¥èåˆ"""
        self.fusion_proj = nn.Sequential(
            nn.Linear(self.clip_dim + self.geometry_dim, self.fusion_dim),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1),
            nn.Linear(self.fusion_dim, self.output_dim)
        )

    def forward(self, clip_features: torch.Tensor,
                geometry_features: torch.Tensor) -> torch.Tensor:
        """
        èåˆ2Då’Œ3Dç‰¹å¾

        Args:
            clip_features: CLIP 2Dç‰¹å¾ [B, clip_dim]
            geometry_features: å‡ ä½•3Dç‰¹å¾ [B, geometry_dim]

        Returns:
            fused_features: èåˆç‰¹å¾ [B, output_dim]
        """
        if self.fusion_type == "cross_attention":
            return self._cross_attention_forward(clip_features, geometry_features)
        elif self.fusion_type == "adaptive_fusion":
            return self._adaptive_fusion_forward(clip_features, geometry_features)
        elif self.fusion_type == "simple_concat":
            return self._simple_concat_forward(clip_features, geometry_features)

    # æŠ•å½±
    def _cross_attention_forward(self, clip_feat, geometry_feat):
        clip_proj = self.clip_proj(clip_feat).unsqueeze(1)  # [B, 1, fusion_dim]
        geometry_proj = self.geometry_proj(geometry_feat).unsqueeze(1)  # [B, 1, fusion_dim]

        # äº¤å‰æ³¨æ„åŠ›ï¼šCLIPæŸ¥è¯¢å‡ ä½•ç‰¹å¾
        clip_enhanced, _ = self.cross_attention(clip_proj, geometry_proj, geometry_proj)

        # äº¤å‰æ³¨æ„åŠ›ï¼šå‡ ä½•æŸ¥è¯¢CLIPç‰¹å¾
        geometry_enhanced, _ = self.cross_attention(geometry_proj, clip_proj, clip_proj)

        # è¿æ¥å’ŒæŠ•å½±
        fused = torch.cat([clip_enhanced.squeeze(1), geometry_enhanced.squeeze(1)], dim=1)
        return self.output_proj(fused)

    def _adaptive_fusion_forward(self, clip_feat, geometry_feat):
        # æŠ•å½±
        clip_proj = self.clip_proj(clip_feat)
        geometry_proj = self.geometry_proj(geometry_feat)

        # è®¡ç®—è‡ªé€‚åº”æƒé‡
        combined = torch.cat([clip_proj, geometry_proj], dim=1)
        weights = self.weight_net(combined)  # [B, 2]

        # åŠ æƒèåˆ
        w1, w2 = weights[:, 0:1], weights[:, 1:2]
        fused = w1 * clip_proj + w2 * geometry_proj

        return self.output_proj(fused)

    def _simple_concat_forward(self, clip_feat, geometry_feat):
        # ç®€å•è¿æ¥
        concatenated = torch.cat([clip_feat, geometry_feat], dim=1)
        return self.fusion_proj(concatenated)


class AnomalyDetectionHead(nn.Module):
    """
    å¼‚å¸¸æ£€æµ‹å¤´
    åŸºäºèåˆç‰¹å¾è¿›è¡Œå¼‚å¸¸æ£€æµ‹
    """

    def __init__(self,
                 input_dim: int = 512,
                 hidden_dim: int = 256,
                 num_classes: int = 2,  # æ­£å¸¸/å¼‚å¸¸
                 detection_type: str = "classification"):
        super(AnomalyDetectionHead, self).__init__()

        self.detection_type = detection_type
        self.input_dim = input_dim

        if detection_type == "classification":
            self.head = nn.Sequential(
                nn.Linear(input_dim, hidden_dim),
                nn.ReLU(inplace=True),
                nn.Dropout(0.1),
                nn.Linear(hidden_dim, hidden_dim),
                nn.ReLU(inplace=True),
                nn.Dropout(0.1),
                nn.Linear(hidden_dim, num_classes)
            )
        elif detection_type == "regression":
            self.head = nn.Sequential(
                nn.Linear(input_dim, hidden_dim),
                nn.ReLU(inplace=True),
                nn.Dropout(0.1),
                nn.Linear(hidden_dim, hidden_dim),
                nn.ReLU(inplace=True),
                nn.Dropout(0.1),
                nn.Linear(hidden_dim, 1),
                nn.Sigmoid()  # å¼‚å¸¸åˆ†æ•° [0, 1]
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ£€æµ‹ç±»å‹: {detection_type}")

    def forward(self, features: torch.Tensor) -> torch.Tensor:
        """
        å¼‚å¸¸æ£€æµ‹å‰å‘ä¼ æ’­

        Args:
            features: èåˆç‰¹å¾ [B, input_dim]

        Returns:
            predictions: é¢„æµ‹ç»“æœ
                - classification: [B, num_classes]
                - regression: [B, 1]
        """
        return self.head(features)


class GeoCLIP(nn.Module):
    def __init__(self,
                 clip_model_name: str = "ViT-B/16",
                 clip_pretrained: str = "openai",
                 depth_estimator_type: str = "DPT_Large",
                 geometry_encoder_config: Dict = None,
                 fusion_type: str = "cross_attention",
                 fusion_dim: int = 1024,
                 output_dim: int = 512,
                 detection_type: str = "regression",
                 num_classes: int = 2,
                 device: str = "cuda",
                 freeze_clip: bool = False):

        super(GeoCLIP, self).__init__()

        # ========== å…³é”®ï¼šç»Ÿä¸€è®¾å¤‡ç®¡ç† ==========
        self.device = torch.device(device)
        self.freeze_clip = freeze_clip

        print(f"ğŸ”§ GeoCLIPåˆå§‹åŒ–ï¼Œç›®æ ‡è®¾å¤‡: {self.device}")

        # 1. CLIPæ¨¡å‹ï¼ˆå¯èƒ½å›ºå®šåœ¨cuda:0ï¼‰
        print("1ï¸âƒ£ åŠ è½½CLIPæ¨¡å‹...")
        self.clip_model = self._load_clip_model(clip_model_name, clip_pretrained)
        if freeze_clip:
            for param in self.clip_model.parameters():
                param.requires_grad = False

        # æ£€æµ‹CLIPå®é™…è®¾å¤‡
        if hasattr(self.clip_model, 'device'):
            clip_device = self.clip_model.device
            print(f"   CLIPè®¾å¤‡: {clip_device}")

        # æ£€æµ‹CLIPè¾“å…¥å°ºå¯¸
        if hasattr(self.clip_model, 'input_size'):
            self.clip_input_size = self.clip_model.input_size
        else:
            self.clip_input_size = 224
        print(f"   CLIPè¾“å…¥å°ºå¯¸: {self.clip_input_size}x{self.clip_input_size}")

        # 2. æ·±åº¦ä¼°è®¡å™¨ - æ˜ç¡®è®¾ç½®è®¾å¤‡
        print("2ï¸âƒ£ åŠ è½½æ·±åº¦ä¼°è®¡å™¨...")
        self.depth_input_size = 384
        self.depth_estimator = DepthEstimator(
            model_type=depth_estimator_type,
            device=str(self.device),  # ç¡®ä¿ä¼ é€’å­—ç¬¦ä¸²æ ¼å¼
            input_size=(self.depth_input_size, self.depth_input_size)
        )
        # ç¡®ä¿æ·±åº¦ä¼°è®¡å™¨åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        self.depth_estimator = self.depth_estimator.to(self.device)
        print(f"   æ·±åº¦ä¼°è®¡å™¨è®¾å¤‡: {next(self.depth_estimator.parameters()).device}")

        # 3. ä½“ç´ è½¬æ¢å™¨ - ä¸æ˜¯nn.Moduleï¼Œéœ€è¦æ‰‹åŠ¨ç®¡ç†è®¾å¤‡
        print("3ï¸âƒ£ åˆå§‹åŒ–ä½“ç´ è½¬æ¢å™¨...")
        from geoclip.utils.voxel_utils import DepthToVoxelConverter
        self.voxel_converter = DepthToVoxelConverter(
            voxel_size=64,
            depth_range=(0.1, 10.0),
            use_color=True
        )
        # ä½“ç´ è½¬æ¢å™¨æ˜¯å·¥å…·ç±»ï¼Œåœ¨forwardä¸­å¤„ç†è®¾å¤‡

        # 4. å‡ ä½•ç¼–ç å™¨ - ç¡®ä¿åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        print("4ï¸âƒ£ åˆå§‹åŒ–å‡ ä½•ç¼–ç å™¨...")
        if geometry_encoder_config is None:
            geometry_encoder_config = {
                'type': 'voxel',
                'in_channels': 4,
                'output_channels': 512
            }

        from geoclip.models.geometry_encoder import create_geometry_encoder
        self.geometry_encoder = create_geometry_encoder(geometry_encoder_config)
        self.geometry_encoder = self.geometry_encoder.to(self.device)
        print(f"   å‡ ä½•ç¼–ç å™¨è®¾å¤‡: {next(self.geometry_encoder.parameters()).device}")

        # 5. ç‰¹å¾èåˆæ¨¡å— - ç¡®ä¿åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        print("5ï¸âƒ£ åˆå§‹åŒ–ç‰¹å¾èåˆæ¨¡å—...")
        clip_dim = 512
        geometry_dim = geometry_encoder_config['output_channels']

        from geoclip.models.fusion_module import FeatureFusionModule
        self.fusion_module = FeatureFusionModule(
            clip_dim=clip_dim,
            geometry_dim=geometry_dim,
            fusion_dim=fusion_dim,
            output_dim=output_dim,
            fusion_type=fusion_type
        )
        self.fusion_module = self.fusion_module.to(self.device)
        print(f"   èåˆæ¨¡å—è®¾å¤‡: {next(self.fusion_module.parameters()).device}")

        # 6. å¼‚å¸¸æ£€æµ‹å¤´ - ç¡®ä¿åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        print("6ï¸âƒ£ åˆå§‹åŒ–å¼‚å¸¸æ£€æµ‹å¤´...")

        self.anomaly_head = AnomalyDetectionHead(
            input_dim=output_dim,
            detection_type=detection_type,
            num_classes=num_classes
        )
        self.anomaly_head = self.anomaly_head.to(self.device)
        print(f"   æ£€æµ‹å¤´è®¾å¤‡: {next(self.anomaly_head.parameters()).device}")

        print(f"âœ… GeoCLIPæ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
        print(f"   ç‰¹å¾ç»´åº¦ - CLIP: {clip_dim}, å‡ ä½•: {geometry_dim}")

        print(f"GeoCLIPæ¨¡å‹åˆå§‹åŒ–å®Œæˆ:")
        print(f"  CLIPæ¨¡å‹: {clip_model_name}")
        print(f"  æ·±åº¦ä¼°è®¡: {depth_estimator_type}")
        print(f"  å‡ ä½•ç¼–ç : {geometry_encoder_config['type']}")
        print(f"  èåˆæ–¹å¼: {fusion_type}")
        print(f"  æ£€æµ‹ç±»å‹: {detection_type}")


    # def _load_clip_model(self, model_name: str, pretrained: str):
    #     """
    #     åŠ è½½CLIPæ¨¡å‹ - ä¼˜å…ˆä»æœ¬åœ°ç¼“å­˜åŠ è½½TorchScriptæ¨¡å‹
    #     æ³¨æ„: TorchScriptæ¨¡å‹å›ºå®šåœ¨cuda:0ä¸Š
    #     """
    #     import open_clip
    #     import torch
    #     from pathlib import Path
    #     from torchvision import transforms
    #
    #     model_name_converted = model_name.replace('/', '-')
    #     cache_dir = Path.home() / '.cache' / 'open_clip'
    #     cache_dir.mkdir(parents=True, exist_ok=True)
    #
    #     print(f"æ­£åœ¨ä»æœ¬åœ°åŠ è½½CLIPæ¨¡å‹: {model_name_converted}")
    #
    #     # å®šä¹‰åˆ›å»ºé¢„å¤„ç†çš„å‡½æ•°
    #     def create_preprocess(size):
    #         return transforms.Compose([
    #             transforms.Resize(size, interpolation=transforms.InterpolationMode.BICUBIC),
    #             transforms.CenterCrop(size),
    #             transforms.ToTensor(),
    #             transforms.Normalize(
    #                 mean=[0.48145466, 0.4578275, 0.40821073],
    #                 std=[0.26862954, 0.26130258, 0.27577711]
    #             )
    #         ])
    #
    #     # åˆå§‹é»˜è®¤é¢„å¤„ç†
    #     standard_preprocess = create_preprocess(224)
    #
    #     # ========== 1. ä¼˜å…ˆåŠ è½½æœ¬åœ°TorchScriptæ¨¡å‹ ==========
    #     pt_files = list(cache_dir.glob(f"*{model_name_converted}*.pt"))
    #     if not pt_files:
    #         pt_files = list(cache_dir.glob("*.pt"))
    #
    #     if pt_files:
    #         print(f"å‘ç°æœ¬åœ°TorchScriptæ¨¡å‹: {pt_files[0]}")
    #
    #         # æ£€æŸ¥GPUå¯ç”¨æ€§
    #         if not torch.cuda.is_available():
    #             print("âŒ TorchScriptæ¨¡å‹éœ€è¦GPUï¼Œä½†ç³»ç»Ÿæ— GPUå¯ç”¨")
    #             print("è·³è¿‡TorchScriptåŠ è½½ï¼Œå°è¯•å…¶ä»–æ–¹å¼...")
    #         else:
    #             try:
    #                 print("âš ï¸ TorchScriptæ¨¡å‹å›ºå®šåœ¨cuda:0")
    #                 print("æ­£åœ¨åŠ è½½æ¨¡å‹åˆ°cuda:0...")
    #
    #                 # å¼ºåˆ¶åŠ è½½åˆ°cuda:0
    #                 jit_model = torch.jit.load(str(pt_files[0]), map_location='cuda:0')
    #                 jit_model = jit_model.cuda(0)
    #                 jit_model.eval()
    #
    #                 print("âœ“ TorchScriptæ¨¡å‹å·²åŠ è½½åˆ°cuda:0")
    #
    #                 # ========== æ£€æµ‹æ¨¡å‹è¾“å…¥å°ºå¯¸ ==========
    #                 input_size = 224  # é»˜è®¤å€¼
    #                 print("æ­£åœ¨æ£€æµ‹æ¨¡å‹è¾“å…¥å°ºå¯¸...")
    #
    #                 # å°è¯•224
    #                 try:
    #                     test_input = torch.randn(1, 3, 224, 224).cuda(0)
    #                     with torch.no_grad():
    #                         jit_model.encode_image(test_input)
    #                     input_size = 224
    #                     print("âœ“ æ£€æµ‹åˆ°æ¨¡å‹è¾“å…¥å°ºå¯¸: 224x224")
    #                 except Exception as e1:
    #                     # å°è¯•336
    #                     try:
    #                         test_input = torch.randn(1, 3, 336, 336).cuda(0)
    #                         with torch.no_grad():
    #                             jit_model.encode_image(test_input)
    #                         input_size = 336
    #                         print("âœ“ æ£€æµ‹åˆ°æ¨¡å‹è¾“å…¥å°ºå¯¸: 336x336")
    #                     except Exception as e2:
    #                         # å°è¯•384
    #                         try:
    #                             test_input = torch.randn(1, 3, 384, 384).cuda(0)
    #                             with torch.no_grad():
    #                                 jit_model.encode_image(test_input)
    #                             input_size = 384
    #                             print("âœ“ æ£€æµ‹åˆ°æ¨¡å‹è¾“å…¥å°ºå¯¸: 384x384")
    #                         except Exception as e3:
    #                             print(f"âš ï¸ æ— æ³•è‡ªåŠ¨æ£€æµ‹è¾“å…¥å°ºå¯¸")
    #                             print(f"  224é”™è¯¯: {str(e1)[:100]}")
    #                             print(f"  336é”™è¯¯: {str(e2)[:100]}")
    #                             print(f"  384é”™è¯¯: {str(e3)[:100]}")
    #                             print("ä½¿ç”¨é»˜è®¤224")
    #                             input_size = 224
    #
    #                 # æ ¹æ®æ£€æµ‹åˆ°çš„å°ºå¯¸åˆ›å»ºé¢„å¤„ç†
    #                 standard_preprocess = create_preprocess(input_size)
    #
    #                 # ========== TorchScriptåŒ…è£…å™¨ ==========
    #                 class TorchScriptWrapper(torch.nn.Module):
    #                     """TorchScriptæ¨¡å‹åŒ…è£…å™¨ - å›ºå®šcuda:0"""
    #
    #                     def __init__(self, jit_model, input_size):
    #                         super().__init__()
    #                         self.model = jit_model
    #                         self.device = torch.device('cuda:0')
    #                         self.input_size = input_size
    #                         self.width = 768
    #                         self.visual = self
    #
    #                         class TransformerProxy:
    #                             def __init__(self, width):
    #                                 self.width = width
    #
    #                         self.transformer = TransformerProxy(self.width)
    #
    #                     def encode_image(self, image):
    #                         """å›¾åƒç¼–ç  - è‡ªåŠ¨å¤„ç†è®¾å¤‡å’Œå°ºå¯¸"""
    #                         # 1. ç¡®ä¿åœ¨cuda:0
    #                         if not image.is_cuda:
    #                             image = image.cuda(0)
    #                         elif image.device.index != 0:
    #                             image = image.cuda(0)
    #
    #                         # 2. æ£€æŸ¥å¹¶è°ƒæ•´å°ºå¯¸ï¼ˆå…³é”®ä¿®å¤ï¼‰
    #                         current_size = image.shape[-2:]
    #                         if current_size != (self.input_size, self.input_size):
    #                             print(f"âš ï¸ è¾“å…¥å°ºå¯¸{current_size}ä¸åŒ¹é…ï¼ŒæœŸæœ›{self.input_size}x{self.input_size}")
    #                             print(f"   è¿™è¯´æ˜é¢„å¤„ç†æ²¡æœ‰æ­£ç¡®åº”ç”¨ï¼Œè¯·æ£€æŸ¥æ•°æ®åŠ è½½æµç¨‹")
    #                             # ç´§æ€¥resize
    #                             import torch.nn.functional as F
    #                             image = F.interpolate(
    #                                 image,
    #                                 size=(self.input_size, self.input_size),
    #                                 mode='bicubic',
    #                                 align_corners=False
    #                             )
    #                             print(f"âœ“ å·²ç´§æ€¥è°ƒæ•´åˆ°{self.input_size}x{self.input_size}")
    #
    #                         # 3. è°ƒç”¨æ¨¡å‹
    #                         try:
    #                             if hasattr(self.model, 'encode_image'):
    #                                 return self.model.encode_image(image)
    #                             else:
    #                                 return self.model(image)
    #                         except RuntimeError as e:
    #                             error_msg = str(e)
    #                             if "577" in error_msg and "197" in error_msg:
    #                                 print(f"âŒ Tokenæ•°é‡ä¸åŒ¹é…é”™è¯¯!")
    #                                 print(f"   è¿™æ„å‘³ç€è¾“å…¥å°ºå¯¸ä»ç„¶ä¸æ­£ç¡®")
    #                                 print(f"   æœŸæœ›è¾“å…¥: {self.input_size}x{self.input_size}")
    #                                 print(f"   å®é™…è¾“å…¥: {image.shape}")
    #                                 print(f"   æ¨¡å‹å¯èƒ½åœ¨224x224ä¸Šè®­ç»ƒï¼Œä½†æ”¶åˆ°äº†æ›´å¤§çš„è¾“å…¥")
    #                             raise
    #
    #                     def encode_text(self, text):
    #                         """æ–‡æœ¬ç¼–ç """
    #                         if not text.is_cuda:
    #                             text = text.cuda(0)
    #                         elif text.device.index != 0:
    #                             text = text.cuda(0)
    #
    #                         if hasattr(self.model, 'encode_text'):
    #                             return self.model.encode_text(text)
    #                         else:
    #                             raise AttributeError("TorchScriptæ¨¡å‹æ²¡æœ‰encode_textæ–¹æ³•")
    #
    #                     def forward(self, image):
    #                         return self.encode_image(image)
    #
    #                     def __call__(self, *args, **kwargs):
    #                         if len(args) == 1 and isinstance(args[0], torch.Tensor):
    #                             return self.forward(args[0])
    #                         return self.model(*args, **kwargs)
    #
    #                     def to(self, device):
    #                         """å›ºå®šcuda:0ï¼Œå¿½ç•¥å…¶ä»–è¯·æ±‚"""
    #                         if str(device) not in ['cuda:0', 'cuda']:
    #                             print(f"âš ï¸ TorchScriptæ¨¡å‹å›ºå®šåœ¨cuda:0ï¼Œå¿½ç•¥to({device})")
    #                         return self
    #
    #                     def cuda(self, device=None):
    #                         if device is not None and device != 0:
    #                             print(f"âš ï¸ TorchScriptæ¨¡å‹å›ºå®šåœ¨cuda:0ï¼Œå¿½ç•¥cuda({device})")
    #                         return self
    #
    #                     def cpu(self):
    #                         print(f"âš ï¸ TorchScriptæ¨¡å‹å›ºå®šåœ¨cuda:0ï¼Œä¸æ”¯æŒè½¬åˆ°CPU")
    #                         return self
    #
    #                     def eval(self):
    #                         self.model.eval()
    #                         return self
    #
    #                     def train(self, mode=True):
    #                         if mode:
    #                             self.model.train()
    #                         else:
    #                             self.model.eval()
    #                         return self
    #
    #                     def parameters(self):
    #                         return self.model.parameters()
    #
    #                     def __getattr__(self, name):
    #                         try:
    #                             return super().__getattr__(name)
    #                         except AttributeError:
    #                             try:
    #                                 return getattr(self.model, name)
    #                             except AttributeError:
    #                                 raise AttributeError(f"TorchScriptæ¨¡å‹æ²¡æœ‰å±æ€§: {name}")
    #
    #                 # åˆ›å»ºåŒ…è£…å™¨
    #                 model = TorchScriptWrapper(jit_model, input_size)
    #                 self.preprocess = standard_preprocess
    #
    #                 print("âœ“ æˆåŠŸåŠ è½½TorchScriptæ¨¡å‹")
    #                 print(f"âœ“ æ¨¡å‹å›ºå®šåœ¨: cuda:0")
    #                 print(f"âœ“ è¾“å…¥å°ºå¯¸: {input_size}x{input_size}")
    #                 print(f"âœ“ ç‰¹å¾ç»´åº¦: {model.width}")
    #                 print(f"âš ï¸ é‡è¦: è¯·ç¡®ä¿æ•°æ®é¢„å¤„ç†ä½¿ç”¨{input_size}x{input_size}å°ºå¯¸")
    #
    #                 return model
    #
    #             except Exception as e:
    #                 print(f"âœ— åŠ è½½TorchScriptæ¨¡å‹å¤±è´¥: {e}")
    #                 import traceback
    #                 traceback.print_exc()
    #                 print("å°è¯•å…¶ä»–åŠ è½½æ–¹å¼...")
    #
    #     # ========== 2. ä½¿ç”¨open_clipåŠ è½½ ==========
    #     try:
    #         print("å°è¯•ä½¿ç”¨open_clipåŠ è½½é¢„è®­ç»ƒæ¨¡å‹...")
    #         model, _, preprocess = open_clip.create_model_and_transforms(
    #             model_name_converted,
    #             pretrained=pretrained if pretrained != 'openai' else 'openai',
    #             cache_dir=str(cache_dir)
    #         )
    #         model = model.to(self.device)
    #         model.eval()
    #         self.preprocess = preprocess
    #         print(f"âœ“ æˆåŠŸåŠ è½½open_clipé¢„è®­ç»ƒæ¨¡å‹")
    #         return model
    #     except Exception as e:
    #         print(f"âœ— open_clipåŠ è½½å¤±è´¥: {e}")
    #
    #     # ========== 3. åˆ›å»ºæ— é¢„è®­ç»ƒæ¨¡å‹ ==========
    #     print("âš ï¸ å›é€€åˆ°æ— é¢„è®­ç»ƒæƒé‡æ¨¡å‹")
    #     try:
    #         model, _, preprocess = open_clip.create_model_and_transforms(
    #             model_name_converted,
    #             pretrained=None
    #         )
    #         model = model.to(self.device)
    #         model.eval()
    #         self.preprocess = preprocess if preprocess is not None else standard_preprocess
    #         print(f"âœ“ æˆåŠŸåˆ›å»ºæ— é¢„è®­ç»ƒæƒé‡æ¨¡å‹")
    #         print("âš ï¸ æ­¤æ¨¡å‹éœ€è¦é‡æ–°è®­ç»ƒ")
    #         return model
    #     except Exception as e:
    #         print(f"âœ— åˆ›å»ºæ¨¡å‹å¤±è´¥: {e}")
    #         raise RuntimeError(f"æ— æ³•åŠ è½½æˆ–åˆ›å»ºCLIPæ¨¡å‹: {e}")

    def _load_clip_model(self, model_name: str, pretrained: str):
        """
        åŠ è½½CLIPæ¨¡å‹ - ä¼˜å…ˆä»æœ¬åœ°ç¼“å­˜åŠ è½½TorchScriptæ¨¡å‹
        æ³¨æ„: TorchScriptæ¨¡å‹å›ºå®šåœ¨cuda:0ä¸Š
        """
        import open_clip
        import torch
        from pathlib import Path
        from torchvision import transforms

        model_name_converted = model_name.replace('/', '-')
        cache_dir = Path.home() / '.cache' / 'open_clip'
        cache_dir.mkdir(parents=True, exist_ok=True)

        print(f"æ­£åœ¨ä»æœ¬åœ°åŠ è½½CLIPæ¨¡å‹: {model_name_converted}")

        # å®šä¹‰åˆ›å»ºé¢„å¤„ç†çš„å‡½æ•°
        def create_preprocess(size):
            return transforms.Compose([
                transforms.Resize(size, interpolation=transforms.InterpolationMode.BICUBIC),
                transforms.CenterCrop(size),
                transforms.ToTensor(),
                transforms.Normalize(
                    mean=[0.48145466, 0.4578275, 0.40821073],
                    std=[0.26862954, 0.26130258, 0.27577711]
                )
            ])

        # åˆå§‹é»˜è®¤é¢„å¤„ç†
        standard_preprocess = create_preprocess(224)

        # ========== 1. ä¼˜å…ˆåŠ è½½æœ¬åœ°TorchScriptæ¨¡å‹ ==========
        pt_files = list(cache_dir.glob(f"*{model_name_converted}*.pt"))
        if not pt_files:
            pt_files = list(cache_dir.glob("*.pt"))

        if pt_files:
            print(f"å‘ç°æœ¬åœ°TorchScriptæ¨¡å‹: {pt_files[0]}")

            # æ£€æŸ¥GPUå¯ç”¨æ€§
            if not torch.cuda.is_available():
                print("âŒ TorchScriptæ¨¡å‹éœ€è¦GPUï¼Œä½†ç³»ç»Ÿæ— GPUå¯ç”¨")
                print("è·³è¿‡TorchScriptåŠ è½½ï¼Œå°è¯•å…¶ä»–æ–¹å¼...")
            else:
                try:
                    print("âš ï¸ TorchScriptæ¨¡å‹å›ºå®šåœ¨cuda:0")
                    print("æ­£åœ¨åŠ è½½æ¨¡å‹åˆ°cuda:0...")

                    # å¼ºåˆ¶åŠ è½½åˆ°cuda:0
                    jit_model = torch.jit.load(str(pt_files[0]), map_location='cuda:0')
                    jit_model = jit_model.cuda(0)
                    jit_model.eval()

                    print("âœ“ TorchScriptæ¨¡å‹å·²åŠ è½½åˆ°cuda:0")

                    # ========== æ£€æµ‹æ¨¡å‹è¾“å…¥å°ºå¯¸ ==========
                    input_size = 224  # é»˜è®¤å€¼
                    print("æ­£åœ¨æ£€æµ‹æ¨¡å‹è¾“å…¥å°ºå¯¸...")

                    # å°è¯•224
                    try:
                        test_input = torch.randn(1, 3, 224, 224).cuda(0)
                        with torch.no_grad():
                            jit_model.encode_image(test_input)
                        input_size = 224
                        print("âœ“ æ£€æµ‹åˆ°æ¨¡å‹è¾“å…¥å°ºå¯¸: 224x224")
                    except Exception as e1:
                        # å°è¯•336
                        try:
                            test_input = torch.randn(1, 3, 336, 336).cuda(0)
                            with torch.no_grad():
                                jit_model.encode_image(test_input)
                            input_size = 336
                            print("âœ“ æ£€æµ‹åˆ°æ¨¡å‹è¾“å…¥å°ºå¯¸: 336x336")
                        except Exception as e2:
                            # å°è¯•384
                            try:
                                test_input = torch.randn(1, 3, 384, 384).cuda(0)
                                with torch.no_grad():
                                    jit_model.encode_image(test_input)
                                input_size = 384
                                print("âœ“ æ£€æµ‹åˆ°æ¨¡å‹è¾“å…¥å°ºå¯¸: 384x384")
                            except Exception as e3:
                                print(f"âš ï¸ æ— æ³•è‡ªåŠ¨æ£€æµ‹è¾“å…¥å°ºå¯¸")
                                print(f"  224é”™è¯¯: {str(e1)[:100]}")
                                print(f"  336é”™è¯¯: {str(e2)[:100]}")
                                print(f"  384é”™è¯¯: {str(e3)[:100]}")
                                print("ä½¿ç”¨é»˜è®¤224")
                                input_size = 224

                    # æ ¹æ®æ£€æµ‹åˆ°çš„å°ºå¯¸åˆ›å»ºé¢„å¤„ç†
                    standard_preprocess = create_preprocess(input_size)

                    # ========== TorchScriptåŒ…è£…å™¨ ==========
                    class TorchScriptWrapper(torch.nn.Module):
                        """TorchScriptæ¨¡å‹åŒ…è£…å™¨ - å›ºå®šcuda:0"""

                        def __init__(self, jit_model, input_size):
                            super().__init__()
                            self.model = jit_model
                            self.device = torch.device('cuda:0')
                            self.input_size = input_size
                            self.width = 768
                            self.visual = self

                            class TransformerProxy:
                                def __init__(self, width):
                                    self.width = width

                            self.transformer = TransformerProxy(self.width)

                        def encode_image(self, image):
                            """å›¾åƒç¼–ç  - è‡ªåŠ¨å¤„ç†è®¾å¤‡å’Œå°ºå¯¸"""
                            # 1. ç¡®ä¿åœ¨cuda:0
                            if not image.is_cuda:
                                image = image.cuda(0)
                            elif image.device.index != 0:
                                image = image.cuda(0)

                            # 2. æ£€æŸ¥å¹¶è°ƒæ•´å°ºå¯¸ï¼ˆå…³é”®ä¿®å¤ï¼‰
                            current_size = image.shape[-2:]
                            if current_size != (self.input_size, self.input_size):
                                print(f"âš ï¸ è¾“å…¥å°ºå¯¸{current_size}ä¸åŒ¹é…ï¼ŒæœŸæœ›{self.input_size}x{self.input_size}")
                                print(f"   è¿™è¯´æ˜é¢„å¤„ç†æ²¡æœ‰æ­£ç¡®åº”ç”¨ï¼Œè¯·æ£€æŸ¥æ•°æ®åŠ è½½æµç¨‹")
                                # ç´§æ€¥resize
                                import torch.nn.functional as F
                                image = F.interpolate(
                                    image,
                                    size=(self.input_size, self.input_size),
                                    mode='bicubic',
                                    align_corners=False
                                )
                                print(f"âœ“ å·²ç´§æ€¥è°ƒæ•´åˆ°{self.input_size}x{self.input_size}")

                            # 3. è°ƒç”¨æ¨¡å‹
                            try:
                                if hasattr(self.model, 'encode_image'):
                                    return self.model.encode_image(image)
                                else:
                                    return self.model(image)
                            except RuntimeError as e:
                                error_msg = str(e)
                                if "577" in error_msg and "197" in error_msg:
                                    print(f"âŒ Tokenæ•°é‡ä¸åŒ¹é…é”™è¯¯!")
                                    print(f"   è¿™æ„å‘³ç€è¾“å…¥å°ºå¯¸ä»ç„¶ä¸æ­£ç¡®")
                                    print(f"   æœŸæœ›è¾“å…¥: {self.input_size}x{self.input_size}")
                                    print(f"   å®é™…è¾“å…¥: {image.shape}")
                                    print(f"   æ¨¡å‹å¯èƒ½åœ¨224x224ä¸Šè®­ç»ƒï¼Œä½†æ”¶åˆ°äº†æ›´å¤§çš„è¾“å…¥")
                                raise

                        def encode_text(self, text):
                            """æ–‡æœ¬ç¼–ç """
                            if not text.is_cuda:
                                text = text.cuda(0)
                            elif text.device.index != 0:
                                text = text.cuda(0)

                            if hasattr(self.model, 'encode_text'):
                                return self.model.encode_text(text)
                            else:
                                raise AttributeError("TorchScriptæ¨¡å‹æ²¡æœ‰encode_textæ–¹æ³•")

                        def forward(self, image):
                            return self.encode_image(image)

                        def __call__(self, *args, **kwargs):
                            if len(args) == 1 and isinstance(args[0], torch.Tensor):
                                return self.forward(args[0])
                            return self.model(*args, **kwargs)

                        def to(self, device):
                            """å›ºå®šcuda:0ï¼Œå¿½ç•¥å…¶ä»–è¯·æ±‚"""
                            if str(device) not in ['cuda:0', 'cuda']:
                                print(f"âš ï¸ TorchScriptæ¨¡å‹å›ºå®šåœ¨cuda:0ï¼Œå¿½ç•¥to({device})")
                            return self

                        def cuda(self, device=None):
                            if device is not None and device != 0:
                                print(f"âš ï¸ TorchScriptæ¨¡å‹å›ºå®šåœ¨cuda:0ï¼Œå¿½ç•¥cuda({device})")
                            return self

                        def cpu(self):
                            print(f"âš ï¸ TorchScriptæ¨¡å‹å›ºå®šåœ¨cuda:0ï¼Œä¸æ”¯æŒè½¬åˆ°CPU")
                            return self

                        def eval(self):
                            self.model.eval()
                            return self

                        def train(self, mode=True):
                            if mode:
                                self.model.train()
                            else:
                                self.model.eval()
                            return self

                        def parameters(self):
                            return self.model.parameters()

                        def __getattr__(self, name):
                            try:
                                return super().__getattr__(name)
                            except AttributeError:
                                try:
                                    return getattr(self.model, name)
                                except AttributeError:
                                    raise AttributeError(f"TorchScriptæ¨¡å‹æ²¡æœ‰å±æ€§: {name}")

                    # åˆ›å»ºåŒ…è£…å™¨
                    model = TorchScriptWrapper(jit_model, input_size)
                    self.preprocess = standard_preprocess

                    print("âœ“ æˆåŠŸåŠ è½½TorchScriptæ¨¡å‹")
                    print(f"âœ“ æ¨¡å‹å›ºå®šåœ¨: cuda:0")
                    print(f"âœ“ è¾“å…¥å°ºå¯¸: {input_size}x{input_size}")
                    print(f"âœ“ ç‰¹å¾ç»´åº¦: {model.width}")
                    print(f"âš ï¸ é‡è¦: è¯·ç¡®ä¿æ•°æ®é¢„å¤„ç†ä½¿ç”¨{input_size}x{input_size}å°ºå¯¸")

                    return model

                except Exception as e:
                    print(f"âœ— åŠ è½½TorchScriptæ¨¡å‹å¤±è´¥: {e}")
                    import traceback
                    traceback.print_exc()
                    print("å°è¯•å…¶ä»–åŠ è½½æ–¹å¼...")

        # ========== 2. ä½¿ç”¨open_clipåŠ è½½ ==========
        try:
            print("å°è¯•ä½¿ç”¨open_clipåŠ è½½é¢„è®­ç»ƒæ¨¡å‹...")
            model, _, preprocess = open_clip.create_model_and_transforms(
                model_name_converted,
                pretrained=pretrained if pretrained != 'openai' else 'openai',
                cache_dir=str(cache_dir)
            )
            model = model.to(self.device)
            model.eval()
            self.preprocess = preprocess
            print(f"âœ“ æˆåŠŸåŠ è½½open_clipé¢„è®­ç»ƒæ¨¡å‹")
            return model
        except Exception as e:
            print(f"âœ— open_clipåŠ è½½å¤±è´¥: {e}")

        # ========== 3. åˆ›å»ºæ— é¢„è®­ç»ƒæ¨¡å‹ ==========
        print("âš ï¸ å›é€€åˆ°æ— é¢„è®­ç»ƒæƒé‡æ¨¡å‹")
        try:
            model, _, preprocess = open_clip.create_model_and_transforms(
                model_name_converted,
                pretrained=None
            )
            model = model.to(self.device)
            model.eval()
            self.preprocess = preprocess if preprocess is not None else standard_preprocess
            print(f"âœ“ æˆåŠŸåˆ›å»ºæ— é¢„è®­ç»ƒæƒé‡æ¨¡å‹")
            print("âš ï¸ æ­¤æ¨¡å‹éœ€è¦é‡æ–°è®­ç»ƒ")
            return model
        except Exception as e:
            print(f"âœ— åˆ›å»ºæ¨¡å‹å¤±è´¥: {e}")
            raise RuntimeError(f"æ— æ³•åŠ è½½æˆ–åˆ›å»ºCLIPæ¨¡å‹: {e}")

    def forward(self, batch: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­ - ç¡®ä¿æ‰€æœ‰æ•°æ®åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        """
        # ========== è®¾å¤‡æ£€æŸ¥å’Œè½¬æ¢ ==========
        images = batch['image']

        # å…³é”®ï¼šç¡®ä¿è¾“å…¥åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        if images.device != self.device:
            print(f"âš ï¸ è¾“å…¥å›¾åƒè®¾å¤‡ä¸åŒ¹é…: {images.device} -> {self.device}")
            images = images.to(self.device)

        batch_size = images.size(0)
        original_size = images.shape[-2:]

        # ========== 1. CLIPç‰¹å¾æå– ==========
        # CLIPå¯èƒ½å›ºå®šåœ¨cuda:0ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
        if hasattr(self.clip_model, 'device'):
            clip_device = self.clip_model.device
            if images.device != clip_device:
                images_for_clip = images.to(clip_device)
            else:
                images_for_clip = images
        else:
            images_for_clip = images

        clip_features = self.clip_model.encode_image(images_for_clip)  # [B, 768]
        if clip_features.dtype == torch.float16:
            clip_features = clip_features.float()

        # ç¡®ä¿CLIPè¾“å‡ºåœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        if clip_features.device != self.device:
            clip_features = clip_features.to(self.device)

        # ========== 2. æ·±åº¦ä¼°è®¡ ==========
        if 'depth' in batch and batch['depth'] is not None:
            depth_maps = batch['depth'].to(self.device)
            if depth_maps.shape[-2:] != original_size:
                depth_maps = F.interpolate(
                    depth_maps,
                    size=original_size,
                    mode='bilinear',
                    align_corners=False
                )
        else:
            with torch.no_grad():
                # ä¸ºæ·±åº¦ä¼°è®¡å‡†å¤‡è¾“å…¥ï¼ˆå¯èƒ½éœ€è¦resizeåˆ°384ï¼‰
                if original_size[0] != self.depth_input_size:
                    images_for_depth = F.interpolate(
                        images,
                        size=(self.depth_input_size, self.depth_input_size),
                        mode='bilinear',
                        align_corners=False
                    )
                else:
                    images_for_depth = images

                # æ·±åº¦ä¼°è®¡
                depth_maps = self.depth_estimator(images_for_depth)

                # Resizeå›åŸå§‹å°ºå¯¸
                if depth_maps.shape[-2:] != original_size:
                    depth_maps = F.interpolate(
                        depth_maps,
                        size=original_size,
                        mode='bilinear',
                        align_corners=False
                    )

        # ç¡®ä¿æ·±åº¦å›¾åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        depth_maps = depth_maps.to(self.device)

        # ========== 3. åˆå¹¶RGBD ==========
        rgbd_images = torch.cat([images, depth_maps], dim=1)  # [B, 4, H, W]

        # ========== 4. ä½“ç´ è½¬æ¢ ==========
        # ä½“ç´ è½¬æ¢å™¨æ˜¯å·¥å…·ç±»ï¼Œç¡®ä¿è¾“å…¥åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        voxels = self.voxel_converter.images_to_voxels(rgbd_images)
        voxels = voxels.to(self.device)  # ç¡®ä¿è¾“å‡ºåœ¨æ­£ç¡®è®¾å¤‡ä¸Š

        # ========== 5. å‡ ä½•ç‰¹å¾æå– ==========
        geometry_features = self.geometry_encoder(voxels)

        # ========== 6. ç‰¹å¾èåˆ ==========
        fused_features = self.fusion_module(clip_features, geometry_features)

        # ========== 7. å¼‚å¸¸æ£€æµ‹ ==========
        anomaly_predictions = self.anomaly_head(fused_features)

        return {
            'anomaly_predictions': anomaly_predictions,
            'clip_features': clip_features,
            'geometry_features': geometry_features,
            'fused_features': fused_features,
            'depth_maps': depth_maps
        }

    def predict_anomaly(self, batch: Dict[str, torch.Tensor]) -> torch.Tensor:
        """
        é¢„æµ‹å¼‚å¸¸åˆ†æ•°

        Args:
            batch: è¾“å…¥æ‰¹æ¬¡

        Returns:
            anomaly_scores: å¼‚å¸¸åˆ†æ•° [B] æˆ– [B, num_classes]
        """
        with torch.no_grad():
            results = self.forward(batch)
            predictions = results['anomaly_predictions']

            if self.anomaly_head.detection_type == "regression":
                return predictions.squeeze(-1)  # [B]
            else:
                # åˆ†ç±»æƒ…å†µï¼Œè¿”å›å¼‚å¸¸ç±»åˆ«çš„æ¦‚ç‡
                return F.softmax(predictions, dim=-1)[:, 1]  # [B]

    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)

        return {
            'total_parameters': total_params,
            'trainable_parameters': trainable_params,
            'clip_frozen': self.freeze_clip,
            'fusion_type': self.fusion_module.fusion_type,
            'detection_type': self.anomaly_head.detection_type,
            'device': self.device
        }


def create_geoclip_model(config: Dict[str, Any]) -> GeoCLIP:
    """
    å·¥å‚å‡½æ•°ï¼šæ ¹æ®é…ç½®åˆ›å»ºGeoCLIPæ¨¡å‹

    Args:
        config: æ¨¡å‹é…ç½®å­—å…¸

    Returns:
        GeoCLIPæ¨¡å‹å®ä¾‹
    """
    return GeoCLIP(
        clip_model_name=config.get('clip_model', 'ViT-B/16'),
        clip_pretrained=config.get('clip_pretrained', 'openai'),
        depth_estimator_type=config.get('depth_estimator', 'DPT_Large'),
        geometry_encoder_config=config.get('geometry_encoder', None),
        fusion_type=config.get('fusion_type', 'cross_attention'),
        fusion_dim=config.get('fusion_dim', 1024),
        output_dim=config.get('output_dim', 512),
        detection_type=config.get('detection_type', 'regression'),
        num_classes=config.get('num_classes', 2),
        device=config.get('device', 'cuda'),
        freeze_clip=config.get('freeze_clip', False)
    )


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    print("=== æµ‹è¯•GeoCLIPæ¨¡å‹ ===")

    try:
        # åˆ›å»ºæ¨¡å‹é…ç½®
        config = {
            'clip_model': 'ViT-B/16',
            'depth_estimator': 'DPT_Large',
            'fusion_type': 'cross_attention',
            'detection_type': 'regression',
            'device': 'cuda' if torch.cuda.is_available() else 'cpu'
        }

        # åˆ›å»ºæ¨¡å‹
        model = create_geoclip_model(config)
        print(f"âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ")
        print(f"æ¨¡å‹ä¿¡æ¯: {model.get_model_info()}")

        # åˆ›å»ºæµ‹è¯•æ•°æ®
        batch_size = 2
        test_batch = {
            'image': torch.randn(batch_size, 3, 224, 224),
            # 'depth': torch.randn(batch_size, 1, 224, 224),  # å¯é€‰
        }

        # ç§»åŠ¨åˆ°è®¾å¤‡
        device = config['device']
        model = model.to(device)
        for key in test_batch:
            test_batch[key] = test_batch[key].to(device)

        # å‰å‘ä¼ æ’­æµ‹è¯•
        print("\næµ‹è¯•å‰å‘ä¼ æ’­...")
        results = model(test_batch)

        print(f"âœ… å‰å‘ä¼ æ’­æˆåŠŸ:")
        for key, value in results.items():
            if isinstance(value, torch.Tensor):
                print(f"  {key}: {value.shape}")

        # å¼‚å¸¸é¢„æµ‹æµ‹è¯•
        anomaly_scores = model.predict_anomaly(test_batch)
        print(f"âœ… å¼‚å¸¸é¢„æµ‹: {anomaly_scores.shape}")
        print(f"å¼‚å¸¸åˆ†æ•°èŒƒå›´: {anomaly_scores.min():.3f} - {anomaly_scores.max():.3f}")

        print("\nğŸ‰ GeoCLIPæ¨¡å‹æµ‹è¯•å®Œæˆ!")

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()