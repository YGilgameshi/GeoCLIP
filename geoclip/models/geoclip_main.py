"""
GeoCLIP - ä¸»æ¨¡å‹
æ•´åˆ2D CLIPç‰¹å¾ã€3Då‡ ä½•ç‰¹å¾å’Œæ·±åº¦ä¿¡æ¯è¿›è¡Œå¼‚å¸¸æ£€æµ‹
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Optional, Tuple, Union, Any
import numpy as np
import open_clip

# å¯¼å…¥GeoCLIPç»„ä»¶
from geoclip.models.depth_estimator import DepthEstimator
from geoclip.models.geometry_encoder import create_geometry_encoder, VoxelEncoder
from geoclip.utils.voxel_utils import DepthToVoxelConverter


class FeatureFusionModule(nn.Module):
    """
    2D CLIPç‰¹å¾ä¸3Då‡ ä½•ç‰¹å¾èåˆæ¨¡å—
    """

    def __init__(self,
                 clip_dim: int = 512,
                 geometry_dim: int = 512,
                 fusion_dim: int = 1024,
                 output_dim: int = 512,
                 fusion_type: str = "cross_attention"):
        super(FeatureFusionModule, self).__init__()

        self.fusion_type = fusion_type
        self.clip_dim = clip_dim
        self.geometry_dim = geometry_dim
        self.fusion_dim = fusion_dim
        self.output_dim = output_dim

        if fusion_type == "cross_attention":
            self._build_cross_attention()
        elif fusion_type == "adaptive_fusion":
            self._build_adaptive_fusion()
        elif fusion_type == "simple_concat":
            self._build_simple_concat()
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„èåˆç±»å‹: {fusion_type}")

    def _build_cross_attention(self):
        """æ„å»ºäº¤å‰æ³¨æ„åŠ›èåˆ"""
        # æŠ•å½±åˆ°ç›¸åŒç»´åº¦
        self.clip_proj = nn.Linear(self.clip_dim, self.fusion_dim)
        self.geometry_proj = nn.Linear(self.geometry_dim, self.fusion_dim)

        # äº¤å‰æ³¨æ„åŠ›
        self.cross_attention = nn.MultiheadAttention(
            embed_dim=self.fusion_dim,
            num_heads=8,
            dropout=0.1,
            batch_first=True
        )

        # è¾“å‡ºæŠ•å½±
        self.output_proj = nn.Sequential(
            nn.Linear(self.fusion_dim * 2, self.output_dim),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1),
            nn.Linear(self.output_dim, self.output_dim)
        )

    def _build_adaptive_fusion(self):
        """æ„å»ºè‡ªé€‚åº”èåˆ"""
        # ç‰¹å¾æŠ•å½±
        self.clip_proj = nn.Linear(self.clip_dim, self.fusion_dim)
        self.geometry_proj = nn.Linear(self.geometry_dim, self.fusion_dim)

        # è‡ªé€‚åº”æƒé‡ç½‘ç»œ
        self.weight_net = nn.Sequential(
            nn.Linear(self.fusion_dim * 2, self.fusion_dim),
            nn.ReLU(inplace=True),
            nn.Linear(self.fusion_dim, 2),
            nn.Softmax(dim=-1)
        )

        # è¾“å‡ºæŠ•å½±
        self.output_proj = nn.Linear(self.fusion_dim, self.output_dim)

    def _build_simple_concat(self):
        """æ„å»ºç®€å•è¿æ¥èåˆ"""
        self.fusion_proj = nn.Sequential(
            nn.Linear(self.clip_dim + self.geometry_dim, self.fusion_dim),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1),
            nn.Linear(self.fusion_dim, self.output_dim)
        )

    def forward(self, clip_features: torch.Tensor,
                geometry_features: torch.Tensor) -> torch.Tensor:
        """
        èåˆ2Då’Œ3Dç‰¹å¾

        Args:
            clip_features: CLIP 2Dç‰¹å¾ [B, clip_dim]
            geometry_features: å‡ ä½•3Dç‰¹å¾ [B, geometry_dim]

        Returns:
            fused_features: èåˆç‰¹å¾ [B, output_dim]
        """
        if self.fusion_type == "cross_attention":
            return self._cross_attention_forward(clip_features, geometry_features)
        elif self.fusion_type == "adaptive_fusion":
            return self._adaptive_fusion_forward(clip_features, geometry_features)
        elif self.fusion_type == "simple_concat":
            return self._simple_concat_forward(clip_features, geometry_features)

    # æŠ•å½±
    def _cross_attention_forward(self, clip_feat, geometry_feat):
        clip_proj = self.clip_proj(clip_feat).unsqueeze(1)  # [B, 1, fusion_dim]
        geometry_proj = self.geometry_proj(geometry_feat).unsqueeze(1)  # [B, 1, fusion_dim]

        # äº¤å‰æ³¨æ„åŠ›ï¼šCLIPæŸ¥è¯¢å‡ ä½•ç‰¹å¾
        clip_enhanced, _ = self.cross_attention(clip_proj, geometry_proj, geometry_proj)

        # äº¤å‰æ³¨æ„åŠ›ï¼šå‡ ä½•æŸ¥è¯¢CLIPç‰¹å¾
        geometry_enhanced, _ = self.cross_attention(geometry_proj, clip_proj, clip_proj)

        # è¿æ¥å’ŒæŠ•å½±
        fused = torch.cat([clip_enhanced.squeeze(1), geometry_enhanced.squeeze(1)], dim=1)
        return self.output_proj(fused)

    def _adaptive_fusion_forward(self, clip_feat, geometry_feat):
        # æŠ•å½±
        clip_proj = self.clip_proj(clip_feat)
        geometry_proj = self.geometry_proj(geometry_feat)

        # è®¡ç®—è‡ªé€‚åº”æƒé‡
        combined = torch.cat([clip_proj, geometry_proj], dim=1)
        weights = self.weight_net(combined)  # [B, 2]

        # åŠ æƒèåˆ
        w1, w2 = weights[:, 0:1], weights[:, 1:2]
        fused = w1 * clip_proj + w2 * geometry_proj

        return self.output_proj(fused)

    def _simple_concat_forward(self, clip_feat, geometry_feat):
        # ç®€å•è¿æ¥
        concatenated = torch.cat([clip_feat, geometry_feat], dim=1)
        return self.fusion_proj(concatenated)


class AnomalyDetectionHead(nn.Module):
    """
    å¼‚å¸¸æ£€æµ‹å¤´
    åŸºäºèåˆç‰¹å¾è¿›è¡Œå¼‚å¸¸æ£€æµ‹
    """

    def __init__(self,
                 input_dim: int = 512,
                 hidden_dim: int = 256,
                 num_classes: int = 2,  # æ­£å¸¸/å¼‚å¸¸
                 detection_type: str = "classification"):
        super(AnomalyDetectionHead, self).__init__()

        self.detection_type = detection_type
        self.input_dim = input_dim

        if detection_type == "classification":
            self.head = nn.Sequential(
                nn.Linear(input_dim, hidden_dim),
                nn.ReLU(inplace=True),
                nn.Dropout(0.1),
                nn.Linear(hidden_dim, hidden_dim),
                nn.ReLU(inplace=True),
                nn.Dropout(0.1),
                nn.Linear(hidden_dim, num_classes)
            )
        elif detection_type == "regression":
            self.head = nn.Sequential(
                nn.Linear(input_dim, hidden_dim),
                nn.ReLU(inplace=True),
                nn.Dropout(0.1),
                nn.Linear(hidden_dim, hidden_dim),
                nn.ReLU(inplace=True),
                nn.Dropout(0.1),
                nn.Linear(hidden_dim, 1),
                nn.Sigmoid()  # å¼‚å¸¸åˆ†æ•° [0, 1]
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ£€æµ‹ç±»å‹: {detection_type}")

    def forward(self, features: torch.Tensor) -> torch.Tensor:
        """
        å¼‚å¸¸æ£€æµ‹å‰å‘ä¼ æ’­

        Args:
            features: èåˆç‰¹å¾ [B, input_dim]

        Returns:
            predictions: é¢„æµ‹ç»“æœ
                - classification: [B, num_classes]
                - regression: [B, 1]
        """
        return self.head(features)


class GeoCLIP(nn.Module):
    """
    GeoCLIPä¸»æ¨¡å‹
    æ•´åˆ2D CLIPã€3Då‡ ä½•ã€æ·±åº¦ä¿¡æ¯è¿›è¡Œå¼‚å¸¸æ£€æµ‹
    """

    def __init__(self,
                 # 2Dæ¨¡å‹é…ç½®
                 clip_model_name: str = "ViT-B/16",
                 clip_pretrained: str = "openai",

                 # 3Dæ¨¡å‹é…ç½®
                 depth_estimator_type: str = "DPT_Large",
                 geometry_encoder_config: Dict = None,

                 # èåˆé…ç½®
                 fusion_type: str = "cross_attention",
                 fusion_dim: int = 1024,
                 output_dim: int = 512,

                 # å¼‚å¸¸æ£€æµ‹é…ç½®
                 detection_type: str = "regression",
                 num_classes: int = 2,

                 # å…¶ä»–é…ç½®
                 device: str = "cuda",
                 freeze_clip: bool = False):

        super(GeoCLIP, self).__init__()

        self.device = device
        self.freeze_clip = freeze_clip

        # 1. 2D CLIPæ¨¡å‹
        self.clip_model = self._load_clip_model(clip_model_name, clip_pretrained)
        if freeze_clip:
            for param in self.clip_model.parameters():
                param.requires_grad = False

        # 2. æ·±åº¦ä¼°è®¡å™¨
        self.depth_estimator = DepthEstimator(
            model_type=depth_estimator_type,
            device=device
        )

        # 3. æ·±åº¦åˆ°ä½“ç´ è½¬æ¢å™¨
        self.voxel_converter = DepthToVoxelConverter(
            voxel_size=64,
            depth_range=(0.1, 10.0)
        )

        # 4. 3Då‡ ä½•ç¼–ç å™¨
        if geometry_encoder_config is None:
            geometry_encoder_config = {
                'type': 'voxel',
                'in_channels': 4,  # RGB + Depth
                'output_channels': 512
            }

        self.geometry_encoder = create_geometry_encoder(geometry_encoder_config)

        # 5. ç‰¹å¾èåˆæ¨¡å—
        # å®‰å…¨åœ°è·å–CLIPç»´åº¦
        def get_clip_dim(model):
            """å®‰å…¨åœ°è·å–CLIPæ¨¡å‹çš„ç‰¹å¾ç»´åº¦"""
            # æ–¹æ³•1: æ£€æŸ¥transformer.width
            if hasattr(model, 'transformer') and hasattr(model.transformer, 'width'):
                return model.transformer.width

            # æ–¹æ³•2: æ£€æŸ¥widthå±æ€§
            if hasattr(model, 'width'):
                return model.width

            # æ–¹æ³•3: æ£€æŸ¥visual.width
            if hasattr(model, 'visual') and hasattr(model.visual, 'width'):
                return model.visual.width

            # æ–¹æ³•4: æ ¹æ®æ¨¡å‹åç§°æ¨æ–­
            model_name_lower = clip_model_name.lower()
            if 'vit-b' in model_name_lower or 'vitb' in model_name_lower:
                return 768
            elif 'vit-l' in model_name_lower or 'vitl' in model_name_lower:
                return 1024
            elif 'vit-h' in model_name_lower or 'vith' in model_name_lower:
                return 1280
            elif 'rn50' in model_name_lower or 'resnet50' in model_name_lower:
                return 1024
            elif 'rn101' in model_name_lower or 'resnet101' in model_name_lower:
                return 512

            # é»˜è®¤å€¼
            print(f"âš  æ— æ³•ç¡®å®šCLIPç»´åº¦ï¼Œä½¿ç”¨é»˜è®¤å€¼512")
            return 512

        # clip_dim = get_clip_dim(self.clip_model)
        clip_dim = 768
        geometry_dim = geometry_encoder_config['output_channels']

        print(f"  ç‰¹å¾ç»´åº¦ - CLIP: {clip_dim}, å‡ ä½•: {geometry_dim}")

        self.fusion_module = FeatureFusionModule(
            clip_dim=clip_dim,
            geometry_dim=geometry_dim,
            fusion_dim=fusion_dim,
            output_dim=output_dim,
            fusion_type=fusion_type
        )
        clip_dim = self.clip_model.transformer.width if hasattr(self.clip_model, 'transformer') else 512
        geometry_dim = geometry_encoder_config['output_channels']

        self.fusion_module = FeatureFusionModule(
            clip_dim=clip_dim,
            geometry_dim=geometry_dim,
            fusion_dim=fusion_dim,
            output_dim=output_dim,
            fusion_type=fusion_type
        )

        # 6. å¼‚å¸¸æ£€æµ‹å¤´
        self.anomaly_head = AnomalyDetectionHead(
            input_dim=output_dim,
            detection_type=detection_type,
            num_classes=num_classes
        )

        print(f"GeoCLIPæ¨¡å‹åˆå§‹åŒ–å®Œæˆ:")
        print(f"  CLIPæ¨¡å‹: {clip_model_name}")
        print(f"  æ·±åº¦ä¼°è®¡: {depth_estimator_type}")
        print(f"  å‡ ä½•ç¼–ç : {geometry_encoder_config['type']}")
        print(f"  èåˆæ–¹å¼: {fusion_type}")
        print(f"  æ£€æµ‹ç±»å‹: {detection_type}")

    # def _load_clip_model(self, model_name: str, pretrained: str):
    #     """åŠ è½½CLIPæ¨¡å‹ - ä»æœ¬åœ°ç¼“å­˜åŠ è½½"""
    #
    #     import os
    #     from pathlib import Path
    #
    #     model_name_converted = model_name.replace('/', '-')
    #
    #     # æŒ‡å®šç¼“å­˜ç›®å½•
    #     cache_dir = Path.home() / '.cache' / 'open_clip'
    #     cache_dir.mkdir(parents=True, exist_ok=True)
    #
    #     print(f"æ­£åœ¨ä»ç¼“å­˜åŠ è½½CLIPæ¨¡å‹: {model_name_converted}")
    #     print(f"ç¼“å­˜ç›®å½•: {cache_dir}")
    #
    #     try:
    #         # å°è¯•ä»æœ¬åœ°ç¼“å­˜åŠ è½½
    #         model, _, preprocess = open_clip.create_model_and_transforms(
    #             model_name_converted,
    #             pretrained=pretrained if pretrained != 'openai' else 'openai',
    #             cache_dir=str(cache_dir)
    #         )
    #
    #         model = model.to(self.device)
    #         self.preprocess = preprocess
    #         print(f"æˆåŠŸåŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {model_name_converted}")
    #         return model
    #
    #     except Exception as e:
    #         print(f"ä»ç¼“å­˜åŠ è½½å¤±è´¥: {e}")
    #         # æ£€æŸ¥ç¼“å­˜æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    #         cache_files = list(cache_dir.glob("*.pt"))
    #         print(f"ç¼“å­˜ç›®å½•ä¸­çš„æ–‡ä»¶: {cache_files}")
    #
    #         # å¦‚æœæœ‰.ptæ–‡ä»¶ï¼Œå°è¯•ç›´æ¥åŠ è½½
    #         if cache_files:
    #             print("å°è¯•ç›´æ¥åŠ è½½æœ¬åœ°æƒé‡æ–‡ä»¶...")
    #             model, _, preprocess = open_clip.create_model_and_transforms(
    #                 model_name_converted,
    #                 pretrained=None
    #             )
    #             # æ‰‹åŠ¨åŠ è½½æƒé‡
    #             state_dict = torch.load(cache_files[0], map_location=self.device)
    #             model.load_state_dict(state_dict, strict=False)
    #             model = model.to(self.device)
    #             self.preprocess = preprocess
    #             print("æˆåŠŸä»æœ¬åœ°æ–‡ä»¶åŠ è½½æƒé‡")
    #             return model
    #         else:
    #             raise RuntimeError(f"ç¼“å­˜ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°æ¨¡å‹æ–‡ä»¶: {cache_dir}")

    def _load_clip_model(self, model_name: str, pretrained: str):
        """
        åŠ è½½CLIPæ¨¡å‹ - ä¼˜å…ˆä»æœ¬åœ°ç¼“å­˜åŠ è½½TorchScriptæ¨¡å‹
        æ³¨æ„: TorchScriptæ¨¡å‹å›ºå®šåœ¨cuda:0ä¸Š
        """
        import open_clip
        import torch
        from pathlib import Path
        from torchvision import transforms

        model_name_converted = model_name.replace('/', '-')
        cache_dir = Path.home() / '.cache' / 'open_clip'
        cache_dir.mkdir(parents=True, exist_ok=True)

        print(f"æ­£åœ¨ä»æœ¬åœ°åŠ è½½CLIPæ¨¡å‹: {model_name_converted}")
        print(f"ç›®æ ‡è®¾å¤‡: {self.device}")

        # å®šä¹‰æ ‡å‡†çš„CLIPé¢„å¤„ç†pipeline
        standard_preprocess = transforms.Compose([
            transforms.Resize(384, interpolation=transforms.InterpolationMode.BICUBIC),
            transforms.CenterCrop(384),
            transforms.ToTensor(),
            transforms.Normalize(
                mean=[0.48145466, 0.4578275, 0.40821073],
                std=[0.26862954, 0.26130258, 0.27577711]
            )
        ])

        # ========== 1. ä¼˜å…ˆåŠ è½½æœ¬åœ°TorchScriptæ¨¡å‹ ==========
        pt_files = list(cache_dir.glob(f"*{model_name_converted}*.pt"))
        if not pt_files:
            pt_files = list(cache_dir.glob("*.pt"))

        if pt_files:
            print(f"å‘ç°æœ¬åœ°TorchScriptæ¨¡å‹: {pt_files[0]}")

            # æ£€æŸ¥GPUå¯ç”¨æ€§
            if not torch.cuda.is_available():
                print("âŒ TorchScriptæ¨¡å‹éœ€è¦GPUï¼Œä½†ç³»ç»Ÿæ— GPUå¯ç”¨")
                print("è·³è¿‡TorchScriptåŠ è½½ï¼Œå°è¯•å…¶ä»–æ–¹å¼...")
            else:
                try:
                    print("âš ï¸ TorchScriptæ¨¡å‹å›ºå®šåœ¨cuda:0")
                    print("æ­£åœ¨åŠ è½½æ¨¡å‹åˆ°cuda:0...")

                    # å¼ºåˆ¶åŠ è½½åˆ°cuda:0ï¼ˆå› ä¸ºæ¨¡å‹å†…éƒ¨ç¡¬ç¼–ç äº†cuda:0ï¼‰
                    jit_model = torch.jit.load(str(pt_files[0]), map_location='cuda:0')
                    jit_model = jit_model.cuda(0)
                    jit_model.eval()

                    print("âœ“ TorchScriptæ¨¡å‹å·²åŠ è½½åˆ°cuda:0")

                    # å°è¯•æ¨æ–­æ¨¡å‹æœŸæœ›çš„è¾“å…¥å°ºå¯¸
                    input_size = 384  # é»˜è®¤å€¼
                    try:
                        # å°è¯•ä»æ¨¡å‹ä¸­è·å–è¾“å…¥å°ºå¯¸ä¿¡æ¯
                        test_input = torch.randn(1, 3, 384, 384).cuda(0)
                        with torch.no_grad():
                            jit_model.encode_image(test_input)
                        input_size = 384
                        print("âœ“ æ¨¡å‹è¾“å…¥å°ºå¯¸: 384")
                    except:
                        # å°è¯•384
                        try:
                            test_input = torch.randn(1, 3, 384, 384).cuda(0)
                            with torch.no_grad():
                                jit_model.encode_image(test_input)
                            input_size = 384
                            print("âœ“ æ¨¡å‹è¾“å…¥å°ºå¯¸: 384x384")
                        except:
                            print("âš ï¸ æ— æ³•è‡ªåŠ¨æ£€æµ‹è¾“å…¥å°ºå¯¸ï¼Œä½¿ç”¨é»˜è®¤224")
                            input_size = 224

                    # æ ¹æ®æ£€æµ‹åˆ°çš„å°ºå¯¸æ›´æ–°é¢„å¤„ç†
                    standard_preprocess = transforms.Compose([
                        transforms.Resize(input_size, interpolation=transforms.InterpolationMode.BICUBIC),
                        transforms.CenterCrop(input_size),
                        transforms.ToTensor(),
                        transforms.Normalize(
                            mean=[0.48145466, 0.4578275, 0.40821073],
                            std=[0.26862954, 0.26130258, 0.27577711]
                        )
                    ])

                    # ========== TorchScriptåŒ…è£…å™¨ ==========
                    class TorchScriptWrapper(torch.nn.Module):
                        """
                        TorchScriptæ¨¡å‹åŒ…è£…å™¨
                        - æ·»åŠ CLIPæ ‡å‡†å±æ€§(width, transformer, visual)
                        - è‡ªåŠ¨å¤„ç†è®¾å¤‡è½¬æ¢(æ‰€æœ‰è¾“å…¥è½¬åˆ°cuda:0)
                        - å›ºå®šåœ¨cuda:0ï¼Œå¿½ç•¥å…¶ä»–è®¾å¤‡è¯·æ±‚
                        """

                        def __init__(self, jit_model, input_size=384):
                            super().__init__()
                            self.model = jit_model
                            self.device = torch.device('cuda:0')
                            self.input_size = input_size

                            # CLIPæ ‡å‡†å±æ€§
                            self.width = 768  # ViT-Bé»˜è®¤ï¼Œå¯æ ¹æ®å®é™…è°ƒæ•´
                            self.visual = self

                            # transformerå±æ€§ï¼ˆç”¨äºè·å–ç‰¹å¾ç»´åº¦ï¼‰
                            class TransformerProxy:
                                def __init__(self, width):
                                    self.width = width

                            self.transformer = TransformerProxy(self.width)

                        def encode_image(self, image):
                            """
                            å›¾åƒç¼–ç  - è‡ªåŠ¨ç¡®ä¿è¾“å…¥åœ¨cuda:0å¹¶æ£€æŸ¥å°ºå¯¸
                            Args:
                                image: è¾“å…¥å›¾åƒå¼ é‡ï¼Œä»»æ„è®¾å¤‡
                            Returns:
                                ç‰¹å¾å¼ é‡ï¼Œåœ¨cuda:0ä¸Š
                            """
                            # ç¡®ä¿è¾“å…¥åœ¨cuda:0
                            if not image.is_cuda:
                                image = image.cuda(0)
                            elif image.device.index != 0:
                                image = image.cuda(0)

                            # æ£€æŸ¥è¾“å…¥å°ºå¯¸
                            if image.shape[-2:] != (self.input_size, self.input_size):
                                print(f"âš ï¸ è¾“å…¥å°ºå¯¸ä¸åŒ¹é…: æœŸæœ›{self.input_size}x{self.input_size}, "
                                      f"å®é™…{image.shape[-2]}x{image.shape[-1]}")
                                # å°è¯•è‡ªåŠ¨resize
                                import torch.nn.functional as F
                                image = F.interpolate(
                                    image,
                                    size=(self.input_size, self.input_size),
                                    mode='bicubic',
                                    align_corners=False
                                )
                                print(f"âœ“ å·²è‡ªåŠ¨è°ƒæ•´åˆ°{self.input_size}x{self.input_size}")

                            # è°ƒç”¨æ¨¡å‹
                            try:
                                if hasattr(self.model, 'encode_image'):
                                    return self.model.encode_image(image)
                                else:
                                    return self.model(image)
                            except RuntimeError as e:
                                if "should be the same" in str(e):
                                    print(f"âŒ è®¾å¤‡ç±»å‹ä¸åŒ¹é…: {e}")
                                    print(f"è¿™é€šå¸¸æ„å‘³ç€æ¨¡å‹å¯¼å‡ºæ—¶å­˜åœ¨é—®é¢˜")
                                    print(f"å»ºè®®é‡æ–°å¯¼å‡ºTorchScriptæ¨¡å‹")
                                raise

                        def encode_text(self, text):
                            """
                            æ–‡æœ¬ç¼–ç  - è‡ªåŠ¨ç¡®ä¿è¾“å…¥åœ¨cuda:0
                            Args:
                                text: æ–‡æœ¬tokenå¼ é‡ï¼Œä»»æ„è®¾å¤‡
                            Returns:
                                ç‰¹å¾å¼ é‡ï¼Œåœ¨cuda:0ä¸Š
                            """
                            # ç¡®ä¿è¾“å…¥åœ¨cuda:0
                            if not text.is_cuda:
                                text = text.cuda(0)
                            elif text.device.index != 0:
                                text = text.cuda(0)

                            if hasattr(self.model, 'encode_text'):
                                return self.model.encode_text(text)
                            else:
                                raise AttributeError("TorchScriptæ¨¡å‹æ²¡æœ‰encode_textæ–¹æ³•")

                        def forward(self, image):
                            """å‰å‘ä¼ æ’­"""
                            return self.encode_image(image)

                        def __call__(self, *args, **kwargs):
                            """æ”¯æŒç›´æ¥è°ƒç”¨"""
                            if len(args) == 1 and isinstance(args[0], torch.Tensor):
                                return self.forward(args[0])
                            return self.model(*args, **kwargs)

                        def to(self, device):
                            """
                            è®¾å¤‡è½¬æ¢ - TorchScriptæ¨¡å‹å›ºå®šåœ¨cuda:0
                            å¿½ç•¥å…¶ä»–è®¾å¤‡è¯·æ±‚ï¼Œè¿”å›selfä¿æŒé“¾å¼è°ƒç”¨
                            """
                            device_str = str(device)
                            if device_str != 'cuda:0' and device_str != 'cuda':
                                print(f"âš ï¸ TorchScriptæ¨¡å‹å›ºå®šåœ¨cuda:0ï¼Œå¿½ç•¥to({device})è¯·æ±‚")
                            return self

                        def cuda(self, device=None):
                            """CUDAè½¬æ¢ - å·²åœ¨cuda:0"""
                            if device is not None and device != 0:
                                print(f"âš ï¸ TorchScriptæ¨¡å‹å›ºå®šåœ¨cuda:0ï¼Œå¿½ç•¥cuda({device})è¯·æ±‚")
                            return self

                        def cpu(self):
                            """CPUè½¬æ¢ - ä¸æ”¯æŒ"""
                            print(f"âš ï¸ TorchScriptæ¨¡å‹å›ºå®šåœ¨cuda:0ï¼Œä¸æ”¯æŒè½¬åˆ°CPU")
                            return self

                        def eval(self):
                            """è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼"""
                            self.model.eval()
                            return self

                        def train(self, mode=True):
                            """è®¾ç½®è®­ç»ƒ/è¯„ä¼°æ¨¡å¼"""
                            if mode:
                                self.model.train()
                            else:
                                self.model.eval()
                            return self

                        def parameters(self):
                            """è¿”å›æ¨¡å‹å‚æ•°"""
                            return self.model.parameters()

                        def named_parameters(self):
                            """è¿”å›å‘½åå‚æ•°"""
                            if hasattr(self.model, 'named_parameters'):
                                return self.model.named_parameters()
                            return []

                        def state_dict(self):
                            """è¿”å›çŠ¶æ€å­—å…¸"""
                            if hasattr(self.model, 'state_dict'):
                                return self.model.state_dict()
                            return {}

                        def __getattr__(self, name):
                            """è½¬å‘å±æ€§è®¿é—®åˆ°åŸå§‹æ¨¡å‹"""
                            try:
                                return super().__getattr__(name)
                            except AttributeError:
                                try:
                                    return getattr(self.model, name)
                                except AttributeError:
                                    raise AttributeError(
                                        f"TorchScriptæ¨¡å‹æ²¡æœ‰å±æ€§: {name}"
                                    )

                    # åˆ›å»ºåŒ…è£…å™¨
                    model = TorchScriptWrapper(jit_model, input_size)
                    self.preprocess = standard_preprocess

                    # è¾“å‡ºä¿¡æ¯
                    print("âœ“ æˆåŠŸåŠ è½½TorchScriptæ¨¡å‹")
                    print(f"âœ“ æ¨¡å‹å›ºå®šåœ¨: cuda:0")
                    print(f"âœ“ è¾“å…¥å°ºå¯¸: {input_size}x{input_size}")
                    print(f"âœ“ ç‰¹å¾ç»´åº¦: {model.width}")

                    # è­¦å‘Šä¿¡æ¯
                    if self.device != torch.device('cuda:0') and self.device != torch.device('cuda'):
                        print(f"âš ï¸ æ³¨æ„: æ‚¨æŒ‡å®šçš„è®¾å¤‡æ˜¯ {self.device}")
                        print(f"âš ï¸ ä½†TorchScriptæ¨¡å‹å°†ä¿æŒåœ¨cuda:0")
                        print(f"âš ï¸ æ‰€æœ‰è¾“å…¥å°†è‡ªåŠ¨è½¬ç§»åˆ°cuda:0è¿›è¡Œå¤„ç†")

                    return model

                except Exception as e:
                    print(f"âœ— åŠ è½½TorchScriptæ¨¡å‹å¤±è´¥: {e}")
                    import traceback
                    traceback.print_exc()
                    print("å°è¯•å…¶ä»–åŠ è½½æ–¹å¼...")

        # ========== 2. ä½¿ç”¨open_clipåŠ è½½é¢„è®­ç»ƒæ¨¡å‹ ==========
        try:
            print("å°è¯•ä½¿ç”¨open_clipåŠ è½½é¢„è®­ç»ƒæ¨¡å‹...")
            model, _, preprocess = open_clip.create_model_and_transforms(
                model_name_converted,
                pretrained=pretrained if pretrained != 'openai' else 'openai',
                cache_dir=str(cache_dir)
            )
            model = model.to(self.device)
            model.eval()
            self.preprocess = preprocess
            print(f"âœ“ æˆåŠŸåŠ è½½open_clipé¢„è®­ç»ƒæ¨¡å‹åˆ° {self.device}")
            return model
        except Exception as e:
            print(f"âœ— open_clipåŠ è½½å¤±è´¥: {e}")

        # ========== 3. åˆ›å»ºæ— é¢„è®­ç»ƒæƒé‡çš„æ¨¡å‹ ==========
        print("âš ï¸ å›é€€åˆ°æ— é¢„è®­ç»ƒæƒé‡æ¨¡å‹")
        try:
            model, _, preprocess = open_clip.create_model_and_transforms(
                model_name_converted,
                pretrained=None
            )
            model = model.to(self.device)
            model.eval()

            self.preprocess = preprocess if preprocess is not None else standard_preprocess
            print(f"âœ“ æˆåŠŸåˆ›å»ºæ— é¢„è®­ç»ƒæƒé‡æ¨¡å‹åˆ° {self.device}")
            print("âš ï¸ æ­¤æ¨¡å‹éœ€è¦é‡æ–°è®­ç»ƒæ‰èƒ½æ­£å¸¸ä½¿ç”¨")
            return model
        except Exception as e:
            print(f"âœ— åˆ›å»ºæ¨¡å‹å¤±è´¥: {e}")
            raise RuntimeError(f"æ— æ³•åŠ è½½æˆ–åˆ›å»ºCLIPæ¨¡å‹: {e}")

    def forward(self, batch: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """
        GeoCLIPå‰å‘ä¼ æ’­

        Args:
            batch: æ‰¹æ¬¡æ•°æ®ï¼ŒåŒ…å«ï¼š
                - 'image': RGBå›¾åƒ [B, 3, H, W]
                - 'depth': æ·±åº¦å›¾ [B, 1, H, W] (å¯é€‰ï¼Œå¦‚æœæ²¡æœ‰ä¼šè‡ªåŠ¨ä¼°è®¡)
                - 'text': æ–‡æœ¬token (å¯é€‰)

        Returns:
            è¾“å‡ºå­—å…¸ï¼ŒåŒ…å«ï¼š
                - 'anomaly_score': å¼‚å¸¸åˆ†æ•°
                - 'clip_features': CLIPç‰¹å¾
                - 'geometry_features': å‡ ä½•ç‰¹å¾
                - 'fused_features': èåˆç‰¹å¾
        """
        images = batch['image']
        batch_size = images.size(0)

        # 1. è·å–æ·±åº¦å›¾
        if 'depth' in batch and batch['depth'] is not None:
            depth_maps = batch['depth']
        else:
            # ä½¿ç”¨æ·±åº¦ä¼°è®¡å™¨ä¼°è®¡æ·±åº¦
            with torch.no_grad():
                depth_maps = self.depth_estimator(images)

        # 2. CLIPç‰¹å¾æå–
        if 'text' in batch:
            # å¦‚æœæœ‰æ–‡æœ¬ï¼Œä½¿ç”¨å›¾åƒ-æ–‡æœ¬å¯¹æ¯”
            clip_features = self.clip_model.encode_image(images)
        else:
            # ä»…ä½¿ç”¨å›¾åƒç¼–ç 
            clip_features = self.clip_model.encode_image(images)

        # 3. è½¬æ¢ä¸ºä½“ç´ è¡¨ç¤º
        # åˆå¹¶RGBå’Œæ·±åº¦ä¿¡æ¯
        rgbd_images = torch.cat([images, depth_maps], dim=1)  # [B, 4, H, W]

        # è½¬æ¢ä¸º3Dä½“ç´ 
        voxels = self.voxel_converter.images_to_voxels(rgbd_images)  # [B, 4, D, H, W]

        # 4. 3Då‡ ä½•ç‰¹å¾æå–
        geometry_features = self.geometry_encoder(voxels)

        # 5. ç‰¹å¾èåˆ
        fused_features = self.fusion_module(clip_features, geometry_features)

        # 6. å¼‚å¸¸æ£€æµ‹
        anomaly_predictions = self.anomaly_head(fused_features)

        # è¿”å›ç»“æœ
        results = {
            'anomaly_predictions': anomaly_predictions,
            'clip_features': clip_features,
            'geometry_features': geometry_features,
            'fused_features': fused_features,
            'depth_maps': depth_maps
        }

        return results

    def predict_anomaly(self, batch: Dict[str, torch.Tensor]) -> torch.Tensor:
        """
        é¢„æµ‹å¼‚å¸¸åˆ†æ•°

        Args:
            batch: è¾“å…¥æ‰¹æ¬¡

        Returns:
            anomaly_scores: å¼‚å¸¸åˆ†æ•° [B] æˆ– [B, num_classes]
        """
        with torch.no_grad():
            results = self.forward(batch)
            predictions = results['anomaly_predictions']

            if self.anomaly_head.detection_type == "regression":
                return predictions.squeeze(-1)  # [B]
            else:
                # åˆ†ç±»æƒ…å†µï¼Œè¿”å›å¼‚å¸¸ç±»åˆ«çš„æ¦‚ç‡
                return F.softmax(predictions, dim=-1)[:, 1]  # [B]

    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)

        return {
            'total_parameters': total_params,
            'trainable_parameters': trainable_params,
            'clip_frozen': self.freeze_clip,
            'fusion_type': self.fusion_module.fusion_type,
            'detection_type': self.anomaly_head.detection_type,
            'device': self.device
        }


def create_geoclip_model(config: Dict[str, Any]) -> GeoCLIP:
    """
    å·¥å‚å‡½æ•°ï¼šæ ¹æ®é…ç½®åˆ›å»ºGeoCLIPæ¨¡å‹

    Args:
        config: æ¨¡å‹é…ç½®å­—å…¸

    Returns:
        GeoCLIPæ¨¡å‹å®ä¾‹
    """
    return GeoCLIP(
        clip_model_name=config.get('clip_model', 'ViT-B/16'),
        clip_pretrained=config.get('clip_pretrained', 'openai'),
        depth_estimator_type=config.get('depth_estimator', 'DPT_Large'),
        geometry_encoder_config=config.get('geometry_encoder', None),
        fusion_type=config.get('fusion_type', 'cross_attention'),
        fusion_dim=config.get('fusion_dim', 1024),
        output_dim=config.get('output_dim', 512),
        detection_type=config.get('detection_type', 'regression'),
        num_classes=config.get('num_classes', 2),
        device=config.get('device', 'cuda'),
        freeze_clip=config.get('freeze_clip', False)
    )


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    print("=== æµ‹è¯•GeoCLIPæ¨¡å‹ ===")

    try:
        # åˆ›å»ºæ¨¡å‹é…ç½®
        config = {
            'clip_model': 'ViT-B/16',
            'depth_estimator': 'DPT_Large',
            'fusion_type': 'cross_attention',
            'detection_type': 'regression',
            'device': 'cuda' if torch.cuda.is_available() else 'cpu'
        }

        # åˆ›å»ºæ¨¡å‹
        model = create_geoclip_model(config)
        print(f"âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ")
        print(f"æ¨¡å‹ä¿¡æ¯: {model.get_model_info()}")

        # åˆ›å»ºæµ‹è¯•æ•°æ®
        batch_size = 2
        test_batch = {
            'image': torch.randn(batch_size, 3, 224, 224),
            # 'depth': torch.randn(batch_size, 1, 224, 224),  # å¯é€‰
        }

        # ç§»åŠ¨åˆ°è®¾å¤‡
        device = config['device']
        model = model.to(device)
        for key in test_batch:
            test_batch[key] = test_batch[key].to(device)

        # å‰å‘ä¼ æ’­æµ‹è¯•
        print("\næµ‹è¯•å‰å‘ä¼ æ’­...")
        results = model(test_batch)

        print(f"âœ… å‰å‘ä¼ æ’­æˆåŠŸ:")
        for key, value in results.items():
            if isinstance(value, torch.Tensor):
                print(f"  {key}: {value.shape}")

        # å¼‚å¸¸é¢„æµ‹æµ‹è¯•
        anomaly_scores = model.predict_anomaly(test_batch)
        print(f"âœ… å¼‚å¸¸é¢„æµ‹: {anomaly_scores.shape}")
        print(f"å¼‚å¸¸åˆ†æ•°èŒƒå›´: {anomaly_scores.min():.3f} - {anomaly_scores.max():.3f}")

        print("\nğŸ‰ GeoCLIPæ¨¡å‹æµ‹è¯•å®Œæˆ!")

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()