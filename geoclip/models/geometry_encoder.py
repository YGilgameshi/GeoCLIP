
"""
GeoCLIP - 3Då‡ ä½•ç¼–ç å™¨
å¤„ç†3Dä½“ç´ æ•°æ®çš„æ·±åº¦ç½‘ç»œ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from typing import List, Optional, Tuple, Dict, Union


class Conv3DBlock(nn.Module):
    """3Då·ç§¯å—"""

    def __init__(self,
                 in_channels: int,
                 out_channels: int,
                 kernel_size: int = 3,
                 stride: int = 1,
                 padding: int = 1,
                 use_bn: bool = True,
                 activation: str = 'relu',
                 dropout: float = 0.0):
        super(Conv3DBlock, self).__init__()

        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size,
                              stride=stride, padding=padding, bias=not use_bn)

        self.bn = nn.BatchNorm3d(out_channels) if use_bn else nn.Identity()

        if activation == 'relu':
            self.activation = nn.ReLU(inplace=True)
        elif activation == 'gelu':
            self.activation = nn.GELU()
        elif activation == 'leaky_relu':
            self.activation = nn.LeakyReLU(0.2, inplace=True)
        else:
            self.activation = nn.Identity()

        self.dropout = nn.Dropout3d(dropout) if dropout > 0 else nn.Identity()

    def forward(self, x):
        x = self.conv(x)
        x = self.bn(x)
        x = self.activation(x)
        x = self.dropout(x)
        return x


class ResNet3DBlock(nn.Module):
    """3D ResNetå—"""

    def __init__(self,
                 in_channels: int,
                 out_channels: int,
                 stride: int = 1,
                 downsample: Optional[nn.Module] = None):
        super(ResNet3DBlock, self).__init__()

        self.conv1 = Conv3DBlock(in_channels, out_channels, kernel_size=3,
                                 stride=stride, padding=1)
        self.conv2 = Conv3DBlock(out_channels, out_channels, kernel_size=3,
                                 stride=1, padding=1, activation='none')

        self.downsample = downsample
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        identity = x

        out = self.conv1(x)
        out = self.conv2(out)

        if self.downsample is not None:
            identity = self.downsample(x)

        out += identity
        out = self.relu(out)

        return out


class VoxelEncoder(nn.Module):
    """
    åŸºç¡€ä½“ç´ ç¼–ç å™¨
    ä½¿ç”¨3D CNNæå–ä½“ç´ ç‰¹å¾
    """

    def __init__(self,
                 in_channels: int = 3,
                 base_channels: int = 64,
                 num_stages: int = 4,
                 output_channels: int = 512,
                 voxel_size: int = 64):
        super(VoxelEncoder, self).__init__()

        self.voxel_size = voxel_size
        self.num_stages = num_stages

        # è¾“å…¥å±‚
        self.stem = Conv3DBlock(in_channels, base_channels, kernel_size=7,
                                stride=2, padding=3)

        # å¤šé˜¶æ®µç¼–ç å™¨
        self.stages = nn.ModuleList()
        current_channels = base_channels
        current_size = voxel_size // 2  # stemå·²ç»ä¸‹é‡‡æ ·2å€

        for i in range(num_stages):
            stage_channels = base_channels * (2 ** i)

            # ä¸‹é‡‡æ ·å±‚
            if i > 0:
                downsample = Conv3DBlock(current_channels, stage_channels,
                                         kernel_size=3, stride=2, padding=1)
                current_size = current_size // 2
            else:
                downsample = Conv3DBlock(current_channels, stage_channels,
                                         kernel_size=3, stride=1, padding=1)

            # ResNetå—
            blocks = [ResNet3DBlock(stage_channels, stage_channels)]
            blocks.append(ResNet3DBlock(stage_channels, stage_channels))

            stage = nn.Sequential(downsample, *blocks)
            self.stages.append(stage)
            current_channels = stage_channels

        # å…¨å±€å¹³å‡æ± åŒ–
        self.global_pool = nn.AdaptiveAvgPool3d(1)

        # è¾“å‡ºæŠ•å½±
        self.output_proj = nn.Linear(current_channels, output_channels)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­

        Args:
            x: è¾“å…¥ä½“ç´  [B, C, D, H, W]

        Returns:
            global_feature: å…¨å±€ç‰¹å¾ [B, output_channels]
        """
        # å­˜å‚¨å¤šå°ºåº¦ç‰¹å¾
        multi_scale_features = []

        # Stem
        x = self.stem(x)  # [B, base_channels, D/2, H/2, W/2]
        multi_scale_features.append(x)

        # å¤šé˜¶æ®µå¤„ç†
        for stage in self.stages:
            x = stage(x)
            multi_scale_features.append(x)

        # å…¨å±€ç‰¹å¾
        global_feature = self.global_pool(x)  # [B, C, 1, 1, 1]
        global_feature = global_feature.flatten(1)  # [B, C]
        global_feature = self.output_proj(global_feature)  # [B, output_channels]

        return global_feature


class SparseVoxelEncoder(nn.Module):
    """
    ç¨€ç–ä½“ç´ ç¼–ç å™¨
    ä¸“é—¨å¤„ç†ç¨€ç–ä½“ç´ æ•°æ®ï¼Œæé«˜å†…å­˜æ•ˆç‡
    """

    def __init__(self,
                 in_channels: int = 3,
                 hidden_channels: int = 128,
                 output_channels: int = 512,
                 num_layers: int = 4):
        super(SparseVoxelEncoder, self).__init__()

        # ç‚¹çº§ç‰¹å¾ç¼–ç å™¨
        self.point_encoder = nn.Sequential(
            nn.Linear(in_channels + 3, hidden_channels),  # +3 for 3D coordinates
            nn.ReLU(inplace=True),
            nn.Linear(hidden_channels, hidden_channels),
            nn.ReLU(inplace=True)
        )

        # å¤šå±‚æ„ŸçŸ¥æœº
        layers = []
        current_channels = hidden_channels

        for i in range(num_layers - 1):
            next_channels = hidden_channels if i < num_layers - 2 else output_channels
            layers.extend([
                nn.Linear(current_channels, next_channels),
                nn.ReLU(inplace=True) if i < num_layers - 2 else nn.Identity()
            ])
            current_channels = next_channels

        self.mlp = nn.Sequential(*layers)

        # å…¨å±€ç‰¹å¾èšåˆ
        self.global_aggregator = nn.Sequential(
            nn.Linear(output_channels, output_channels),
            nn.ReLU(inplace=True),
            nn.Linear(output_channels, output_channels)
        )

    def forward(self, sparse_voxels: Union[Dict[str, torch.Tensor], torch.Tensor]) -> torch.Tensor:
        """
        å¤„ç†ç¨€ç–ä½“ç´ æ•°æ®

        Args:
            sparse_voxels: ç¨€ç–ä½“ç´ æ•°æ®å­—å…¸æˆ–å¯†é›†ä½“ç´ å¼ é‡

        Returns:
            global_feature: å…¨å±€ç‰¹å¾ [B, output_channels]
        """
        # å¦‚æœè¾“å…¥æ˜¯å¯†é›†ä½“ç´ ï¼Œè½¬æ¢ä¸ºç¨€ç–æ ¼å¼
        if isinstance(sparse_voxels, torch.Tensor):
            return self._process_dense_voxels(sparse_voxels)

        # å¤„ç†ç¨€ç–ä½“ç´ å­—å…¸
        indices = sparse_voxels['indices']  # [N, 4] (B, X, Y, Z)
        values = sparse_voxels['values']  # [N, C]
        shape = sparse_voxels['shape']  # (B, C, D, H, W)

        batch_size = shape[0]

        if indices.shape[0] == 0:
            # å¦‚æœæ²¡æœ‰æœ‰æ•ˆä½“ç´ ï¼Œè¿”å›é›¶ç‰¹å¾
            return torch.zeros(batch_size, self.global_aggregator[-1].out_features,
                               device=indices.device)

        # æå–3Dåæ ‡
        coords_3d = indices[:, 1:4].float()  # [N, 3]

        # å½’ä¸€åŒ–åæ ‡åˆ°[-1, 1]
        voxel_size = shape[2]  # å‡è®¾D=H=W
        coords_3d = (coords_3d / (voxel_size - 1)) * 2 - 1

        # ç»„åˆåæ ‡å’Œé¢œè‰²ç‰¹å¾
        point_features = torch.cat([coords_3d, values], dim=1)  # [N, 3+C]

        # ç¼–ç ç‚¹ç‰¹å¾
        encoded_features = self.point_encoder(point_features)  # [N, hidden_channels]
        encoded_features = self.mlp(encoded_features)  # [N, output_channels]

        # æŒ‰batchèšåˆç‰¹å¾
        batch_features = []
        for b in range(batch_size):
            batch_mask = (indices[:, 0] == b)
            if batch_mask.any():
                batch_point_features = encoded_features[batch_mask]
                # ä½¿ç”¨å¹³å‡æ± åŒ–èšåˆ
                aggregated = batch_point_features.mean(dim=0)
            else:
                aggregated = torch.zeros(encoded_features.shape[1],
                                         device=encoded_features.device)

            batch_features.append(aggregated)

        # å †å å¹¶é€šè¿‡å…¨å±€èšåˆå™¨
        global_features = torch.stack(batch_features, dim=0)  # [B, output_channels]
        global_features = self.global_aggregator(global_features)

        return global_features

    def _process_dense_voxels(self, voxels: torch.Tensor) -> torch.Tensor:
        """å¤„ç†å¯†é›†ä½“ç´ æ•°æ®"""
        B, C, D, H, W = voxels.shape
        device = voxels.device

        # ç®€åŒ–å¤„ç†ï¼šå…¨å±€å¹³å‡æ± åŒ– + MLP
        global_features = torch.mean(voxels, dim=[2, 3, 4])  # [B, C]

        # æŠ•å½±åˆ°è¾“å‡ºç»´åº¦
        if hasattr(self, '_dense_proj'):
            projected = self._dense_proj(global_features)
        else:
            # åŠ¨æ€åˆ›å»ºæŠ•å½±å±‚
            self._dense_proj = nn.Linear(C, self.global_aggregator[-1].out_features).to(device)
            projected = self._dense_proj(global_features)

        return self.global_aggregator(projected)


class HierarchicalVoxelEncoder(nn.Module):
    """
    å±‚æ¬¡åŒ–ä½“ç´ ç¼–ç å™¨
    åœ¨å¤šä¸ªåˆ†è¾¨ç‡çº§åˆ«å¤„ç†ä½“ç´ æ•°æ®
    """

    def __init__(self,
                 in_channels: int = 3,
                 scales: List[int] = [16, 32, 64],
                 base_channels: int = 64,
                 output_channels: int = 512):
        super(HierarchicalVoxelEncoder, self).__init__()

        self.scales = scales

        # ä¸ºæ¯ä¸ªå°ºåº¦åˆ›å»ºç¼–ç å™¨
        self.encoders = nn.ModuleList()
        scale_output_channels = output_channels // len(scales)

        for scale in scales:
            encoder = VoxelEncoder(
                in_channels=in_channels,
                base_channels=base_channels,
                num_stages=3,
                output_channels=scale_output_channels,
                voxel_size=scale
            )
            self.encoders.append(encoder)

        # ç‰¹å¾èåˆ - ä¿®å¤ç»´åº¦åŒ¹é…é—®é¢˜
        total_features = scale_output_channels * len(scales)
        self.fusion = nn.Sequential(
            nn.Linear(total_features, output_channels),
            nn.ReLU(inplace=True),
            nn.Linear(output_channels, output_channels)
        )

    def forward(self, voxels_dict: Union[Dict[int, torch.Tensor], torch.Tensor]) -> torch.Tensor:
        """
        å¤„ç†å¤šå°ºåº¦ä½“ç´ æ•°æ®

        Args:
            voxels_dict: ä¸åŒå°ºåº¦çš„ä½“ç´ æ•°æ®å­—å…¸æˆ–å•ä¸ªä½“ç´ å¼ é‡

        Returns:
            fused_features: èåˆçš„å¤šå°ºåº¦ç‰¹å¾
        """
        # å¦‚æœè¾“å…¥æ˜¯å•ä¸ªå¼ é‡ï¼Œä¸ºæ‰€æœ‰å°ºåº¦å¤åˆ¶
        if isinstance(voxels_dict, torch.Tensor):
            original_voxels = voxels_dict
            voxels_dict = {}
            for scale in self.scales:
                # é‡é‡‡æ ·åˆ°å¯¹åº”å°ºåº¦
                resampled = F.interpolate(
                    original_voxels,
                    size=(scale, scale, scale),
                    mode='trilinear',
                    align_corners=False
                )
                voxels_dict[scale] = resampled

        scale_features = []

        for i, (encoder, scale) in enumerate(zip(self.encoders, self.scales)):
            if scale in voxels_dict:
                voxels = voxels_dict[scale]
                # åªå–å…¨å±€ç‰¹å¾ï¼Œå¿½ç•¥å¤šå°ºåº¦ç‰¹å¾
                global_feat = encoder(voxels)
                scale_features.append(global_feat)
            else:
                # å¦‚æœæŸä¸ªå°ºåº¦çš„æ•°æ®ä¸å­˜åœ¨ï¼Œç”¨é›¶å¡«å……
                batch_size = next(iter(voxels_dict.values())).shape[0]
                device = next(iter(voxels_dict.values())).device
                zero_feat = torch.zeros(batch_size, encoder.output_proj.out_features,
                                        device=device)
                scale_features.append(zero_feat)

        # è¿æ¥æ‰€æœ‰å°ºåº¦çš„ç‰¹å¾
        fused_features = torch.cat(scale_features, dim=1)

        # ç‰¹å¾èåˆ
        fused_features = self.fusion(fused_features)

        return fused_features


class GeometryAwareEncoder(nn.Module):
    """
    å‡ ä½•æ„ŸçŸ¥ç¼–ç å™¨
    æ˜¾å¼å»ºæ¨¡å‡ ä½•ç»“æ„å’Œç©ºé—´å…³ç³»
    """

    def __init__(self,
                 in_channels: int = 3,
                 hidden_channels: int = 128,
                 output_channels: int = 512,
                 num_heads: int = 8,
                 num_layers: int = 4):
        super(GeometryAwareEncoder, self).__init__()

        # åŸºç¡€ç‰¹å¾æå–
        self.feature_extractor = VoxelEncoder(
            in_channels=in_channels,
            base_channels=64,
            num_stages=3,
            output_channels=hidden_channels
        )

        # å‡ ä½•æ³¨æ„åŠ›å±‚
        self.geometry_attention = nn.ModuleList([
            GeometryAttentionLayer(hidden_channels, num_heads)
            for _ in range(num_layers)
        ])

        # è¾“å‡ºæŠ•å½±
        self.output_proj = nn.Linear(hidden_channels, output_channels)

    def forward(self, voxels: torch.Tensor) -> torch.Tensor:
        """
        å‡ ä½•æ„ŸçŸ¥ç¼–ç 

        Args:
            voxels: ä½“ç´ æ•°æ® [B, C, D, H, W]

        Returns:
            geometry_features: å‡ ä½•æ„ŸçŸ¥ç‰¹å¾
        """
        # æå–åŸºç¡€ç‰¹å¾
        features = self.feature_extractor(voxels)

        # åº”ç”¨å‡ ä½•æ³¨æ„åŠ› (è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œä¸ä½¿ç”¨å¤šå°ºåº¦ç‰¹å¾)
        enhanced_features = features
        for attn_layer in self.geometry_attention:
            enhanced_features = attn_layer(enhanced_features, None)

        # è¾“å‡ºæŠ•å½±
        geometry_features = self.output_proj(enhanced_features)

        return geometry_features


class GeometryAttentionLayer(nn.Module):
    """å‡ ä½•æ³¨æ„åŠ›å±‚"""

    def __init__(self, channels: int, num_heads: int = 8):
        super(GeometryAttentionLayer, self).__init__()

        self.channels = channels
        self.num_heads = num_heads
        self.head_dim = channels // num_heads

        self.q_proj = nn.Linear(channels, channels)
        self.k_proj = nn.Linear(channels, channels)
        self.v_proj = nn.Linear(channels, channels)

        self.out_proj = nn.Linear(channels, channels)

        # å‡ ä½•ä½ç½®ç¼–ç 
        self.geometry_pos_encoding = nn.Parameter(torch.randn(1, channels))

    def forward(self, global_features: torch.Tensor,
                spatial_features: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        å‡ ä½•æ³¨æ„åŠ›è®¡ç®—

        Args:
            global_features: å…¨å±€ç‰¹å¾ [B, C]
            spatial_features: ç©ºé—´ç‰¹å¾ [B, C, D, H, W] (å¯é€‰)

        Returns:
            enhanced_features: å¢å¼ºçš„å‡ ä½•ç‰¹å¾
        """
        B, C = global_features.shape

        if spatial_features is not None:
            # ä½¿ç”¨ç©ºé—´ç‰¹å¾è¿›è¡Œæ³¨æ„åŠ›è®¡ç®—
            # å±•å¹³ç©ºé—´ç‰¹å¾
            spatial_flat = spatial_features.flatten(2).transpose(1, 2)  # [B, D*H*W, C]

            # æ·»åŠ å‡ ä½•ä½ç½®ç¼–ç 
            spatial_flat = spatial_flat + self.geometry_pos_encoding

            # è®¡ç®—æ³¨æ„åŠ›
            q = self.q_proj(global_features).unsqueeze(1)  # [B, 1, C]
            k = self.k_proj(spatial_flat)  # [B, D*H*W, C]
            v = self.v_proj(spatial_flat)  # [B, D*H*W, C]

            # å¤šå¤´æ³¨æ„åŠ›
            q = q.view(B, 1, self.num_heads, self.head_dim).transpose(1, 2)
            k = k.view(B, -1, self.num_heads, self.head_dim).transpose(1, 2)
            v = v.view(B, -1, self.num_heads, self.head_dim).transpose(1, 2)

            # è®¡ç®—æ³¨æ„åŠ›æƒé‡
            attn_weights = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)
            attn_weights = F.softmax(attn_weights, dim=-1)

            # åº”ç”¨æ³¨æ„åŠ›
            attn_output = torch.matmul(attn_weights, v)  # [B, num_heads, 1, head_dim]
            attn_output = attn_output.transpose(1, 2).contiguous().view(B, 1, C)

            # è¾“å‡ºæŠ•å½±
            enhanced_features = self.out_proj(attn_output.squeeze(1))

            # æ®‹å·®è¿æ¥
            enhanced_features = enhanced_features + global_features
        else:
            # å¦‚æœæ²¡æœ‰ç©ºé—´ç‰¹å¾ï¼Œç›´æ¥ä½¿ç”¨è‡ªæ³¨æ„åŠ›
            enhanced_features = self.out_proj(global_features) + global_features

        return enhanced_features


def create_geometry_encoder(config: dict) -> nn.Module:
    """
    å·¥å‚å‡½æ•°ï¼šæ ¹æ®é…ç½®åˆ›å»ºå‡ ä½•ç¼–ç å™¨

    Args:
        config: é…ç½®å­—å…¸

    Returns:
        encoder: å‡ ä½•ç¼–ç å™¨å®ä¾‹
    """
    encoder_type = config.get('type', 'voxel')

    if encoder_type == 'voxel':
        return VoxelEncoder(
            in_channels=config.get('in_channels', 3),
            base_channels=config.get('base_channels', 64),
            num_stages=config.get('num_stages', 4),
            output_channels=config.get('output_channels', 512),
            voxel_size=config.get('voxel_size', 64)
        )
    elif encoder_type == 'sparse':
        return SparseVoxelEncoder(
            in_channels=config.get('in_channels', 3),
            hidden_channels=config.get('hidden_channels', 128),
            output_channels=config.get('output_channels', 512),
            num_layers=config.get('num_layers', 4)
        )
    elif encoder_type == 'hierarchical':
        return HierarchicalVoxelEncoder(
            in_channels=config.get('in_channels', 3),
            scales=config.get('scales', [16, 32, 64]),
            base_channels=config.get('base_channels', 64),
            output_channels=config.get('output_channels', 512)
        )
    elif encoder_type == 'geometry_aware':
        return GeometryAwareEncoder(
            in_channels=config.get('in_channels', 3),
            hidden_channels=config.get('hidden_channels', 128),
            output_channels=config.get('output_channels', 512),
            num_heads=config.get('num_heads', 8),
            num_layers=config.get('num_layers', 4)
        )
    else:
        raise ValueError(f"Unknown encoder type: {encoder_type}")


# ç¤ºä¾‹ä½¿ç”¨å’Œæµ‹è¯•
if __name__ == "__main__":
    print("=== æµ‹è¯•å‡ ä½•ç¼–ç å™¨ ===")

    try:
        # æµ‹è¯•åŸºç¡€ä½“ç´ ç¼–ç å™¨
        print("\n1. æµ‹è¯•åŸºç¡€ä½“ç´ ç¼–ç å™¨")
        encoder = VoxelEncoder(in_channels=3, voxel_size=64)

        # åˆ›å»ºæµ‹è¯•æ•°æ®
        batch_size = 2
        voxel_input = torch.randn(batch_size, 3, 64, 64, 64)

        # å‰å‘ä¼ æ’­
        global_feat, multi_scale_feat = encoder(voxel_input)

        print(f"âœ… è¾“å…¥ä½“ç´ å½¢çŠ¶: {voxel_input.shape}")
        print(f"âœ… å…¨å±€ç‰¹å¾å½¢çŠ¶: {global_feat.shape}")
        print(f"âœ… å¤šå°ºåº¦ç‰¹å¾æ•°é‡: {len(multi_scale_feat)}")
        for i, feat in enumerate(multi_scale_feat):
            print(f"   å°ºåº¦ {i}: {feat.shape}")

        # æµ‹è¯•ç¨€ç–ä½“ç´ ç¼–ç å™¨
        print("\n2. æµ‹è¯•ç¨€ç–ä½“ç´ ç¼–ç å™¨")
        sparse_encoder = SparseVoxelEncoder(in_channels=3)

        # åˆ›å»ºç¨€ç–ä½“ç´ æµ‹è¯•æ•°æ®
        sparse_data = {
            'indices': torch.tensor([[0, 10, 20, 30], [0, 15, 25, 35],
                                     [1, 5, 15, 25]], dtype=torch.long),
            'values': torch.randn(3, 3),
            'shape': (2, 3, 64, 64, 64)
        }

        sparse_feat = sparse_encoder(sparse_data)
        print(f"âœ… ç¨€ç–ä½“ç´ ç‰¹å¾å½¢çŠ¶: {sparse_feat.shape}")

        # æµ‹è¯•å±‚æ¬¡åŒ–ä½“ç´ ç¼–ç å™¨
        print("\n3. æµ‹è¯•å±‚æ¬¡åŒ–ä½“ç´ ç¼–ç å™¨")
        hierarchical_encoder = HierarchicalVoxelEncoder()

        # åˆ›å»ºå¤šå°ºåº¦ä½“ç´ æ•°æ®
        multi_scale_voxels = {
            16: torch.randn(batch_size, 3, 16, 16, 16),
            32: torch.randn(batch_size, 3, 32, 32, 32),
            64: torch.randn(batch_size, 3, 64, 64, 64)
        }

        hierarchical_feat = hierarchical_encoder(multi_scale_voxels)
        print(f"âœ… å±‚æ¬¡åŒ–ç‰¹å¾å½¢çŠ¶: {hierarchical_feat.shape}")

        # æµ‹è¯•å‡ ä½•æ„ŸçŸ¥ç¼–ç å™¨
        print("\n4. æµ‹è¯•å‡ ä½•æ„ŸçŸ¥ç¼–ç å™¨")
        geo_encoder = GeometryAwareEncoder(in_channels=3)
        geo_feat = geo_encoder(voxel_input)
        print(f"âœ… å‡ ä½•æ„ŸçŸ¥ç‰¹å¾å½¢çŠ¶: {geo_feat.shape}")

        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback

        traceback.print_exc()