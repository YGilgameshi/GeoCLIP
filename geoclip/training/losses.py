

"""
GeoCLIP - æŸå¤±å‡½æ•°æ¨¡å—
åŒ…å«å¯¹æ¯”å­¦ä¹ ã€å‡ ä½•ä¸€è‡´æ€§ã€å¼‚å¸¸æ£€æµ‹ç­‰æŸå¤±å‡½æ•°
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Optional, Tuple, Union, Any
import math


class ContrastiveLoss(nn.Module):
    """
    å¯¹æ¯”å­¦ä¹ æŸå¤± - ç”¨äº2Då’Œ3Dç‰¹å¾å¯¹é½
    """

    def __init__(self,
                 temperature: float = 0.07,
                 normalize: bool = True):
        super(ContrastiveLoss, self).__init__()
        self.temperature = temperature
        self.normalize = normalize

    def forward(self,
                features_2d: torch.Tensor,
                features_3d: torch.Tensor,
                labels: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        è®¡ç®—å¯¹æ¯”å­¦ä¹ æŸå¤±

        Args:
            features_2d: 2Dç‰¹å¾ [B, D]
            features_3d: 3Dç‰¹å¾ [B, D]
            labels: æ ‡ç­¾ [B] (å¯é€‰ï¼Œç”¨äºç›‘ç£å¯¹æ¯”å­¦ä¹ )

        Returns:
            loss: å¯¹æ¯”æŸå¤±
        """
        batch_size = features_2d.size(0)
        device = features_2d.device

        # æ ‡å‡†åŒ–ç‰¹å¾
        if self.normalize:
            features_2d = F.normalize(features_2d, dim=1)
            features_3d = F.normalize(features_3d, dim=1)

        # è®¡ç®—ç›¸ä¼¼æ€§çŸ©é˜µ
        similarity_matrix = torch.matmul(features_2d, features_3d.t()) / self.temperature

        # åˆ›å»ºæ­£æ ·æœ¬mask
        if labels is not None:
            # ç›‘ç£å¯¹æ¯”å­¦ä¹ ï¼šç›¸åŒæ ‡ç­¾ä¸ºæ­£æ ·æœ¬
            labels = labels.contiguous().view(-1, 1)
            mask = torch.eq(labels, labels.t()).float().to(device)
        else:
            # è‡ªç›‘ç£å¯¹æ¯”å­¦ä¹ ï¼šå¯¹è§’çº¿ä¸ºæ­£æ ·æœ¬
            mask = torch.eye(batch_size, device=device)

        # è®¡ç®—å¯¹æ¯”æŸå¤±
        # åˆ†å­ï¼šæ­£æ ·æœ¬çš„ç›¸ä¼¼æ€§
        pos_similarity = similarity_matrix * mask

        # åˆ†æ¯ï¼šæ‰€æœ‰æ ·æœ¬çš„ç›¸ä¼¼æ€§ï¼ˆé™¤äº†è‡ªå·±ï¼‰
        neg_mask = 1 - mask
        exp_sim = torch.exp(similarity_matrix)

        # è®¡ç®—æŸå¤±
        pos_sum = torch.sum(exp_sim * mask, dim=1)
        neg_sum = torch.sum(exp_sim * neg_mask, dim=1)

        loss = -torch.log(pos_sum / (pos_sum + neg_sum + 1e-8))

        return loss.mean()


class GeometryConsistencyLoss(nn.Module):
    """
    å‡ ä½•ä¸€è‡´æ€§æŸå¤± - ç¡®ä¿2Då’Œ3Dç‰¹å¾çš„å‡ ä½•å…³ç³»ä¸€è‡´
    """

    def __init__(self,
                 consistency_type: str = "distance",
                 lambda_weight: float = 1.0):
        super(GeometryConsistencyLoss, self).__init__()
        self.consistency_type = consistency_type
        self.lambda_weight = lambda_weight

    def forward(self,
                features_2d: torch.Tensor,
                features_3d: torch.Tensor,
                depth_maps: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—å‡ ä½•ä¸€è‡´æ€§æŸå¤±

        Args:
            features_2d: 2Dç‰¹å¾ [B, D]
            features_3d: 3Dç‰¹å¾ [B, D]
            depth_maps: æ·±åº¦å›¾ [B, 1, H, W]

        Returns:
            loss: å‡ ä½•ä¸€è‡´æ€§æŸå¤±
        """
        if self.consistency_type == "distance":
            return self._distance_consistency_loss(features_2d, features_3d, depth_maps)
        elif self.consistency_type == "ranking":
            return self._ranking_consistency_loss(features_2d, features_3d, depth_maps)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ä¸€è‡´æ€§ç±»å‹: {self.consistency_type}")

    def _distance_consistency_loss(self, features_2d, features_3d, depth_maps):
        """åŸºäºè·ç¦»çš„ä¸€è‡´æ€§æŸå¤±"""
        batch_size = features_2d.size(0)

        # è®¡ç®—ç‰¹å¾è·ç¦»çŸ©é˜µ
        dist_2d = torch.cdist(features_2d, features_2d)  # [B, B]
        dist_3d = torch.cdist(features_3d, features_3d)  # [B, B]

        # è®¡ç®—æ·±åº¦ç»Ÿè®¡é‡ä½œä¸ºå‡ ä½•çº¦æŸ
        depth_stats = []
        for b in range(batch_size):
            depth = depth_maps[b, 0]  # [H, W]
            mean_depth = depth.mean()
            std_depth = depth.std()
            depth_stats.append(torch.stack([mean_depth, std_depth]))

        depth_features = torch.stack(depth_stats)  # [B, 2]
        dist_depth = torch.cdist(depth_features, depth_features)  # [B, B]

        # ä¸€è‡´æ€§æŸå¤±ï¼šç‰¹å¾è·ç¦»åº”è¯¥ä¸æ·±åº¦è·ç¦»ç›¸å…³
        loss_2d = F.mse_loss(dist_2d, dist_depth)
        loss_3d = F.mse_loss(dist_3d, dist_depth)

        return self.lambda_weight * (loss_2d + loss_3d) / 2

    def _ranking_consistency_loss(self, features_2d, features_3d, depth_maps):
        """åŸºäºæ’åºçš„ä¸€è‡´æ€§æŸå¤±"""
        batch_size = features_2d.size(0)

        # è®¡ç®—æ·±åº¦æ’åº
        depth_means = torch.stack([depth_maps[b, 0].mean() for b in range(batch_size)])
        depth_order = torch.argsort(depth_means)

        # è®¡ç®—ç‰¹å¾ç›¸ä¼¼æ€§æ’åº
        sim_2d = torch.matmul(F.normalize(features_2d, dim=1),
                              F.normalize(features_2d, dim=1).t())
        sim_3d = torch.matmul(F.normalize(features_3d, dim=1),
                              F.normalize(features_3d, dim=1).t())

        # æ’åºä¸€è‡´æ€§æŸå¤±
        loss = 0
        for i in range(batch_size):
            for j in range(i + 1, batch_size):
                # æ·±åº¦é¡ºåº
                depth_order_ij = 1 if depth_means[i] < depth_means[j] else -1

                # ç‰¹å¾ç›¸ä¼¼æ€§é¡ºåº
                sim_2d_order = 1 if sim_2d[i, j] > sim_2d[j, i] else -1
                sim_3d_order = 1 if sim_3d[i, j] > sim_3d[j, i] else -1

                # ä¸€è‡´æ€§æŸå¤±
                loss += torch.relu(1 - depth_order_ij * sim_2d_order) ** 2
                loss += torch.relu(1 - depth_order_ij * sim_3d_order) ** 2

        return self.lambda_weight * loss / (batch_size * (batch_size - 1))


class AnomalyDetectionLoss(nn.Module):
    """
    å¼‚å¸¸æ£€æµ‹æŸå¤±å‡½æ•°
    """

    def __init__(self,
                 loss_type: str = "focal",
                 alpha: float = 0.25,
                 gamma: float = 2.0,
                 reduction: str = "mean"):
        super(AnomalyDetectionLoss, self).__init__()
        self.loss_type = loss_type
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction

    def forward(self,
                predictions: torch.Tensor,
                targets: torch.Tensor,
                sample_weights: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        è®¡ç®—å¼‚å¸¸æ£€æµ‹æŸå¤±

        Args:
            predictions: é¢„æµ‹ç»“æœ [B] æˆ– [B, num_classes]
            targets: çœŸå®æ ‡ç­¾ [B]
            sample_weights: æ ·æœ¬æƒé‡ [B] (å¯é€‰)

        Returns:
            loss: å¼‚å¸¸æ£€æµ‹æŸå¤±
        """
        # ç¡®ä¿targetsåœ¨æœ‰æ•ˆèŒƒå›´å†…
        if predictions.dim() > 1:
            num_classes = predictions.shape[1]
            targets = torch.clamp(targets, 0, num_classes - 1)
        else:
            # å¯¹äºå›å½’ä»»åŠ¡ï¼Œç¡®ä¿targetsåœ¨[0,1]èŒƒå›´å†…
            targets = torch.clamp(targets.float(), 0.0, 1.0)

        if self.loss_type == "focal":
            return self._focal_loss(predictions, targets, sample_weights)
        elif self.loss_type == "weighted_bce":
            return self._weighted_bce_loss(predictions, targets, sample_weights)
        elif self.loss_type == "dice":
            return self._dice_loss(predictions, targets, sample_weights)
        elif self.loss_type == "mse":
            return self._mse_loss(predictions, targets, sample_weights)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æŸå¤±ç±»å‹: {self.loss_type}")

    def _focal_loss(self, predictions, targets, sample_weights):
        """Focal Loss - å¤„ç†ç±»åˆ«ä¸å¹³è¡¡"""
        if predictions.dim() == 1:
            # å›å½’æƒ…å†µï¼Œè½¬æ¢ä¸ºäºŒåˆ†ç±»æ¦‚ç‡
            predictions = predictions.unsqueeze(1)
            prob_neg = 1 - predictions
            probs = torch.cat([prob_neg, predictions], dim=1)
            targets = targets.long()
        else:
            # åˆ†ç±»æƒ…å†µ
            probs = F.softmax(predictions, dim=1)
            targets = targets.long()

        # ç¡®ä¿targetsåœ¨æœ‰æ•ˆèŒƒå›´å†…
        num_classes = probs.shape[1]
        targets = torch.clamp(targets, 0, num_classes - 1)

        # è®¡ç®—äº¤å‰ç†µ
        ce_loss = F.cross_entropy(probs, targets, reduction='none')

        # è®¡ç®—æ¦‚ç‡
        pt = torch.gather(probs, 1, targets.unsqueeze(1)).squeeze(1)

        # Focal weight
        focal_weight = self.alpha * (1 - pt) ** self.gamma

        # åŠ æƒæŸå¤±
        focal_loss = focal_weight * ce_loss

        if sample_weights is not None:
            focal_loss = focal_loss * sample_weights

        if self.reduction == "mean":
            return focal_loss.mean()
        elif self.reduction == "sum":
            return focal_loss.sum()
        else:
            return focal_loss

    def _weighted_bce_loss(self, predictions, targets, sample_weights):
        """åŠ æƒäºŒå…ƒäº¤å‰ç†µæŸå¤±"""
        if predictions.dim() > 1:
            predictions = predictions[:, -1]  # å–æœ€åä¸€ä¸ªç±»åˆ«çš„æ¦‚ç‡

        targets = targets.float()

        # è®¡ç®—ç±»åˆ«æƒé‡
        pos_count = (targets == 1).sum().float()
        neg_count = (targets == 0).sum().float()

        if pos_count > 0 and neg_count > 0:
            pos_weight = neg_count / pos_count
        else:
            pos_weight = 1.0

        # BCEæŸå¤±
        bce_loss = F.binary_cross_entropy_with_logits(
            predictions, targets,
            pos_weight=pos_weight,
            reduction='none'
        )

        if sample_weights is not None:
            bce_loss = bce_loss * sample_weights

        if self.reduction == "mean":
            return bce_loss.mean()
        elif self.reduction == "sum":
            return bce_loss.sum()
        else:
            return bce_loss

    def _dice_loss(self, predictions, targets, sample_weights):
        """Dice Loss - å¤„ç†åˆ†å‰²ä»»åŠ¡"""
        if predictions.dim() > 1:
            predictions = F.softmax(predictions, dim=1)[:, -1]

        targets = targets.float()

        # Diceç³»æ•°
        intersection = (predictions * targets).sum()
        dice_coeff = (2 * intersection + 1e-8) / (predictions.sum() + targets.sum() + 1e-8)

        # DiceæŸå¤±
        dice_loss = 1 - dice_coeff

        if sample_weights is not None:
            dice_loss = dice_loss * sample_weights.mean()

        return dice_loss

    def _mse_loss(self, predictions, targets, sample_weights):
        """å‡æ–¹è¯¯å·®æŸå¤± - ç”¨äºå›å½’ä»»åŠ¡"""
        if predictions.dim() > 1:
            predictions = predictions.squeeze(-1)

        targets = targets.float()

        mse_loss = F.mse_loss(predictions, targets, reduction='none')

        if sample_weights is not None:
            mse_loss = mse_loss * sample_weights

        if self.reduction == "mean":
            return mse_loss.mean()
        elif self.reduction == "sum":
            return mse_loss.sum()
        else:
            return mse_loss


class GeoCLIPLoss(nn.Module):
    """
    GeoCLIPç»¼åˆæŸå¤±å‡½æ•°
    ç»“åˆå¯¹æ¯”å­¦ä¹ ã€å‡ ä½•ä¸€è‡´æ€§å’Œå¼‚å¸¸æ£€æµ‹æŸå¤±
    """

    def __init__(self,
                 contrastive_weight: float = 1.0,
                 geometry_weight: float = 0.5,
                 anomaly_weight: float = 2.0,
                 use_contrastive: bool = True,
                 use_geometry: bool = True,
                 contrastive_config: Dict = None,
                 geometry_config: Dict = None,
                 anomaly_config: Dict = None):
        super(GeoCLIPLoss, self).__init__()

        self.contrastive_weight = contrastive_weight
        self.geometry_weight = geometry_weight
        self.anomaly_weight = anomaly_weight
        self.use_contrastive = use_contrastive
        self.use_geometry = use_geometry

        # åˆå§‹åŒ–å„ä¸ªæŸå¤±å‡½æ•°
        if use_contrastive:
            contrastive_config = contrastive_config or {}
            self.contrastive_loss = ContrastiveLoss(**contrastive_config)

        if use_geometry:
            geometry_config = geometry_config or {}
            self.geometry_loss = GeometryConsistencyLoss(**geometry_config)

        anomaly_config = anomaly_config or {}
        self.anomaly_loss = AnomalyDetectionLoss(**anomaly_config)

    def forward(self,
                model_outputs: Dict[str, torch.Tensor],
                targets: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """
        è®¡ç®—GeoCLIPç»¼åˆæŸå¤±

        Args:
            model_outputs: æ¨¡å‹è¾“å‡ºå­—å…¸ï¼ŒåŒ…å«ï¼š
                - 'anomaly_predictions': å¼‚å¸¸é¢„æµ‹
                - 'clip_features': CLIPç‰¹å¾
                - 'geometry_features': å‡ ä½•ç‰¹å¾
                - 'fused_features': èåˆç‰¹å¾
                - 'depth_maps': æ·±åº¦å›¾
            targets: ç›®æ ‡å­—å…¸ï¼ŒåŒ…å«ï¼š
                - 'anomaly_labels': å¼‚å¸¸æ ‡ç­¾
                - 'class_labels': ç±»åˆ«æ ‡ç­¾ (å¯é€‰)

        Returns:
            loss_dict: æŸå¤±å­—å…¸
        """
        loss_dict = {}
        total_loss = 0

        # 1. å¼‚å¸¸æ£€æµ‹æŸå¤±
        anomaly_loss = self.anomaly_loss(
            model_outputs['anomaly_predictions'],
            targets['anomaly_labels']
        )
        loss_dict['anomaly_loss'] = anomaly_loss
        total_loss += self.anomaly_weight * anomaly_loss

        # 2. å¯¹æ¯”å­¦ä¹ æŸå¤±
        if (self.use_contrastive and
                'clip_features' in model_outputs and
                'geometry_features' in model_outputs):

            clip_feat = model_outputs['clip_features']
            geometry_feat = model_outputs['geometry_features']

            # ç¡®ä¿ç‰¹å¾ç»´åº¦åŒ¹é…
            if clip_feat.shape[1] != geometry_feat.shape[1]:
                # æŠ•å½±åˆ°ç›¸åŒç»´åº¦
                min_dim = min(clip_feat.shape[1], geometry_feat.shape[1])
                if clip_feat.shape[1] > min_dim:
                    clip_feat = clip_feat[:, :min_dim]
                if geometry_feat.shape[1] > min_dim:
                    geometry_feat = geometry_feat[:, :min_dim]

            contrastive_loss = self.contrastive_loss(
                clip_feat,
                geometry_feat,
                targets.get('class_labels', None)
            )
            loss_dict['contrastive_loss'] = contrastive_loss
            total_loss += self.contrastive_weight * contrastive_loss

        # 3. å‡ ä½•ä¸€è‡´æ€§æŸå¤±
        if (self.use_geometry and
                'depth_maps' in model_outputs and
                'clip_features' in model_outputs and
                'geometry_features' in model_outputs):

            clip_feat = model_outputs['clip_features']
            geometry_feat = model_outputs['geometry_features']

            # ç¡®ä¿ç‰¹å¾ç»´åº¦åŒ¹é…
            if clip_feat.shape[1] != geometry_feat.shape[1]:
                min_dim = min(clip_feat.shape[1], geometry_feat.shape[1])
                if clip_feat.shape[1] > min_dim:
                    clip_feat = clip_feat[:, :min_dim]
                if geometry_feat.shape[1] > min_dim:
                    geometry_feat = geometry_feat[:, :min_dim]

            try:
                geometry_loss = self.geometry_loss(
                    clip_feat,
                    geometry_feat,
                    model_outputs['depth_maps']
                )
                loss_dict['geometry_loss'] = geometry_loss
                total_loss += self.geometry_weight * geometry_loss
            except Exception as e:
                print(f"å‡ ä½•ä¸€è‡´æ€§æŸå¤±è®¡ç®—å¤±è´¥ï¼Œè·³è¿‡: {e}")
                loss_dict['geometry_loss'] = torch.tensor(0.0, device=clip_feat.device)

        loss_dict['total_loss'] = total_loss

        return loss_dict


class SelfSupervisedLoss(nn.Module):
    """
    è‡ªç›‘ç£å­¦ä¹ æŸå¤±
    ç”¨äºæ— æ ‡ç­¾æ•°æ®çš„é¢„è®­ç»ƒ
    """

    def __init__(self,
                 reconstruction_weight: float = 1.0,
                 depth_consistency_weight: float = 0.5,
                 feature_consistency_weight: float = 0.3):
        super(SelfSupervisedLoss, self).__init__()

        self.reconstruction_weight = reconstruction_weight
        self.depth_consistency_weight = depth_consistency_weight
        self.feature_consistency_weight = feature_consistency_weight

        # é‡å»ºæŸå¤±
        self.reconstruction_loss = nn.MSELoss()

        # ç‰¹å¾ä¸€è‡´æ€§æŸå¤±
        self.feature_consistency_loss = nn.CosineSimilarity(dim=1)

    def forward(self,
                original_images: torch.Tensor,
                reconstructed_images: torch.Tensor,
                original_depth: torch.Tensor,
                estimated_depth: torch.Tensor,
                features_aug1: torch.Tensor,
                features_aug2: torch.Tensor) -> Dict[str, torch.Tensor]:
        """
        è®¡ç®—è‡ªç›‘ç£æŸå¤±

        Args:
            original_images: åŸå§‹å›¾åƒ
            reconstructed_images: é‡å»ºå›¾åƒ
            original_depth: åŸå§‹æ·±åº¦
            estimated_depth: ä¼°è®¡æ·±åº¦
            features_aug1: å¢å¼ºç‰ˆæœ¬1çš„ç‰¹å¾
            features_aug2: å¢å¼ºç‰ˆæœ¬2çš„ç‰¹å¾

        Returns:
            loss_dict: æŸå¤±å­—å…¸
        """
        loss_dict = {}

        # 1. å›¾åƒé‡å»ºæŸå¤±
        recon_loss = self.reconstruction_loss(reconstructed_images, original_images)
        loss_dict['reconstruction_loss'] = recon_loss

        # 2. æ·±åº¦ä¸€è‡´æ€§æŸå¤±
        depth_loss = self.reconstruction_loss(estimated_depth, original_depth)
        loss_dict['depth_consistency_loss'] = depth_loss

        # 3. ç‰¹å¾ä¸€è‡´æ€§æŸå¤±ï¼ˆæ•°æ®å¢å¼ºä¸å˜æ€§ï¼‰
        feature_sim = self.feature_consistency_loss(features_aug1, features_aug2)
        feature_loss = 1 - feature_sim.mean()  # æœ€å¤§åŒ–ç›¸ä¼¼æ€§
        loss_dict['feature_consistency_loss'] = feature_loss

        # æ€»æŸå¤±
        total_loss = (self.reconstruction_weight * recon_loss +
                      self.depth_consistency_weight * depth_loss +
                      self.feature_consistency_weight * feature_loss)

        loss_dict['total_loss'] = total_loss

        return loss_dict


class AdversarialLoss(nn.Module):
    """
    å¯¹æŠ—æŸå¤±å‡½æ•°
    ç”¨äºç”Ÿæˆå¯¹æŠ—è®­ç»ƒæˆ–åŸŸé€‚åº”
    """

    def __init__(self,
                 loss_type: str = "wgan",
                 lambda_gp: float = 10.0):
        super(AdversarialLoss, self).__init__()

        self.loss_type = loss_type
        self.lambda_gp = lambda_gp

    def forward(self,
                discriminator_real: torch.Tensor,
                discriminator_fake: torch.Tensor,
                real_data: Optional[torch.Tensor] = None,
                fake_data: Optional[torch.Tensor] = None,
                discriminator: Optional[nn.Module] = None) -> Dict[str, torch.Tensor]:
        """
        è®¡ç®—å¯¹æŠ—æŸå¤±

        Args:
            discriminator_real: åˆ¤åˆ«å™¨å¯¹çœŸå®æ•°æ®çš„è¾“å‡º
            discriminator_fake: åˆ¤åˆ«å™¨å¯¹ç”Ÿæˆæ•°æ®çš„è¾“å‡º
            real_data: çœŸå®æ•°æ® (ç”¨äºæ¢¯åº¦æƒ©ç½š)
            fake_data: ç”Ÿæˆæ•°æ® (ç”¨äºæ¢¯åº¦æƒ©ç½š)
            discriminator: åˆ¤åˆ«å™¨æ¨¡å‹ (ç”¨äºæ¢¯åº¦æƒ©ç½š)

        Returns:
            loss_dict: æŸå¤±å­—å…¸
        """
        loss_dict = {}

        if self.loss_type == "wgan":
            # Wasserstein GANæŸå¤±
            d_loss = discriminator_real.mean() - discriminator_fake.mean()
            g_loss = -discriminator_fake.mean()

            # æ¢¯åº¦æƒ©ç½š
            if real_data is not None and fake_data is not None and discriminator is not None:
                gp_loss = self._gradient_penalty(discriminator, real_data, fake_data)
                d_loss += self.lambda_gp * gp_loss
                loss_dict['gradient_penalty'] = gp_loss

        elif self.loss_type == "lsgan":
            # Least Squares GANæŸå¤±
            d_loss = 0.5 * (torch.mean((discriminator_real - 1) ** 2) +
                            torch.mean(discriminator_fake ** 2))
            g_loss = 0.5 * torch.mean((discriminator_fake - 1) ** 2)

        else:
            # æ ‡å‡†GANæŸå¤±
            d_loss = -(torch.log(discriminator_real + 1e-8).mean() +
                       torch.log(1 - discriminator_fake + 1e-8).mean())
            g_loss = -torch.log(discriminator_fake + 1e-8).mean()

        loss_dict['discriminator_loss'] = d_loss
        loss_dict['generator_loss'] = g_loss

        return loss_dict

    def _gradient_penalty(self, discriminator, real_data, fake_data):
        """è®¡ç®—æ¢¯åº¦æƒ©ç½š"""
        batch_size = real_data.size(0)
        device = real_data.device

        # éšæœºæ’å€¼
        alpha = torch.rand(batch_size, 1, 1, 1, device=device)
        interpolated = alpha * real_data + (1 - alpha) * fake_data
        interpolated.requires_grad_(True)

        # è®¡ç®—åˆ¤åˆ«å™¨è¾“å‡º
        d_interpolated = discriminator(interpolated)

        # è®¡ç®—æ¢¯åº¦
        gradients = torch.autograd.grad(
            outputs=d_interpolated,
            inputs=interpolated,
            grad_outputs=torch.ones_like(d_interpolated),
            create_graph=True,
            retain_graph=True,
            only_inputs=True
        )[0]

        # æ¢¯åº¦æƒ©ç½š
        gradients = gradients.view(batch_size, -1)
        gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()

        return gradient_penalty


def create_loss_function(config: Dict[str, Any]) -> nn.Module:
    """
    æ ¹æ®é…ç½®åˆ›å»ºæŸå¤±å‡½æ•°

    Args:
        config: æŸå¤±å‡½æ•°é…ç½®

    Returns:
        loss_function: æŸå¤±å‡½æ•°å®ä¾‹
    """
    loss_type = config.get('type', 'geoclip')

    if loss_type == 'geoclip':
        return GeoCLIPLoss(
            contrastive_weight=config.get('contrastive_weight', 1.0),
            geometry_weight=config.get('geometry_weight', 0.5),
            anomaly_weight=config.get('anomaly_weight', 2.0),
            use_contrastive=config.get('use_contrastive', True),
            use_geometry=config.get('use_geometry', True),
            contrastive_config=config.get('contrastive_config', {}),
            geometry_config=config.get('geometry_config', {}),
            anomaly_config=config.get('anomaly_config', {})
        )
    elif loss_type == 'self_supervised':
        return SelfSupervisedLoss(
            reconstruction_weight=config.get('reconstruction_weight', 1.0),
            depth_consistency_weight=config.get('depth_consistency_weight', 0.5),
            feature_consistency_weight=config.get('feature_consistency_weight', 0.3)
        )
    elif loss_type == 'adversarial':
        return AdversarialLoss(
            loss_type=config.get('adversarial_type', 'wgan'),
            lambda_gp=config.get('lambda_gp', 10.0)
        )
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æŸå¤±ç±»å‹: {loss_type}")


# æµ‹è¯•å‡½æ•°
def test_loss_functions():
    """æµ‹è¯•æŸå¤±å‡½æ•°"""
    print("=== æµ‹è¯•GeoCLIPæŸå¤±å‡½æ•° ===")

    try:
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        batch_size = 4
        feature_dim = 512

        # 1. æµ‹è¯•GeoCLIPç»¼åˆæŸå¤±
        print("\n1. æµ‹è¯•GeoCLIPç»¼åˆæŸå¤±")

        geoclip_loss = GeoCLIPLoss(
            contrastive_weight=1.0,
            geometry_weight=0.5,
            anomaly_weight=2.0
        )

        # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
        model_outputs = {
            'anomaly_predictions': torch.randn(batch_size, 1, device=device).sigmoid(),
            'clip_features': torch.randn(batch_size, feature_dim, device=device),
            'geometry_features': torch.randn(batch_size, feature_dim, device=device),
            'depth_maps': torch.rand(batch_size, 1, 64, 64, device=device) * 5
        }

        targets = {
            'anomaly_labels': torch.randint(0, 2, (batch_size,), device=device),
            'class_labels': torch.randint(0, 5, (batch_size,), device=device)
        }

        loss_dict = geoclip_loss(model_outputs, targets)

        print(f"âœ… GeoCLIPæŸå¤±è®¡ç®—æˆåŠŸ:")
        for key, value in loss_dict.items():
            print(f"   {key}: {value.item():.4f}")

        # 2. æµ‹è¯•å¯¹æ¯”å­¦ä¹ æŸå¤±
        print("\n2. æµ‹è¯•å¯¹æ¯”å­¦ä¹ æŸå¤±")
        contrastive_loss = ContrastiveLoss(temperature=0.07)

        features_2d = torch.randn(batch_size, feature_dim, device=device)
        features_3d = torch.randn(batch_size, feature_dim, device=device)

        contrast_loss = contrastive_loss(features_2d, features_3d)
        print(f"âœ… å¯¹æ¯”å­¦ä¹ æŸå¤±: {contrast_loss.item():.4f}")

        # 3. æµ‹è¯•å‡ ä½•ä¸€è‡´æ€§æŸå¤±
        print("\n3. æµ‹è¯•å‡ ä½•ä¸€è‡´æ€§æŸå¤±")
        geometry_loss = GeometryConsistencyLoss(consistency_type="distance")

        depth_maps = torch.rand(batch_size, 1, 32, 32, device=device) * 10
        geom_loss = geometry_loss(features_2d, features_3d, depth_maps)
        print(f"âœ… å‡ ä½•ä¸€è‡´æ€§æŸå¤±: {geom_loss.item():.4f}")

        # 4. æµ‹è¯•å¼‚å¸¸æ£€æµ‹æŸå¤±
        print("\n4. æµ‹è¯•å¼‚å¸¸æ£€æµ‹æŸå¤±")
        anomaly_loss = AnomalyDetectionLoss(loss_type="focal")

        predictions = torch.randn(batch_size, device=device).sigmoid()
        labels = torch.randint(0, 2, (batch_size,), device=device)

        anom_loss = anomaly_loss(predictions, labels)
        print(f"âœ… å¼‚å¸¸æ£€æµ‹æŸå¤±: {anom_loss.item():.4f}")

        # 5. æµ‹è¯•è‡ªç›‘ç£æŸå¤±
        print("\n5. æµ‹è¯•è‡ªç›‘ç£æŸå¤±")
        ssl_loss = SelfSupervisedLoss()

        orig_imgs = torch.randn(batch_size, 3, 64, 64, device=device)
        recon_imgs = torch.randn(batch_size, 3, 64, 64, device=device)
        orig_depth = torch.rand(batch_size, 1, 64, 64, device=device)
        est_depth = torch.rand(batch_size, 1, 64, 64, device=device)
        feat_aug1 = torch.randn(batch_size, feature_dim, device=device)
        feat_aug2 = torch.randn(batch_size, feature_dim, device=device)

        ssl_loss_dict = ssl_loss(orig_imgs, recon_imgs, orig_depth,
                                 est_depth, feat_aug1, feat_aug2)

        print(f"âœ… è‡ªç›‘ç£æŸå¤±:")
        for key, value in ssl_loss_dict.items():
            print(f"   {key}: {value.item():.4f}")

        print("\nğŸ‰ æŸå¤±å‡½æ•°æµ‹è¯•å®Œæˆ!")

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    test_loss_functions()