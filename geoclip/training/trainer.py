"""
GeoCLIP - è®­ç»ƒå™¨æ¨¡å—
ç«¯åˆ°ç«¯çš„è®­ç»ƒå’ŒéªŒè¯æµç¨‹
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
import os
import time
import json
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any, Union
from tqdm import tqdm
import numpy as np
from collections import defaultdict
import wandb

# å¯¼å…¥GeoCLIPç»„ä»¶
from geoclip.models.geoclip_main import GeoCLIP, create_geoclip_model
from geoclip.training.losses import GeoCLIPLoss, create_loss_function
from geoclip.utils.metrics import AnomalyMetrics
from geoclip.utils.visualization import Visualizer


class EarlyStopping:
    """æ—©åœæœºåˆ¶"""

    def __init__(self,
                 patience: int = 10,
                 min_delta: float = 0.001,
                 mode: str = 'min'):
        self.patience = patience
        self.min_delta = min_delta
        self.mode = mode
        self.counter = 0
        self.best_score = None
        self.early_stop = False

    def __call__(self, score: float) -> bool:
        if self.best_score is None:
            self.best_score = score
        elif self.mode == 'min':
            if score < self.best_score - self.min_delta:
                self.best_score = score
                self.counter = 0
            else:
                self.counter += 1
        else:  # mode == 'max'
            if score > self.best_score + self.min_delta:
                self.best_score = score
                self.counter = 0
            else:
                self.counter += 1

        if self.counter >= self.patience:
            self.early_stop = True

        return self.early_stop


class GeoCLIPTrainer:
    """
    GeoCLIPè®­ç»ƒå™¨
    ç®¡ç†æ•´ä¸ªè®­ç»ƒå’ŒéªŒè¯æµç¨‹
    """

    def __init__(self,
                 model: GeoCLIP,
                 train_loader: DataLoader,
                 val_loader: DataLoader,
                 test_loader: Optional[DataLoader] = None,
                 loss_function: Optional[nn.Module] = None,
                 optimizer: Optional[optim.Optimizer] = None,
                 scheduler: Optional[optim.lr_scheduler._LRScheduler] = None,
                 device: str = 'cuda',
                 experiment_dir: str = './experiments',
                 experiment_name: str = 'geoclip_exp',
                 use_wandb: bool = False,
                 wandb_config: Dict = None,
                 log_interval: int = 10,
                 save_interval: int = 5,
                 early_stopping_patience: int = 15):

        self.model = model.to(device)
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.test_loader = test_loader
        self.device = device
        self.log_interval = log_interval
        self.save_interval = save_interval

        # è®¾ç½®å®éªŒç›®å½•
        self.experiment_dir = Path(experiment_dir) / experiment_name
        self.experiment_dir.mkdir(parents=True, exist_ok=True)

        # æŸå¤±å‡½æ•°
        if loss_function is None:
            self.loss_function = GeoCLIPLoss()
        else:
            self.loss_function = loss_function

        # ä¼˜åŒ–å™¨
        if optimizer is None:
            self.optimizer = optim.AdamW(
                self.model.parameters(),
                lr=1e-4,
                weight_decay=1e-5
            )
        else:
            self.optimizer = optimizer

        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = scheduler

        # æ—©åœæœºåˆ¶
        self.early_stopping = EarlyStopping(
            patience=early_stopping_patience,
            mode='max'  # ç›‘æ§éªŒè¯AUCï¼Œè¶Šå¤§è¶Šå¥½
        )

        # æŒ‡æ ‡è®¡ç®—
        self.metrics = AnomalyMetrics()

        # å¯è§†åŒ–å·¥å…·
        self.visualizer = Visualizer(save_dir=self.experiment_dir / 'visualizations')

        # TensorBoard
        self.writer = SummaryWriter(log_dir=self.experiment_dir / 'tensorboard')

        # Wandb
        self.use_wandb = use_wandb
        if use_wandb:
            wandb_config = wandb_config or {}
            wandb.init(
                project=wandb_config.get('project', 'GeoCLIP'),
                name=experiment_name,
                config=wandb_config,
                dir=str(self.experiment_dir)
            )
            wandb.watch(self.model, log_freq=100)

        # è®­ç»ƒçŠ¶æ€
        self.current_epoch = 0
        self.global_step = 0
        self.best_val_auc = 0.0
        self.train_history = defaultdict(list)
        self.val_history = defaultdict(list)

        print(f"GeoCLIPè®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"å®éªŒç›®å½•: {self.experiment_dir}")
        print(f"è®¾å¤‡: {device}")
        print(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in self.model.parameters()):,}")

    def train_epoch(self) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        epoch_losses = defaultdict(list)
        epoch_metrics = defaultdict(list)

        progress_bar = tqdm(self.train_loader, desc=f'è®­ç»ƒ Epoch {self.current_epoch}')

        for batch_idx, batch in enumerate(progress_bar):
            # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
            batch = self._move_to_device(batch)

            # å‰å‘ä¼ æ’­
            self.optimizer.zero_grad()

            try:
                model_outputs = self.model(batch)

                # å‡†å¤‡ç›®æ ‡
                targets = {
                    'anomaly_labels': batch['anomaly'],
                    'class_labels': batch.get('cls_name', None)
                }

                # è®¡ç®—æŸå¤±
                loss_dict = self.loss_function(model_outputs, targets)
                total_loss = loss_dict['total_loss']

                # åå‘ä¼ æ’­
                total_loss.backward()

                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)

                self.optimizer.step()

                # è®°å½•æŸå¤±
                for key, value in loss_dict.items():
                    epoch_losses[key].append(value.item())

                # è®¡ç®—æŒ‡æ ‡
                if 'anomaly_predictions' in model_outputs:
                    predictions = model_outputs['anomaly_predictions']
                    if predictions.dim() > 1:
                        predictions = predictions[:, 1]  # å–æ­£ç±»æ¦‚ç‡

                    labels = targets['anomaly_labels']

                    # è½¬æ¢ä¸ºnumpy
                    pred_np = predictions.detach().cpu().numpy()
                    label_np = labels.detach().cpu().numpy()

                    # è®¡ç®—æ‰¹æ¬¡æŒ‡æ ‡
                    batch_metrics = self.metrics.compute_metrics(pred_np, label_np)
                    for key, value in batch_metrics.items():
                        if not np.isnan(value):
                            epoch_metrics[key].append(value)

                # æ›´æ–°è¿›åº¦æ¡
                progress_bar.set_postfix({
                    'loss': f"{total_loss.item():.4f}",
                    'auc': f"{np.mean(epoch_metrics['auc']) if epoch_metrics['auc'] else 0:.3f}"
                })

                # è®°å½•åˆ°TensorBoard
                if self.global_step % self.log_interval == 0:
                    self._log_to_tensorboard('train', loss_dict, self.global_step)

                    if self.use_wandb:
                        wandb_log = {f'train/{k}': v.item() if torch.is_tensor(v) else v
                                     for k, v in loss_dict.items()}
                        wandb.log(wandb_log, step=self.global_step)

                self.global_step += 1

            except Exception as e:
                print(f"è®­ç»ƒæ‰¹æ¬¡ {batch_idx} å‡ºé”™: {e}")
                continue

        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        avg_losses = {key: np.mean(values) for key, values in epoch_losses.items()}
        avg_metrics = {key: np.mean(values) for key, values in epoch_metrics.items()}

        return {**avg_losses, **avg_metrics}

    def validate_epoch(self) -> Dict[str, float]:
        """éªŒè¯ä¸€ä¸ªepoch"""
        self.model.eval()
        epoch_losses = defaultdict(list)
        all_predictions = []
        all_labels = []

        progress_bar = tqdm(self.val_loader, desc=f'éªŒè¯ Epoch {self.current_epoch}')

        with torch.no_grad():
            for batch_idx, batch in enumerate(progress_bar):
                # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
                batch = self._move_to_device(batch)

                try:
                    # å‰å‘ä¼ æ’­
                    model_outputs = self.model(batch)

                    # å‡†å¤‡ç›®æ ‡
                    targets = {
                        'anomaly_labels': batch['anomaly'],
                        'class_labels': batch.get('cls_name', None)
                    }

                    # è®¡ç®—æŸå¤±
                    loss_dict = self.loss_function(model_outputs, targets)

                    # è®°å½•æŸå¤±
                    for key, value in loss_dict.items():
                        epoch_losses[key].append(value.item())

                    # æ”¶é›†é¢„æµ‹å’Œæ ‡ç­¾
                    if 'anomaly_predictions' in model_outputs:
                        predictions = model_outputs['anomaly_predictions']
                        if predictions.dim() > 1:
                            predictions = predictions[:, 1]  # å–æ­£ç±»æ¦‚ç‡

                        labels = targets['anomaly_labels']

                        all_predictions.extend(predictions.cpu().numpy())
                        all_labels.extend(labels.cpu().numpy())

                    # æ›´æ–°è¿›åº¦æ¡
                    progress_bar.set_postfix({
                        'val_loss': f"{loss_dict['total_loss'].item():.4f}"
                    })

                except Exception as e:
                    print(f"éªŒè¯æ‰¹æ¬¡ {batch_idx} å‡ºé”™: {e}")
                    continue

        # è®¡ç®—å¹³å‡æŸå¤±
        avg_losses = {key: np.mean(values) for key, values in epoch_losses.items()}

        # è®¡ç®—æ•´ä½“æŒ‡æ ‡
        if all_predictions and all_labels:
            val_metrics = self.metrics.compute_metrics(
                np.array(all_predictions),
                np.array(all_labels)
            )
        else:
            val_metrics = {}

        return {**avg_losses, **val_metrics}

    def test(self) -> Dict[str, float]:
        """æµ‹è¯•æ¨¡å‹"""
        if self.test_loader is None:
            print("è­¦å‘Š: æ²¡æœ‰æä¾›æµ‹è¯•æ•°æ®é›†")
            return {}

        print("å¼€å§‹æµ‹è¯•...")
        self.model.eval()

        all_predictions = []
        all_labels = []
        all_features = []

        with torch.no_grad():
            for batch in tqdm(self.test_loader, desc='æµ‹è¯•'):
                batch = self._move_to_device(batch)

                try:
                    model_outputs = self.model(batch)

                    if 'anomaly_predictions' in model_outputs:
                        predictions = model_outputs['anomaly_predictions']
                        if predictions.dim() > 1:
                            predictions = predictions[:, 1]

                        all_predictions.extend(predictions.cpu().numpy())
                        all_labels.extend(batch['anomaly'].cpu().numpy())

                        # æ”¶é›†ç‰¹å¾ç”¨äºå¯è§†åŒ–
                        if 'fused_features' in model_outputs:
                            all_features.extend(model_outputs['fused_features'].cpu().numpy())

                except Exception as e:
                    print(f"æµ‹è¯•æ‰¹æ¬¡å‡ºé”™: {e}")
                    continue

        # è®¡ç®—æµ‹è¯•æŒ‡æ ‡
        if all_predictions and all_labels:
            test_metrics = self.metrics.compute_metrics(
                np.array(all_predictions),
                np.array(all_labels)
            )

            # ä¿å­˜æµ‹è¯•ç»“æœ
            test_results = {
                'predictions': all_predictions,
                'labels': all_labels,
                'metrics': test_metrics
            }

            results_path = self.experiment_dir / 'test_results.json'
            with open(results_path, 'w') as f:
                # è½¬æ¢numpyæ•°ç»„ä¸ºåˆ—è¡¨ä»¥ä¾¿JSONåºåˆ—åŒ–
                serializable_results = {
                    'predictions': [float(x) for x in all_predictions],
                    'labels': [int(x) for x in all_labels],
                    'metrics': {k: float(v) for k, v in test_metrics.items()}
                }
                json.dump(serializable_results, f, indent=2)

            print(f"æµ‹è¯•å®Œæˆï¼Œç»“æœä¿å­˜åˆ°: {results_path}")
            print("æµ‹è¯•æŒ‡æ ‡:")
            for key, value in test_metrics.items():
                print(f"  {key}: {value:.4f}")

            # å¯è§†åŒ–æµ‹è¯•ç»“æœ
            if all_features:
                self.visualizer.plot_feature_distribution(
                    np.array(all_features),
                    np.array(all_labels),
                    save_name='test_feature_distribution.png'
                )

                self.visualizer.plot_roc_curve(
                    np.array(all_labels),
                    np.array(all_predictions),
                    save_name='test_roc_curve.png'
                )
        else:
            test_metrics = {}
            print("æµ‹è¯•å¤±è´¥: æ²¡æœ‰æ”¶é›†åˆ°æœ‰æ•ˆçš„é¢„æµ‹ç»“æœ")

        return test_metrics

    def train(self, num_epochs: int, resume_from: Optional[str] = None):
        """å®Œæ•´çš„è®­ç»ƒæµç¨‹"""
        # æ¢å¤è®­ç»ƒ
        if resume_from:
            self.load_checkpoint(resume_from)
            print(f"ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ: {resume_from}")

        print(f"å¼€å§‹è®­ç»ƒï¼Œå…± {num_epochs} ä¸ªepoch")
        print(f"è®­ç»ƒæ ·æœ¬: {len(self.train_loader.dataset)}")
        print(f"éªŒè¯æ ·æœ¬: {len(self.val_loader.dataset)}")

        start_time = time.time()

        for epoch in range(self.current_epoch, num_epochs):
            self.current_epoch = epoch

            # è®­ç»ƒ
            train_results = self.train_epoch()
            self.train_history['epoch'].append(epoch)
            for key, value in train_results.items():
                self.train_history[key].append(value)

            # éªŒè¯
            val_results = self.validate_epoch()
            self.val_history['epoch'].append(epoch)
            for key, value in val_results.items():
                self.val_history[key].append(value)

            # å­¦ä¹ ç‡è°ƒåº¦
            if self.scheduler:
                if isinstance(self.scheduler, optim.lr_scheduler.ReduceLROnPlateau):
                    self.scheduler.step(val_results.get('auc', 0))
                else:
                    self.scheduler.step()

            # è®°å½•åˆ°TensorBoardå’Œwandb
            current_lr = self.optimizer.param_groups[0]['lr']
            self._log_to_tensorboard('val', val_results, epoch)
            self.writer.add_scalar('learning_rate', current_lr, epoch)

            if self.use_wandb:
                wandb_log = {
                    'epoch': epoch,
                    'learning_rate': current_lr,
                    **{f'train/{k}': v for k, v in train_results.items()},
                    **{f'val/{k}': v for k, v in val_results.items()}
                }
                wandb.log(wandb_log, step=epoch)

            # æ‰“å°è¿›åº¦
            print(f"\nEpoch {epoch + 1}/{num_epochs}")
            print(f"è®­ç»ƒ - Loss: {train_results.get('total_loss', 0):.4f}, "
                  f"AUC: {train_results.get('auc', 0):.4f}")
            print(f"éªŒè¯ - Loss: {val_results.get('total_loss', 0):.4f}, "
                  f"AUC: {val_results.get('auc', 0):.4f}")

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            val_auc = val_results.get('auc', 0)
            if val_auc > self.best_val_auc:
                self.best_val_auc = val_auc
                self.save_checkpoint('best_model.pth', is_best=True)
                print(f"âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ (AUC: {val_auc:.4f})")

            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
            if (epoch + 1) % self.save_interval == 0:
                self.save_checkpoint(f'checkpoint_epoch_{epoch + 1}.pth')

            # æ—©åœæ£€æŸ¥
            if self.early_stopping(val_auc):
                print(f"æ—©åœè§¦å‘ï¼Œåœ¨ç¬¬ {epoch + 1} ä¸ªepochåœæ­¢è®­ç»ƒ")
                break

        # è®­ç»ƒå®Œæˆ
        total_time = time.time() - start_time
        print(f"\nè®­ç»ƒå®Œæˆ! æ€»ç”¨æ—¶: {total_time / 3600:.2f} å°æ—¶")
        print(f"æœ€ä½³éªŒè¯AUC: {self.best_val_auc:.4f}")

        # ä¿å­˜è®­ç»ƒå†å²
        self._save_training_history()

        # ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š
        self._generate_training_report()

        # åœ¨æœ€ä½³æ¨¡å‹ä¸Šè¿›è¡Œæµ‹è¯•
        print("\nåœ¨æœ€ä½³æ¨¡å‹ä¸Šè¿›è¡Œæµ‹è¯•...")
        self.load_checkpoint(self.experiment_dir / 'best_model.pth')
        test_results = self.test()

        return test_results

    def save_checkpoint(self, filename: str, is_best: bool = False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': self.current_epoch,
            'global_step': self.global_step,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'best_val_auc': self.best_val_auc,
            'train_history': dict(self.train_history),
            'val_history': dict(self.val_history)
        }

        if self.scheduler:
            checkpoint['scheduler_state_dict'] = self.scheduler.state_dict()

        save_path = self.experiment_dir / filename
        torch.save(checkpoint, save_path)

        if is_best:
            # åŒæ—¶ä¿å­˜ä¸ºlatest.pth
            torch.save(checkpoint, self.experiment_dir / 'latest.pth')

    def load_checkpoint(self, checkpoint_path: Union[str, Path]):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        checkpoint_path = Path(checkpoint_path)
        if not checkpoint_path.exists():
            raise FileNotFoundError(f"æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")

        checkpoint = torch.load(checkpoint_path, map_location=self.device)

        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.current_epoch = checkpoint['epoch']
        self.global_step = checkpoint['global_step']
        self.best_val_auc = checkpoint['best_val_auc']
        self.train_history = defaultdict(list, checkpoint['train_history'])
        self.val_history = defaultdict(list, checkpoint['val_history'])

        if self.scheduler and 'scheduler_state_dict' in checkpoint:
            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])

        print(f"æˆåŠŸåŠ è½½æ£€æŸ¥ç‚¹: {checkpoint_path}")

    def _move_to_device(self, batch: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """å°†æ‰¹æ¬¡æ•°æ®ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡"""
        moved_batch = {}
        for key, value in batch.items():
            if torch.is_tensor(value):
                moved_batch[key] = value.to(self.device)
            else:
                moved_batch[key] = value
        return moved_batch

    def _log_to_tensorboard(self, phase: str, metrics: Dict[str, float], step: int):
        """è®°å½•åˆ°TensorBoard"""
        for key, value in metrics.items():
            if torch.is_tensor(value):
                value = value.item()
            self.writer.add_scalar(f'{phase}/{key}', value, step)

    def _save_training_history(self):
        """ä¿å­˜è®­ç»ƒå†å²"""
        history = {
            'train': dict(self.train_history),
            'val': dict(self.val_history)
        }

        history_path = self.experiment_dir / 'training_history.json'
        with open(history_path, 'w') as f:
            json.dump(history, f, indent=2)

        # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
        self.visualizer.plot_training_curves(
            dict(self.train_history),
            dict(self.val_history),
            save_name='training_curves.png'
        )

    def _generate_training_report(self):
        """ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š"""
        report = {
            'experiment_info': {
                'name': self.experiment_dir.name,
                'total_epochs': self.current_epoch + 1,
                'best_val_auc': self.best_val_auc,
                'final_lr': self.optimizer.param_groups[0]['lr']
            },
            'model_info': self.model.get_model_info(),
            'training_config': {
                'optimizer': type(self.optimizer).__name__,
                'scheduler': type(self.scheduler).__name__ if self.scheduler else None,
                'batch_size': self.train_loader.batch_size,
                'train_samples': len(self.train_loader.dataset),
                'val_samples': len(self.val_loader.dataset)
            }
        }

        report_path = self.experiment_dir / 'training_report.json'
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2)

        print(f"è®­ç»ƒæŠ¥å‘Šå·²ä¿å­˜: {report_path}")


def create_trainer(config: Dict[str, Any],
                   train_loader: DataLoader,
                   val_loader: DataLoader,
                   test_loader: Optional[DataLoader] = None) -> GeoCLIPTrainer:
    """
    æ ¹æ®é…ç½®åˆ›å»ºè®­ç»ƒå™¨

    Args:
        config: è®­ç»ƒé…ç½®
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
        test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨ (å¯é€‰)

    Returns:
        trainer: GeoCLIPè®­ç»ƒå™¨å®ä¾‹
    """
    # åˆ›å»ºæ¨¡å‹
    model_config = config.get('model', {})
    model = create_geoclip_model(model_config)

    # åˆ›å»ºæŸå¤±å‡½æ•°
    loss_config = config.get('loss', {})
    loss_function = create_loss_function(loss_config)

    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer_config = config.get('optimizer', {})
    optimizer_type = optimizer_config.get('type', 'adamw')
    optimizer_params = optimizer_config.get('params', {})

    if optimizer_type.lower() == 'adamw':
        optimizer = optim.AdamW(model.parameters(), **optimizer_params)
    elif optimizer_type.lower() == 'adam':
        optimizer = optim.Adam(model.parameters(), **optimizer_params)
    elif optimizer_type.lower() == 'sgd':
        optimizer = optim.SGD(model.parameters(), **optimizer_params)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„ä¼˜åŒ–å™¨ç±»å‹: {optimizer_type}")

    # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = None
    if 'scheduler' in config:
        scheduler_config = config['scheduler']
        scheduler_type = scheduler_config.get('type', 'cosine')
        scheduler_params = scheduler_config.get('params', {})

        if scheduler_type == 'cosine':
            scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, **scheduler_params)
        elif scheduler_type == 'step':
            scheduler = optim.lr_scheduler.StepLR(optimizer, **scheduler_params)
        elif scheduler_type == 'plateau':
            scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, **scheduler_params)

    # åˆ›å»ºè®­ç»ƒå™¨
    trainer_config = config.get('trainer', {})
    trainer = GeoCLIPTrainer(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        test_loader=test_loader,
        loss_function=loss_function,
        optimizer=optimizer,
        scheduler=scheduler,
        **trainer_config
    )

    return trainer


# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•
def test_trainer():
    """æµ‹è¯•è®­ç»ƒå™¨"""
    print("=== æµ‹è¯•GeoCLIPè®­ç»ƒå™¨ ===")

    try:
        from torch.utils.data import TensorDataset, DataLoader

        # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®é›†
        def create_mock_dataset(num_samples: int):
            images = torch.randn(num_samples, 3, 224, 224)
            depths = torch.rand(num_samples, 1, 224, 224) * 5
            anomalies = torch.randint(0, 2, (num_samples,))
            img_paths = [f'image_{i}.jpg' for i in range(num_samples)]

            dataset_dict = {
                'img': images,
                'depth': depths,
                'anomaly': anomalies,
                'img_path': img_paths,
                'has_depth': torch.ones(num_samples, dtype=torch.bool)
            }

            return dataset_dict

        # åˆ›å»ºæ•°æ®é›†
        train_data = create_mock_dataset(100)
        val_data = create_mock_dataset(50)
        test_data = create_mock_dataset(30)

        # è½¬æ¢ä¸ºTensorDataset (ç®€åŒ–ç‰ˆæœ¬)
        class MockDataset:
            def __init__(self, data_dict):
                self.data = data_dict
                self.length = len(data_dict['img'])

            def __len__(self):
                return self.length

            def __getitem__(self, idx):
                return {key: value[idx] if torch.is_tensor(value) else value[idx]
                        for key, value in self.data.items()}

        train_dataset = MockDataset(train_data)
        val_dataset = MockDataset(val_data)
        test_dataset = MockDataset(test_data)

        # åˆ›å»ºDataLoader
        train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)
        test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)

        # åˆ›å»ºé…ç½®
        config = {
            'model': {
                'clip_model': 'ViT-B/16',
                'depth_estimator': 'DPT_Large',
                'fusion_type': 'cross_attention',
                'detection_type': 'regression',
                'device': 'cuda' if torch.cuda.is_available() else 'cpu'
            },
            'loss': {
                'type': 'geoclip',
                'contrastive_weight': 1.0,
                'geometry_weight': 0.5,
                'anomaly_weight': 2.0
            },
            'optimizer': {
                'type': 'adamw',
                'params': {
                    'lr': 1e-4,
                    'weight_decay': 1e-5
                }
            },
            'scheduler': {
                'type': 'cosine',
                'params': {
                    'T_max': 10
                }
            },
            'trainer': {
                'experiment_name': 'test_geoclip',
                'experiment_dir': './test_experiments',
                'use_wandb': False,
                'log_interval': 5,
                'save_interval': 2,
                'early_stopping_patience': 5
            }
        }

        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = create_trainer(config, train_loader, val_loader, test_loader)
        print(f"âœ… è®­ç»ƒå™¨åˆ›å»ºæˆåŠŸ")

        # æµ‹è¯•è®­ç»ƒå¾ªç¯ (å°‘é‡epoch)
        print("\nå¼€å§‹æµ‹è¯•è®­ç»ƒ...")
        test_results = trainer.train(num_epochs=3)

        print(f"âœ… è®­ç»ƒæµ‹è¯•å®Œæˆ")
        print(f"æµ‹è¯•ç»“æœ: {test_results}")

        print("\nğŸ‰ è®­ç»ƒå™¨æµ‹è¯•å®Œæˆ!")

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    test_trainer()