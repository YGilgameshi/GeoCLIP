"""
GeoCLIP - å¼‚å¸¸æ£€æµ‹æŒ‡æ ‡
è®¡ç®—å„ç§å¼‚å¸¸æ£€æµ‹æ€§èƒ½æŒ‡æ ‡
"""

import numpy as np
import torch
from typing import Dict, List, Optional, Tuple, Union, Any
from sklearn.metrics import (
    roc_auc_score, average_precision_score, precision_recall_curve,
    roc_curve, confusion_matrix, f1_score, precision_score, recall_score
)
import warnings

warnings.filterwarnings("ignore")


class AnomalyMetrics:
    """
    å¼‚å¸¸æ£€æµ‹æŒ‡æ ‡è®¡ç®—å™¨
    """

    def __init__(self, threshold: float = 0.5):
        self.threshold = threshold
        self.reset()

    def reset(self):
        """é‡ç½®ç´¯ç§¯æŒ‡æ ‡"""
        self.all_predictions = []
        self.all_labels = []
        self.all_scores = []

    def update(self, predictions: Union[np.ndarray, torch.Tensor],
               labels: Union[np.ndarray, torch.Tensor],
               scores: Optional[Union[np.ndarray, torch.Tensor]] = None):
        """
        æ›´æ–°ç´¯ç§¯æŒ‡æ ‡

        Args:
            predictions: é¢„æµ‹ç»“æœ [N]
            labels: çœŸå®æ ‡ç­¾ [N]
            scores: ç½®ä¿¡åº¦åˆ†æ•° [N] (å¯é€‰)
        """
        # è½¬æ¢ä¸ºnumpy
        if torch.is_tensor(predictions):
            predictions = predictions.cpu().numpy()
        if torch.is_tensor(labels):
            labels = labels.cpu().numpy()
        if scores is not None and torch.is_tensor(scores):
            scores = scores.cpu().numpy()

        self.all_predictions.extend(predictions.flatten())
        self.all_labels.extend(labels.flatten())

        if scores is not None:
            self.all_scores.extend(scores.flatten())
        else:
            self.all_scores.extend(predictions.flatten())

    def compute_metrics(self, predictions: Optional[Union[np.ndarray, torch.Tensor]] = None,
                        labels: Optional[Union[np.ndarray, torch.Tensor]] = None) -> Dict[str, float]:
        """
        è®¡ç®—æ‰€æœ‰æŒ‡æ ‡

        Args:
            predictions: é¢„æµ‹ç»“æœ [N] (å¯é€‰ï¼Œå¦‚æœä¸æä¾›åˆ™ä½¿ç”¨ç´¯ç§¯çš„)
            labels: çœŸå®æ ‡ç­¾ [N] (å¯é€‰ï¼Œå¦‚æœä¸æä¾›åˆ™ä½¿ç”¨ç´¯ç§¯çš„)

        Returns:
            metrics: æŒ‡æ ‡å­—å…¸
        """
        # å¦‚æœæä¾›äº†æ–°çš„æ•°æ®ï¼Œåˆ™ä½¿ç”¨æ–°æ•°æ®
        if predictions is not None and labels is not None:
            if torch.is_tensor(predictions):
                predictions = predictions.cpu().numpy()
            if torch.is_tensor(labels):
                labels = labels.cpu().numpy()

            pred_array = predictions.flatten()
            label_array = labels.flatten()
            score_array = pred_array.copy()
        else:
            # ä½¿ç”¨ç´¯ç§¯çš„æ•°æ®
            if not self.all_predictions or not self.all_labels:
                return {}

            pred_array = np.array(self.all_predictions)
            label_array = np.array(self.all_labels)
            score_array = np.array(self.all_scores)

        metrics = {}

        try:
            # åŸºç¡€åˆ†ç±»æŒ‡æ ‡
            if len(np.unique(label_array)) > 1:  # ç¡®ä¿æœ‰æ­£è´Ÿæ ·æœ¬

                # AUC-ROC
                metrics['auc'] = roc_auc_score(label_array, score_array)

                # AUC-PR
                metrics['ap'] = average_precision_score(label_array, score_array)

                # è·å–æœ€ä¼˜é˜ˆå€¼
                optimal_threshold = self._get_optimal_threshold(label_array, score_array)
                metrics['optimal_threshold'] = optimal_threshold

                # åŸºäºæœ€ä¼˜é˜ˆå€¼çš„äºŒåˆ†ç±»æŒ‡æ ‡
                pred_binary = (score_array >= optimal_threshold).astype(int)

                metrics['f1'] = f1_score(label_array, pred_binary)
                metrics['precision'] = precision_score(label_array, pred_binary)
                metrics['recall'] = recall_score(label_array, pred_binary)

                # æ··æ·†çŸ©é˜µç›¸å…³
                tn, fp, fn, tp = confusion_matrix(label_array, pred_binary).ravel()

                metrics['accuracy'] = (tp + tn) / (tp + tn + fp + fn)
                metrics['specificity'] = tn / (tn + fp) if (tn + fp) > 0 else 0
                metrics['sensitivity'] = tp / (tp + fn) if (tp + fn) > 0 else 0

                # å‡é˜³æ€§ç‡å’Œå‡é˜´æ€§ç‡
                metrics['fpr'] = fp / (fp + tn) if (fp + tn) > 0 else 0
                metrics['fnr'] = fn / (fn + tp) if (fn + tp) > 0 else 0

                # å¹³è¡¡å‡†ç¡®ç‡
                metrics['balanced_accuracy'] = (metrics['sensitivity'] + metrics['specificity']) / 2

            else:
                # å¦‚æœåªæœ‰ä¸€ä¸ªç±»åˆ«ï¼Œè¿”å›é»˜è®¤å€¼
                metrics.update({
                    'auc': 0.5, 'ap': 0.0, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0,
                    'accuracy': 0.0, 'specificity': 0.0, 'sensitivity': 0.0,
                    'fpr': 0.0, 'fnr': 0.0, 'balanced_accuracy': 0.0,
                    'optimal_threshold': self.threshold
                })

        except Exception as e:
            print(f"è®¡ç®—æŒ‡æ ‡æ—¶å‡ºé”™: {e}")
            # è¿”å›é»˜è®¤æŒ‡æ ‡
            metrics.update({
                'auc': 0.0, 'ap': 0.0, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0,
                'accuracy': 0.0, 'specificity': 0.0, 'sensitivity': 0.0,
                'fpr': 0.0, 'fnr': 0.0, 'balanced_accuracy': 0.0,
                'optimal_threshold': self.threshold
            })

        return metrics

    def _get_optimal_threshold(self, labels: np.ndarray, scores: np.ndarray) -> float:
        """
        è®¡ç®—æœ€ä¼˜é˜ˆå€¼ï¼ˆåŸºäºF1åˆ†æ•°ï¼‰

        Args:
            labels: çœŸå®æ ‡ç­¾
            scores: é¢„æµ‹åˆ†æ•°

        Returns:
            optimal_threshold: æœ€ä¼˜é˜ˆå€¼
        """
        try:
            precision, recall, thresholds = precision_recall_curve(labels, scores)

            # è®¡ç®—F1åˆ†æ•°
            f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)

            # æ‰¾åˆ°æœ€ä¼˜é˜ˆå€¼
            optimal_idx = np.argmax(f1_scores)

            if optimal_idx < len(thresholds):
                return thresholds[optimal_idx]
            else:
                return self.threshold

        except:
            return self.threshold

    def get_roc_curve_data(self, predictions: Optional[np.ndarray] = None,
                           labels: Optional[np.ndarray] = None) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """
        è·å–ROCæ›²çº¿æ•°æ®

        Returns:
            fpr, tpr, thresholds: ROCæ›²çº¿æ•°æ®
        """
        if predictions is not None and labels is not None:
            score_array = predictions
            label_array = labels
        else:
            score_array = np.array(self.all_scores)
            label_array = np.array(self.all_labels)

        try:
            fpr, tpr, thresholds = roc_curve(label_array, score_array)
            return fpr, tpr, thresholds
        except:
            return np.array([0, 1]), np.array([0, 1]), np.array([0, 1])

    def get_pr_curve_data(self, predictions: Optional[np.ndarray] = None,
                          labels: Optional[np.ndarray] = None) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """
        è·å–Precision-Recallæ›²çº¿æ•°æ®

        Returns:
            precision, recall, thresholds: PRæ›²çº¿æ•°æ®
        """
        if predictions is not None and labels is not None:
            score_array = predictions
            label_array = labels
        else:
            score_array = np.array(self.all_scores)
            label_array = np.array(self.all_labels)

        try:
            precision, recall, thresholds = precision_recall_curve(label_array, score_array)
            return precision, recall, thresholds
        except:
            return np.array([1, 0]), np.array([0, 1]), np.array([0, 1])

    def compute_class_metrics(self, predictions: np.ndarray,
                              labels: np.ndarray,
                              class_names: Optional[List[str]] = None) -> Dict[str, Dict[str, float]]:
        """
        è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡

        Args:
            predictions: é¢„æµ‹ç»“æœ [N]
            labels: çœŸå®æ ‡ç­¾ [N]
            class_names: ç±»åˆ«åç§°åˆ—è¡¨

        Returns:
            class_metrics: æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡å­—å…¸
        """
        if torch.is_tensor(predictions):
            predictions = predictions.cpu().numpy()
        if torch.is_tensor(labels):
            labels = labels.cpu().numpy()

        unique_classes = np.unique(labels)

        if class_names is None:
            class_names = [f'class_{i}' for i in unique_classes]

        class_metrics = {}

        for i, class_id in enumerate(unique_classes):
            if i < len(class_names):
                class_name = class_names[i]
            else:
                class_name = f'class_{class_id}'

            # åˆ›å»ºäºŒåˆ†ç±»æ ‡ç­¾ï¼ˆå½“å‰ç±» vs å…¶ä»–ï¼‰
            binary_labels = (labels == class_id).astype(int)
            binary_preds = (predictions == class_id).astype(int)

            # è®¡ç®—æŒ‡æ ‡
            class_metrics[class_name] = {
                'precision': precision_score(binary_labels, binary_preds, zero_division=0),
                'recall': recall_score(binary_labels, binary_preds, zero_division=0),
                'f1': f1_score(binary_labels, binary_preds, zero_division=0),
                'support': np.sum(binary_labels)
            }

        return class_metrics

    def print_metrics(self, metrics: Optional[Dict[str, float]] = None):
        """
        æ‰“å°æŒ‡æ ‡

        Args:
            metrics: æŒ‡æ ‡å­—å…¸ï¼ˆå¯é€‰ï¼‰
        """
        if metrics is None:
            metrics = self.compute_metrics()

        if not metrics:
            print("æ²¡æœ‰å¯ç”¨çš„æŒ‡æ ‡æ•°æ®")
            return

        print("\n=== å¼‚å¸¸æ£€æµ‹æŒ‡æ ‡ ===")
        print(f"AUC-ROC: {metrics.get('auc', 0):.4f}")
        print(f"AUC-PR:  {metrics.get('ap', 0):.4f}")
        print(f"F1åˆ†æ•°:  {metrics.get('f1', 0):.4f}")
        print(f"ç²¾ç¡®ç‡:  {metrics.get('precision', 0):.4f}")
        print(f"å¬å›ç‡:  {metrics.get('recall', 0):.4f}")
        print(f"å‡†ç¡®ç‡:  {metrics.get('accuracy', 0):.4f}")
        print(f"ç‰¹å¼‚æ€§:  {metrics.get('specificity', 0):.4f}")
        print(f"æ•æ„Ÿæ€§:  {metrics.get('sensitivity', 0):.4f}")
        print(f"å¹³è¡¡å‡†ç¡®ç‡: {metrics.get('balanced_accuracy', 0):.4f}")
        print(f"æœ€ä¼˜é˜ˆå€¼: {metrics.get('optimal_threshold', 0.5):.4f}")


class PerPixelMetrics:
    """
    åƒç´ çº§å¼‚å¸¸æ£€æµ‹æŒ‡æ ‡ï¼ˆç”¨äºåˆ†å‰²ä»»åŠ¡ï¼‰
    """

    def __init__(self):
        self.reset()

    def reset(self):
        """é‡ç½®ç´¯ç§¯æŒ‡æ ‡"""
        self.all_predictions = []
        self.all_labels = []

    def update(self, predictions: Union[np.ndarray, torch.Tensor],
               labels: Union[np.ndarray, torch.Tensor]):
        """
        æ›´æ–°ç´¯ç§¯æŒ‡æ ‡

        Args:
            predictions: é¢„æµ‹mask [B, H, W] æˆ– [B, 1, H, W]
            labels: çœŸå®mask [B, H, W] æˆ– [B, 1, H, W]
        """
        # è½¬æ¢ä¸ºnumpyå¹¶å±•å¹³
        if torch.is_tensor(predictions):
            predictions = predictions.cpu().numpy()
        if torch.is_tensor(labels):
            labels = labels.cpu().numpy()

        # ç¡®ä¿å½¢çŠ¶ä¸€è‡´
        if predictions.ndim == 4:
            predictions = predictions.squeeze(1)
        if labels.ndim == 4:
            labels = labels.squeeze(1)

        self.all_predictions.extend(predictions.flatten())
        self.all_labels.extend(labels.flatten())

    def compute_metrics(self) -> Dict[str, float]:
        """è®¡ç®—åƒç´ çº§æŒ‡æ ‡"""
        if not self.all_predictions or not self.all_labels:
            return {}

        pred_array = np.array(self.all_predictions)
        label_array = np.array(self.all_labels)

        # ä½¿ç”¨AnomalyMetricsè®¡ç®—åŸºç¡€æŒ‡æ ‡
        metrics_calculator = AnomalyMetrics()
        metrics = metrics_calculator.compute_metrics(pred_array, label_array)

        # æ·»åŠ åƒç´ çº§ç‰¹å®šæŒ‡æ ‡
        try:
            # IoU (Intersection over Union)
            intersection = np.sum((pred_array > 0.5) & (label_array > 0.5))
            union = np.sum((pred_array > 0.5) | (label_array > 0.5))
            metrics['iou'] = intersection / union if union > 0 else 0

            # Dice coefficient
            dice = 2 * intersection / (np.sum(pred_array > 0.5) + np.sum(label_array > 0.5))
            metrics['dice'] = dice if not np.isnan(dice) else 0

        except:
            metrics['iou'] = 0
            metrics['dice'] = 0

        return metrics


# æµ‹è¯•å‡½æ•°
def test_metrics():
    """æµ‹è¯•æŒ‡æ ‡è®¡ç®—"""
    print("=== æµ‹è¯•å¼‚å¸¸æ£€æµ‹æŒ‡æ ‡ ===")

    try:
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        np.random.seed(42)
        n_samples = 1000

        # æ¨¡æ‹Ÿé¢„æµ‹åˆ†æ•°å’Œæ ‡ç­¾
        normal_scores = np.random.beta(2, 5, n_samples // 2)  # æ­£å¸¸æ ·æœ¬å€¾å‘äºä½åˆ†
        anomaly_scores = np.random.beta(5, 2, n_samples // 2)  # å¼‚å¸¸æ ·æœ¬å€¾å‘äºé«˜åˆ†

        scores = np.concatenate([normal_scores, anomaly_scores])
        labels = np.concatenate([np.zeros(n_samples // 2), np.ones(n_samples // 2)])

        # æ‰“ä¹±æ•°æ®
        indices = np.random.permutation(n_samples)
        scores = scores[indices]
        labels = labels[indices]

        print(f"æµ‹è¯•æ•°æ®: {n_samples} æ ·æœ¬")
        print(f"æ­£å¸¸æ ·æœ¬: {np.sum(labels == 0)}")
        print(f"å¼‚å¸¸æ ·æœ¬: {np.sum(labels == 1)}")

        # æµ‹è¯•å¼‚å¸¸æ£€æµ‹æŒ‡æ ‡
        print("\n1. æµ‹è¯•å¼‚å¸¸æ£€æµ‹æŒ‡æ ‡")
        metrics_calculator = AnomalyMetrics()

        # æµ‹è¯•æ‰¹é‡æ›´æ–°
        batch_size = 100
        for i in range(0, n_samples, batch_size):
            end_idx = min(i + batch_size, n_samples)
            metrics_calculator.update(
                scores[i:end_idx],
                labels[i:end_idx]
            )

        # è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
        metrics = metrics_calculator.compute_metrics()
        metrics_calculator.print_metrics(metrics)

        # æµ‹è¯•å•æ¬¡è®¡ç®—
        print("\n2. æµ‹è¯•å•æ¬¡æŒ‡æ ‡è®¡ç®—")
        metrics_single = metrics_calculator.compute_metrics(scores, labels)
        print(f"AUC (å•æ¬¡): {metrics_single['auc']:.4f}")
        print(f"F1 (å•æ¬¡): {metrics_single['f1']:.4f}")

        # æµ‹è¯•ROCå’ŒPRæ›²çº¿æ•°æ®
        print("\n3. æµ‹è¯•æ›²çº¿æ•°æ®")
        fpr, tpr, roc_thresholds = metrics_calculator.get_roc_curve_data(scores, labels)
        precision, recall, pr_thresholds = metrics_calculator.get_pr_curve_data(scores, labels)

        print(f"ROCæ›²çº¿ç‚¹æ•°: {len(fpr)}")
        print(f"PRæ›²çº¿ç‚¹æ•°: {len(precision)}")

        # æµ‹è¯•åƒç´ çº§æŒ‡æ ‡
        print("\n4. æµ‹è¯•åƒç´ çº§æŒ‡æ ‡")
        pixel_metrics = PerPixelMetrics()

        # åˆ›å»ºæ¨¡æ‹Ÿçš„åˆ†å‰²mask
        pred_masks = np.random.rand(10, 64, 64) > 0.7
        true_masks = np.random.rand(10, 64, 64) > 0.8

        pixel_metrics.update(pred_masks, true_masks)
        pixel_results = pixel_metrics.compute_metrics()

        print(f"åƒç´ çº§IoU: {pixel_results.get('iou', 0):.4f}")
        print(f"åƒç´ çº§Dice: {pixel_results.get('dice', 0):.4f}")
        print(f"åƒç´ çº§AUC: {pixel_results.get('auc', 0):.4f}")

        print("\nğŸ‰ æŒ‡æ ‡æµ‹è¯•å®Œæˆ!")

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    test_metrics()