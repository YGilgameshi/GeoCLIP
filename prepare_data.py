"""
GeoCLIP æ•°æ®å‡†å¤‡å’ŒéªŒè¯è„šæœ¬
åŸºäºAdaCLIPçš„æ•°æ®é›†ç»“æ„ï¼Œå‡†å¤‡GeoCLIPè®­ç»ƒæ‰€éœ€çš„æ•°æ®
"""

import os
import sys
import argparse
from pathlib import Path
import torch
from tqdm import tqdm

# å¯¼å…¥é…ç½®å’ŒGeoCLIPç»„ä»¶
from config import (
    check_dataset_availability, get_dataset_info, auto_detect_best_config,
    DATA_ROOT, DEPTH_CACHE_ROOT
)
from geoclip.datasets.adaclip_adapter import create_geoclip_dataset, AdaCLIPToGeoCLIPAdapter
from geoclip.models.depth_estimator import DepthEstimator


def validate_dataset_structure(dataset_name: str):
    """éªŒè¯æ•°æ®é›†ç»“æ„æ˜¯å¦ç¬¦åˆAdaCLIPæ ¼å¼"""
    print(f"ğŸ” éªŒè¯æ•°æ®é›†ç»“æ„: {dataset_name}")

    info = get_dataset_info(dataset_name)
    if not info:
        print(f"âŒ ä¸æ”¯æŒçš„æ•°æ®é›†: {dataset_name}")
        return False

    root_path = info['root']
    if not root_path.exists():
        print(f"âŒ æ•°æ®é›†è·¯å¾„ä¸å­˜åœ¨: {root_path}")
        return False

    # æ£€æŸ¥åŸºæœ¬ç›®å½•ç»“æ„
    expected_dirs = []

    if dataset_name == 'mvtec':
        # MVTec AD ç»“æ„æ£€æŸ¥
        for cls_name in info['classes'][:3]:  # æ£€æŸ¥å‰3ä¸ªç±»åˆ«ä½œä¸ºç¤ºä¾‹
            train_dir = root_path / cls_name / 'train'
            test_dir = root_path / cls_name / 'test'
            if train_dir.exists() and test_dir.exists():
                expected_dirs.append(cls_name)

    elif dataset_name in ['colondb', 'clinicdb']:
        # åŒ»å­¦æ•°æ®é›†ç»“æ„æ£€æŸ¥
        if (root_path / 'train').exists() and (root_path / 'test').exists():
            expected_dirs = ['train', 'test']

    if expected_dirs:
        print(f"âœ… æ•°æ®é›†ç»“æ„æ­£ç¡®ï¼Œå‘ç°ç›®å½•: {expected_dirs}")
        return True
    else:
        print(f"âŒ æ•°æ®é›†ç»“æ„ä¸å®Œæ•´")
        return False


def test_dataset_loading(dataset_name: str, max_samples: int = 5):
    """æµ‹è¯•æ•°æ®é›†åŠ è½½åŠŸèƒ½"""
    print(f"ğŸ§ª æµ‹è¯•æ•°æ®é›†åŠ è½½: {dataset_name}")

    try:
        # åˆ›å»ºç®€å•çš„transform
        import torchvision.transforms as transforms
        transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor()
        ])

        # åˆ›å»ºæ•°æ®é›†ï¼ˆä¸é¢„å¤„ç†æ·±åº¦å›¾ï¼‰
        dataset = create_geoclip_dataset(
            dataset_name=dataset_name,
            clsnames=None,
            transform=transform,
            target_transform=transform,
            training=True,
            cache_depth=False,  # æš‚æ—¶ä¸ç¼“å­˜ï¼Œå¿«é€Ÿæµ‹è¯•
            preprocess_all=False
        )

        print(f"âœ… æ•°æ®é›†åŠ è½½æˆåŠŸ: {len(dataset)} ä¸ªæ ·æœ¬")

        # æµ‹è¯•è¯»å–æ ·æœ¬
        if len(dataset) > 0:
            print(f"ğŸ”¬ æµ‹è¯•å‰ {min(max_samples, len(dataset))} ä¸ªæ ·æœ¬...")

            for i in range(min(max_samples, len(dataset))):
                try:
                    sample = dataset[i]
                    print(f"   æ ·æœ¬ {i}: keys={list(sample.keys())}")

                    if 'img' in sample:
                        print(f"     å›¾åƒå½¢çŠ¶: {sample['img'].shape}")
                    if 'depth' in sample:
                        print(f"     æ·±åº¦å½¢çŠ¶: {sample['depth'].shape}")
                        print(f"     æ·±åº¦èŒƒå›´: {sample.get('depth_range', 'N/A')}")
                    if 'anomaly' in sample:
                        print(f"     å¼‚å¸¸æ ‡ç­¾: {sample['anomaly']}")

                except Exception as e:
                    print(f"     âŒ æ ·æœ¬ {i} åŠ è½½å¤±è´¥: {e}")
                    return False

            print(f"âœ… æ ·æœ¬æµ‹è¯•å®Œæˆ")
            return True
        else:
            print(f"âŒ æ•°æ®é›†ä¸ºç©º")
            return False

    except Exception as e:
        print(f"âŒ æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
        return False


def preprocess_depth_cache(dataset_name: str, max_samples: int = None, force: bool = False):
    """é¢„å¤„ç†æ·±åº¦ç¼“å­˜"""
    print(f"âš™ï¸ é¢„å¤„ç†æ·±åº¦ç¼“å­˜: {dataset_name}")

    # åˆ›å»ºæ·±åº¦ç¼“å­˜ç›®å½•
    cache_dir = DEPTH_CACHE_ROOT / dataset_name
    cache_dir.mkdir(parents=True, exist_ok=True)

    # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç¼“å­˜
    existing_cache = list(cache_dir.glob("depth_*.npy"))
    if existing_cache and not force:
        print(f"â„¹ï¸ å·²å­˜åœ¨ {len(existing_cache)} ä¸ªæ·±åº¦ç¼“å­˜æ–‡ä»¶")
        user_input = input("æ˜¯å¦é‡æ–°ç”Ÿæˆï¼Ÿ(y/N): ")
        if user_input.lower() != 'y':
            print("è·³è¿‡æ·±åº¦é¢„å¤„ç†")
            return True

    try:
        # åˆ›å»ºé€‚é…å™¨
        adapter = AdaCLIPToGeoCLIPAdapter(
            cache_depth=True,
            depth_cache_dir=str(cache_dir)
        )

        print(f"âœ… æ·±åº¦ä¼°è®¡å™¨åˆå§‹åŒ–å®Œæˆ")

        # åˆ›å»ºæ•°æ®é›†è¿›è¡Œé¢„å¤„ç†
        import torchvision.transforms as transforms
        transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor()
        ])

        # åˆ†åˆ«å¤„ç†è®­ç»ƒå’Œæµ‹è¯•æ•°æ®
        total_processed = 0

        for training in [True, False]:
            phase = "è®­ç»ƒ" if training else "æµ‹è¯•"
            print(f"ğŸ”„ å¤„ç†{phase}æ•°æ®...")

            try:
                dataset = create_geoclip_dataset(
                    dataset_name=dataset_name,
                    clsnames=None,
                    transform=transform,
                    target_transform=transform,
                    training=training,
                    cache_depth=True,
                    preprocess_all=False,
                    depth_adapter=adapter
                )

                # é™åˆ¶å¤„ç†æ ·æœ¬æ•°é‡
                num_samples = len(dataset)
                if max_samples and num_samples > max_samples:
                    num_samples = max_samples
                    print(f"âš ï¸ é™åˆ¶å¤„ç†æ ·æœ¬æ•°: {num_samples}")

                # é€ä¸ªå¤„ç†æ ·æœ¬ç”Ÿæˆæ·±åº¦ç¼“å­˜
                for i in tqdm(range(num_samples), desc=f"å¤„ç†{phase}æ ·æœ¬"):
                    try:
                        # è·å–æ ·æœ¬ä¼šè‡ªåŠ¨ç”Ÿæˆæ·±åº¦ç¼“å­˜
                        _ = dataset[i]
                        total_processed += 1
                    except Exception as e:
                        print(f"âŒ å¤„ç†æ ·æœ¬ {i} å¤±è´¥: {e}")
                        continue

            except Exception as e:
                print(f"âŒ å¤„ç†{phase}æ•°æ®å¤±è´¥: {e}")
                continue

        print(f"âœ… æ·±åº¦é¢„å¤„ç†å®Œæˆï¼Œæ€»å…±å¤„ç† {total_processed} ä¸ªæ ·æœ¬")

        # ç»Ÿè®¡ç”Ÿæˆçš„ç¼“å­˜æ–‡ä»¶
        final_cache = list(cache_dir.glob("depth_*.npy"))
        print(f"ğŸ“ ç”Ÿæˆæ·±åº¦ç¼“å­˜æ–‡ä»¶: {len(final_cache)} ä¸ª")

        return True

    except Exception as e:
        print(f"âŒ æ·±åº¦é¢„å¤„ç†å¤±è´¥: {e}")
        return False


def validate_geoclip_integration():
    """éªŒè¯GeoCLIPç»„ä»¶é›†æˆ"""
    print("ğŸ”— éªŒè¯GeoCLIPç»„ä»¶é›†æˆ...")

    try:
        # # æµ‹è¯•æ·±åº¦ä¼°è®¡å™¨
        # print("  æµ‹è¯•æ·±åº¦ä¼°è®¡å™¨...")
        # depth_estimator = DepthEstimator()
        # test_image = torch.randn(1, 3, 224, 224)
        # with torch.no_grad():
        #     depth_output = depth_estimator(test_image)
        # print(f"    âœ… æ·±åº¦ä¼°è®¡å™¨: {test_image.shape} -> {depth_output.shape}")

        # æµ‹è¯•GeoCLIPä¸»æ¨¡å‹
        print("  æµ‹è¯•GeoCLIPä¸»æ¨¡å‹...")
        from geoclip.models.geoclip_main import create_geoclip_model

        model_config = {
            'clip_model': 'ViT-B-16',
            'detection_type': 'regression',
            'device': 'cpu'  # ä½¿ç”¨CPUè¿›è¡Œæµ‹è¯•
        }

        model = create_geoclip_model(model_config)
        test_batch = {'image': torch.randn(1, 3, 224, 224)}

        with torch.no_grad():
            results = model(test_batch)

        print(f"    âœ… GeoCLIPæ¨¡å‹æµ‹è¯•æˆåŠŸ")
        for key, value in results.items():
            if torch.is_tensor(value):
                print(f"      {key}: {value.shape}")

        return True

    except Exception as e:
        print(f"    âŒ ç»„ä»¶é›†æˆæµ‹è¯•å¤±è´¥: {e}")
        return False


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser("GeoCLIPæ•°æ®å‡†å¤‡å·¥å…·")

    parser.add_argument("--action", type=str,
                        choices=['check', 'validate', 'test_load', 'preprocess', 'full_check'],
                        default='full_check',
                        help="æ‰§è¡Œçš„æ“ä½œ")

    parser.add_argument("--dataset", type=str,
                        choices=['mvtec', 'visa', 'btad', 'colondb', 'clinicdb', 'all'],
                        default='all',
                        help="æ“ä½œçš„æ•°æ®é›†")

    parser.add_argument("--max_samples", type=int, default=100,
                        help="é¢„å¤„ç†æˆ–æµ‹è¯•çš„æœ€å¤§æ ·æœ¬æ•°")

    parser.add_argument("--force", action='store_true',
                        help="å¼ºåˆ¶é‡æ–°å¤„ç†")

    args = parser.parse_args()

    print("ğŸš€ GeoCLIPæ•°æ®å‡†å¤‡å·¥å…·")
    print("=" * 40)

    if args.action == 'check':
        # æ£€æŸ¥æ•°æ®é›†å¯ç”¨æ€§
        available, missing = check_dataset_availability()
        if missing:
            print(f"\nğŸ’¡ å»ºè®®ï¼šè¯·å…ˆä¸‹è½½ç¼ºå¤±çš„æ•°æ®é›†: {missing}")

    elif args.action == 'validate':
        # éªŒè¯æ•°æ®é›†ç»“æ„
        datasets = [args.dataset] if args.dataset != 'all' else ['mvtec', 'visa', 'colondb']

        for dataset in datasets:
            if not validate_dataset_structure(dataset):
                print(f"âŒ {dataset} éªŒè¯å¤±è´¥")
                return 1

    elif args.action == 'test_load':
        # æµ‹è¯•æ•°æ®é›†åŠ è½½
        datasets = [args.dataset] if args.dataset != 'all' else ['mvtec', 'visa', 'colondb']

        for dataset in datasets:
            if not test_dataset_loading(dataset, args.max_samples):
                print(f"âŒ {dataset} åŠ è½½æµ‹è¯•å¤±è´¥")
                return 1

    elif args.action == 'preprocess':
        # é¢„å¤„ç†æ·±åº¦ç¼“å­˜
        datasets = [args.dataset] if args.dataset != 'all' else ['mvtec']  # é»˜è®¤åªå¤„ç†mvtec

        for dataset in datasets:
            if not preprocess_depth_cache(dataset, args.max_samples, args.force):
                print(f"âŒ {dataset} æ·±åº¦é¢„å¤„ç†å¤±è´¥")
                return 1

    elif args.action == 'full_check':
        # å®Œæ•´æ£€æŸ¥
        print("ğŸ” æ‰§è¡Œå®Œæ•´æ£€æŸ¥...")

        # 1. æ£€æŸ¥æ•°æ®é›†å¯ç”¨æ€§
        available, missing = check_dataset_availability()

        # 2. éªŒè¯GeoCLIPé›†æˆ
        if not validate_geoclip_integration():
            return 1

        # 3. æµ‹è¯•æ•°æ®é›†åŠ è½½
        test_datasets = ['mvtec'] if 'mvtec' in available else available[:1]
        for dataset in test_datasets:
            if not test_dataset_loading(dataset, 3):  # åªæµ‹è¯•3ä¸ªæ ·æœ¬
                print(f"âŒ {dataset} æµ‹è¯•å¤±è´¥")
                return 1

        # 4. ç»™å‡ºå»ºè®®
        print("\nğŸ’¡ å‡†å¤‡å»ºè®®:")
        best_config = auto_detect_best_config()
        if best_config:
            print("âœ… æ‚¨çš„ç¯å¢ƒå·²å‡†å¤‡å°±ç»ªï¼")
            print("æ¨èä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¼€å§‹è®­ç»ƒ:")
            print(f"  bash quick_train.sh standard")
        else:
            print("âŒ ç¯å¢ƒæœªå‡†å¤‡å°±ç»ªï¼Œè¯·å…ˆä¸‹è½½æ•°æ®é›†")

    print("\nğŸ‰ æ•°æ®å‡†å¤‡æ£€æŸ¥å®Œæˆï¼")
    return 0


if __name__ == "__main__":
    exit_code = main()
    exit(exit_code)