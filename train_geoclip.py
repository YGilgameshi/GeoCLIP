"""
GeoCLIP Training Script
åŸºäºAdaCLIPçš„è®­ç»ƒè„šæœ¬ï¼Œæ·»åŠ äº†3Dç‰¹å¾å¤„ç†
"""

import warnings

warnings.filterwarnings("ignore", category=RuntimeWarning)

import sys
import argparse
import json
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from pathlib import Path
import numpy as np

# æ·»åŠ AdaCLIPè·¯å¾„åˆ°Pythonè·¯å¾„ (ç”¨äºå¯¼å…¥AdaCLIPçš„æ•°æ®é›†)
current_dir = Path(__file__).parent.absolute()
if current_dir not in sys.path:
    sys.path.insert(0, str(current_dir))

# GeoCLIPç»„ä»¶å¯¼å…¥
from geoclip.models.geoclip_main import create_geoclip_model
from geoclip.training.trainer import create_trainer
from geoclip.training.losses import create_loss_function
from geoclip.datasets.adaclip_adapter import create_geoclip_dataset
from geoclip.utils.metrics import AnomalyMetrics
from geoclip.utils.visualization import Visualizer

# AdaCLIPå·¥å…·å¯¼å…¥ (ä»å¤åˆ¶çš„æ–‡ä»¶å¯¼å…¥)
try:
    from tools.tools import setup_seed, Logger
except ImportError:
    print("è­¦å‘Š: æ— æ³•å¯¼å…¥AdaCLIPå·¥å…·ï¼Œå°†ä½¿ç”¨GeoCLIPå†…ç½®åŠŸèƒ½")


    def setup_seed(seed):
        torch.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        np.random.seed(seed)
        import random
        random.seed(seed)


    class Logger:
        def __init__(self, log_path):
            self.log_path = log_path
            if log_path:
                # æ¸…ç©ºæ—¥å¿—æ–‡ä»¶
                with open(log_path, 'w') as f:
                    f.write("")

        def info(self, msg):
            print(msg)
            if self.log_path:
                with open(self.log_path, 'a', encoding='utf-8') as f:
                    f.write(msg + '\n')

# ç¡®ä¿èƒ½æ‰¾åˆ°AdaCLIPå¤åˆ¶çš„æ•°æ®é›†
try:
    from dataset import get_data

    ADACLIP_DATA_AVAILABLE = True
except ImportError:
    print("è­¦å‘Š: AdaCLIPæ•°æ®é›†å‡½æ•°ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨GeoCLIPæ•°æ®é›†æ¥å£")
    ADACLIP_DATA_AVAILABLE = False


def str2bool(v):
    """å­—ç¬¦ä¸²è½¬å¸ƒå°”å€¼"""
    return v.lower() in ("yes", "true", "t", "1")


def create_data_loaders(args, model_transforms=None):
    """
    åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼Œå…¼å®¹AdaCLIPçš„æ•°æ®é›†ç»“æ„
    """
    print("ğŸ”„ åˆ›å»ºæ•°æ®åŠ è½½å™¨...")

    # è®¾ç½®é»˜è®¤transforms
    if model_transforms is None:
        import torchvision.transforms as transforms
        model_transforms = {
            'preprocess': transforms.Compose([
                transforms.Resize((args.image_size, args.image_size)),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                     std=[0.229, 0.224, 0.225])
            ]),
            'transform': transforms.Compose([
                transforms.Resize((args.image_size, args.image_size)),
                transforms.ToTensor()
            ])
        }

    # åˆ›å»ºè®­ç»ƒæ•°æ®é›†
    train_datasets = []
    for dataset_name in args.training_data:
        try:
            print(f"ğŸ“Š åŠ è½½è®­ç»ƒæ•°æ®é›†: {dataset_name}")

            # å¯¹äºMVTecç­‰å¤§æ•°æ®é›†ï¼Œå¯ä»¥é€‰æ‹©æ€§åœ°é¢„å¤„ç†æ·±åº¦å›¾
            preprocess_depth = args.preprocess_depth and dataset_name in ['mvtec', 'visa']

            train_dataset = create_geoclip_dataset(
                dataset_name=dataset_name,
                clsnames=None,  # ä½¿ç”¨é»˜è®¤ç±»åˆ«
                transform=model_transforms['preprocess'],
                target_transform=model_transforms['transform'],
                depth_transform=model_transforms['transform'],
                training=True,
                cache_depth=True,
                preprocess_all=preprocess_depth,
                depth_cache_dir=f'./depth_cache/{dataset_name}'
            )

            train_datasets.append(train_dataset)
            print(f"âœ… {dataset_name} è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")

        except Exception as e:
            print(f"âŒ åŠ è½½ {dataset_name} è®­ç»ƒé›†å¤±è´¥: {e}")
            continue

    # åˆå¹¶è®­ç»ƒæ•°æ®é›†
    if len(train_datasets) > 1:
        from torch.utils.data import ConcatDataset
        train_dataset = ConcatDataset(train_datasets)
    elif len(train_datasets) == 1:
        train_dataset = train_datasets[0]
    else:
        raise ValueError("æ— æ³•åˆ›å»ºä»»ä½•è®­ç»ƒæ•°æ®é›†")

    # åˆ›å»ºæµ‹è¯•æ•°æ®é›† (ç”¨ä½œéªŒè¯)
    test_datasets = []
    for dataset_name in args.testing_data:
        try:
            print(f"ğŸ“Š åŠ è½½æµ‹è¯•æ•°æ®é›†: {dataset_name}")

            test_dataset = create_geoclip_dataset(
                dataset_name=dataset_name,
                clsnames=None,
                transform=model_transforms['preprocess'],
                target_transform=model_transforms['transform'],
                depth_transform=model_transforms['transform'],
                training=False,
                cache_depth=True,
                preprocess_all=False,  # æµ‹è¯•é›†ä¸é¢„å¤„ç†
                depth_cache_dir=f'./depth_cache/{dataset_name}'
            )

            test_datasets.append(test_dataset)
            print(f"âœ… {dataset_name} æµ‹è¯•é›†: {len(test_dataset)} æ ·æœ¬")

        except Exception as e:
            print(f"âŒ åŠ è½½ {dataset_name} æµ‹è¯•é›†å¤±è´¥: {e}")
            continue

    # åˆå¹¶æµ‹è¯•æ•°æ®é›†
    if len(test_datasets) > 1:
        from torch.utils.data import ConcatDataset
        test_dataset = ConcatDataset(test_datasets)
    elif len(test_datasets) == 1:
        test_dataset = test_datasets[0]
    else:
        raise ValueError("æ— æ³•åˆ›å»ºä»»ä½•æµ‹è¯•æ•°æ®é›†")

    # åˆ›å»ºDataLoader
    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=args.num_workers,
        pin_memory=True,
        drop_last=True
    )

    test_loader = DataLoader(
        test_dataset,
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=args.num_workers,
        pin_memory=True
    )

    print(f"ğŸ“Š æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ:")
    print(f"   è®­ç»ƒæ ·æœ¬: {len(train_dataset)}")
    print(f"   æµ‹è¯•æ ·æœ¬: {len(test_dataset)}")
    print(f"   æ‰¹æ¬¡å¤§å°: {args.batch_size}")

    return train_loader, test_loader


def setup_experiment_paths(args):
    """è®¾ç½®å®éªŒè·¯å¾„"""
    # åˆ›å»ºå®éªŒåç§°
    train_data_str = '_'.join(args.training_data)
    test_data_str = '_'.join(args.testing_data) if isinstance(args.testing_data, list) else args.testing_data

    experiment_name = f"geoclip_{train_data_str}_vs_{test_data_str}_{args.model.replace('/', '-')}"

    # åˆ›å»ºè·¯å¾„
    save_path = Path(args.save_path) / experiment_name
    save_path.mkdir(parents=True, exist_ok=True)

    paths = {
        'experiment_name': experiment_name,
        'save_path': save_path,
        'log_path': save_path / 'train.log',
        'config_path': save_path / 'config.json',
        'results_path': save_path / 'results',
        'checkpoints_path': save_path / 'checkpoints',
        'visualizations_path': save_path / 'visualizations'
    }

    # åˆ›å»ºå­ç›®å½•
    for key in ['results_path', 'checkpoints_path', 'visualizations_path']:
        paths[key].mkdir(exist_ok=True)

    return paths


def create_model_config(args):
    """åˆ›å»ºGeoCLIPæ¨¡å‹é…ç½®"""
    model_config = {
        # CLIPé…ç½®
        'clip_model': args.model,
        'clip_pretrained': 'openai',
        'freeze_clip': args.freeze_clip,

        # æ·±åº¦ä¼°è®¡é…ç½®
        'depth_estimator': 'DPT_Large',

        # å‡ ä½•ç¼–ç å™¨é…ç½®
        'geometry_encoder': {
            'type': args.geometry_encoder,
            'in_channels': 4,  # RGB + Depth
            'base_channels': 64,
            'num_stages': 4,
            'output_channels': 512,
            'voxel_size': 64
        },

        # ç‰¹å¾èåˆé…ç½®
        'fusion': {
            'type': args.fusion_type,
            'clip_dim': 512,  # ä¼šæ ¹æ®å®é™…CLIPæ¨¡å‹è°ƒæ•´
            'geometry_dim': 512,
            'fusion_dim': args.fusion_dim,
            'output_dim': 512
        },

        # å¼‚å¸¸æ£€æµ‹é…ç½®
        'detection_type': 'regression',
        'num_classes': 2,
        'device': 'cuda' if torch.cuda.is_available() else 'cpu'
    }

    return model_config


def create_training_config(args):
    """åˆ›å»ºè®­ç»ƒé…ç½®"""
    training_config = {
        # æŸå¤±å‡½æ•°é…ç½®
        'loss': {
            'type': 'geoclip',
            'contrastive_weight': args.contrastive_weight,
            'geometry_weight': args.geometry_weight,
            'anomaly_weight': args.anomaly_weight,
            'use_contrastive': True,
            'use_geometry': True,
            'anomaly_config': {
                'loss_type': 'focal',
                'alpha': 0.25,
                'gamma': 2.0
            }
        },

        # ä¼˜åŒ–å™¨é…ç½®
        'optimizer': {
            'type': 'adamw',
            'params': {
                'lr': args.learning_rate,
                'weight_decay': args.weight_decay
            }
        },

        # å­¦ä¹ ç‡è°ƒåº¦å™¨é…ç½®
        'scheduler': {
            'type': 'cosine',
            'params': {
                'T_max': args.epoch
            }
        },

        # è®­ç»ƒå™¨é…ç½®
        'trainer': {
            'experiment_name': None,  # ä¼šåœ¨åé¢è®¾ç½®
            'experiment_dir': str(args.save_path),
            'device': 'cuda' if torch.cuda.is_available() else 'cpu',
            'use_wandb': args.use_wandb,
            'wandb_config': {
                'project': 'GeoCLIP',
                'entity': args.wandb_entity
            } if args.use_wandb else {},
            'log_interval': args.print_freq,
            'save_interval': args.save_freq,
            'early_stopping_patience': args.early_stopping_patience
        }
    }

    return training_config


def train_geoclip(args):
    """ä¸»è®­ç»ƒå‡½æ•°"""
    print("ğŸš€ å¼€å§‹GeoCLIPè®­ç»ƒ")
    print("=" * 50)

    # è®¾ç½®éšæœºç§å­
    setup_seed(args.seed)

    # è®¾ç½®å®éªŒè·¯å¾„
    paths = setup_experiment_paths(args)

    # è®¾ç½®æ—¥å¿—
    logger = Logger(str(paths['log_path']))
    logger.info("å¼€å§‹GeoCLIPè®­ç»ƒ")

    # æ‰“å°é…ç½®ä¿¡æ¯
    for key, value in sorted(vars(args).items()):
        logger.info(f'{key} = {value}')

    # åˆ›å»ºæ¨¡å‹å’Œè®­ç»ƒé…ç½®
    model_config = create_model_config(args)
    training_config = create_training_config(args)
    training_config['trainer']['experiment_name'] = paths['experiment_name']

    # ä¿å­˜é…ç½®
    full_config = {
        'model': model_config,
        'training': training_config,
        'args': vars(args)
    }

    with open(paths['config_path'], 'w') as f:
        json.dump(full_config, f, indent=2)

    logger.info(f"é…ç½®å·²ä¿å­˜: {paths['config_path']}")

    try:
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader, test_loader = create_data_loaders(args)

        # åˆ›å»ºè®­ç»ƒå™¨
        logger.info("åˆ›å»ºGeoCLIPè®­ç»ƒå™¨...")
        trainer = create_trainer(
            config={
                'model': model_config,
                'loss': training_config['loss'],
                'optimizer': training_config['optimizer'],
                'scheduler': training_config['scheduler'],
                'trainer': training_config['trainer']
            },
            train_loader=train_loader,
            val_loader=test_loader,  # ä½¿ç”¨æµ‹è¯•é›†ä½œä¸ºéªŒè¯é›†
            test_loader=test_loader
        )

        logger.info("âœ… è®­ç»ƒå™¨åˆ›å»ºæˆåŠŸ")
        logger.info(f"æ¨¡å‹ä¿¡æ¯: {trainer.model.get_model_info()}")

        # å¼€å§‹è®­ç»ƒ
        logger.info(f"å¼€å§‹è®­ç»ƒï¼Œå…± {args.epoch} ä¸ªepoch...")
        test_results = trainer.train(
            num_epochs=args.epoch,
            resume_from=args.ckt_path if args.ckt_path else None
        )

        # ä¿å­˜æœ€ç»ˆç»“æœ
        results_file = paths['results_path'] / 'final_results.json'
        with open(results_file, 'w') as f:
            json.dump(test_results, f, indent=2)

        logger.info(f"âœ… è®­ç»ƒå®Œæˆï¼")
        logger.info(f"æœ€ç»ˆç»“æœå·²ä¿å­˜: {results_file}")

        return test_results

    except Exception as e:
        logger.info(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser("GeoCLIP Training", add_help=True)

    # æ•°æ®é›†é…ç½®
    parser.add_argument("--training_data", type=str, default=["mvtec", "colondb"], nargs='+',
                        help="è®­ç»ƒæ•°æ®é›†åˆ—è¡¨")
    parser.add_argument("--testing_data", type=str, default=["visa"], nargs='+',
                        help="æµ‹è¯•æ•°æ®é›†åˆ—è¡¨")

    # è·¯å¾„é…ç½®
    parser.add_argument("--save_path", type=str, default='./workspaces_geoclip',
                        help="ç»“æœä¿å­˜ç›®å½•")

    # æ¨¡å‹é…ç½®
    parser.add_argument("--model", type=str, default="ViT-B-16",
                        choices=["ViT-B-16", "ViT-B-32", "ViT-L-14", "ViT-L-14-336"],
                        help="CLIPæ¨¡å‹é€‰æ‹©")

    parser.add_argument("--geometry_encoder", type=str, default="voxel",
                        choices=["voxel", "sparse", "hierarchical"],
                        help="å‡ ä½•ç¼–ç å™¨ç±»å‹")

    parser.add_argument("--fusion_type", type=str, default="cross_attention",
                        choices=["cross_attention", "adaptive_fusion", "simple_concat", "gated_fusion"],
                        help="ç‰¹å¾èåˆç±»å‹")

    parser.add_argument("--fusion_dim", type=int, default=1024,
                        help="èåˆå±‚ç»´åº¦")

    parser.add_argument("--freeze_clip", type=str2bool, default=False,
                        help="æ˜¯å¦å†»ç»“CLIPå‚æ•°")

    # è®­ç»ƒé…ç½®
    parser.add_argument("--epoch", type=int, default=100, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--learning_rate", type=float, default=1e-4, help="å­¦ä¹ ç‡")
    parser.add_argument("--batch_size", type=int, default=8, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--image_size", type=int, default=224, help="å›¾åƒå°ºå¯¸")
    parser.add_argument("--num_workers", type=int, default=4, help="æ•°æ®åŠ è½½è¿›ç¨‹æ•°")
    parser.add_argument("--weight_decay", type=float, default=1e-5, help="æƒé‡è¡°å‡")

    # æŸå¤±å‡½æ•°æƒé‡
    parser.add_argument("--contrastive_weight", type=float, default=1.0, help="å¯¹æ¯”å­¦ä¹ æŸå¤±æƒé‡")
    parser.add_argument("--geometry_weight", type=float, default=0.5, help="å‡ ä½•ä¸€è‡´æ€§æŸå¤±æƒé‡")
    parser.add_argument("--anomaly_weight", type=float, default=2.0, help="å¼‚å¸¸æ£€æµ‹æŸå¤±æƒé‡")

    # æ·±åº¦å¤„ç†é…ç½®
    parser.add_argument("--preprocess_depth", type=str2bool, default=True,
                        help="æ˜¯å¦é¢„å¤„ç†æ·±åº¦å›¾ç¼“å­˜")

    # è®­ç»ƒæ§åˆ¶
    parser.add_argument("--print_freq", type=int, default=10, help="æ—¥å¿—æ‰“å°é¢‘ç‡")
    parser.add_argument("--save_freq", type=int, default=5, help="æ¨¡å‹ä¿å­˜é¢‘ç‡")
    parser.add_argument("--early_stopping_patience", type=int, default=15, help="æ—©åœè€å¿ƒå€¼")

    # å®éªŒè·Ÿè¸ª
    parser.add_argument("--use_wandb", type=str2bool, default=False, help="æ˜¯å¦ä½¿ç”¨wandb")
    parser.add_argument("--wandb_entity", type=str, default=None, help="wandbå®ä½“å")

    # å…¶ä»–é…ç½®
    parser.add_argument("--seed", type=int, default=111, help="éšæœºç§å­")
    parser.add_argument("--ckt_path", type=str, default=None, help="æ¢å¤è®­ç»ƒçš„æ£€æŸ¥ç‚¹è·¯å¾„")

    args = parser.parse_args()

    # å¼€å§‹è®­ç»ƒ
    results = train_geoclip(args)

    if results is not None:
        print("\nğŸ‰ GeoCLIPè®­ç»ƒæˆåŠŸå®Œæˆï¼")
        print(f"æœ€ä½³æµ‹è¯•ç»“æœ:")
        for key, value in results.items():
            if isinstance(value, float):
                print(f"  {key}: {value:.4f}")
    else:
        print("\nâŒ GeoCLIPè®­ç»ƒå¤±è´¥")
        return 1

    return 0


if __name__ == "__main__":
    exit_code = main()
    exit(exit_code)